From db4501e3d10e38319ebc68042c87f1b77358a715 Mon Sep 17 00:00:00 2001
From: openhands <openhands@all-hands.dev>
Date: Thu, 3 Jul 2025 00:30:19 +0000
Subject: [PATCH] Fix formatting for all Python files using black

---
 agents/__init__.py                         |   2 +-
 agents/a2a_framework.py                    | 208 +++----
 agents/a2a_mcp_integration.py              | 545 ++++++++--------
 agents/executor.py                         |  23 +-
 agents/mutator.py                          |  59 +-
 agents/specialized/__init__.py             |   2 +-
 agents/specialized/code_generator.py       | 165 ++---
 agents/specialized/filesystem_agent.py     |  78 +--
 agents/unified/mcp_a2a_mojo_integration.py | 262 ++++----
 agents/unified_transport_layer.py          | 331 +++++-----
 analyzers/pattern_detector.py              | 383 ++++++------
 connectors/dwave_quantum_connector.py      | 393 ++++++------
 connectors/llm_connector.py                | 153 +++--
 connectors/mcp_base.py                     | 310 +++++----
 connectors/mcp_debug_tool.py               | 577 +++++++++--------
 connectors/xai_connector.py                | 140 ++---
 deploy_mcp_llm_integration.py              | 242 ++++----
 deploy_production_mcp.py                   | 210 +++----
 fabric/integrated_mcp_fabric.py            | 231 +++----
 fabric/state_continuity_core.py            | 254 ++++----
 guardian_linter_watchdog.py                |  31 +-
 llm/continuous_learning_system.py          | 690 +++++++++++----------
 main.py                                    |  34 +-
 mcp_runtime_template_hg/api/app.py         |   4 +-
 mcp_runtime_template_hg/cli/mcp.py         |   8 +-
 mcp_server/__init__.py                     |   2 +-
 mcp_server/main.py                         |  48 +-
 mcp_server/quantum_tools.py                | 479 +++++++-------
 orchestrator.py                            | 433 +++++++------
 protocols/--development.py                 |   9 +-
 protocols/__init__.py                      |   2 +-
 protocols/always_fails.py                  |   9 +-
 protocols/api_health_checker.py            |  60 +-
 protocols/data_processor.py                | 101 +--
 protocols/database_health_check.py         |  85 +--
 protocols/default_protocol.py              |   9 +-
 protocols/execution_analytics.py           | 143 +++--
 protocols/file_validator.py                |   9 +-
 protocols/file_validator_docker.py         |   9 +-
 protocols/loader.py                        |  23 +-
 protocols/log_analyzer.py                  |  59 +-
 protocols/multimodal_llm_analyzer.py       | 561 ++++++++++-------
 protocols/nonexistent.py                   |   9 +-
 protocols/redis_cache_manager.py           | 126 ++--
 protocols/system_monitor.py                |  55 +-
 protocols/test_protocol.py                 |   4 +-
 protocols/user_data_processor.py           | 105 ++--
 test_mcp_debug_quantum.py                  | 312 ++++++----
 test_mcp_debug_simple.py                   | 264 ++++----
 test_mcp_ecosystem_expansion.py            | 498 ++++++++-------
 test_real_dwave_quantum.py                 | 187 +++---
 utils/__init__.py                          |   2 +-
 utils/db_tracker.py                        | 242 +++++---
 utils/logger.py                            |  16 +-
 utils/protocol_registry.py                 | 162 ++---
 utils/tracker.py                           |  60 +-
 56 files changed, 4966 insertions(+), 4452 deletions(-)

diff --git a/agents/__init__.py b/agents/__init__.py
index daf1d8b..7d5e763 100644
--- a/agents/__init__.py
+++ b/agents/__init__.py
@@ -4,4 +4,4 @@ Autonomous agents with protocol mutation capabilities
 """
 
 __version__ = "1.0.0"
-__all__ = ["executor", "mutator", "registry"] 
\ No newline at end of file
+__all__ = ["executor", "mutator", "registry"]
diff --git a/agents/a2a_framework.py b/agents/a2a_framework.py
index 4e80dc3..d0fd2d0 100644
--- a/agents/a2a_framework.py
+++ b/agents/a2a_framework.py
@@ -8,15 +8,18 @@ from datetime import datetime
 from abc import ABC, abstractmethod
 import uuid
 
+
 class A2AMessage:
     """Standard message format for agent communication"""
-    
-    def __init__(self, 
-                 sender: str,
-                 recipient: str,
-                 message_type: str,
-                 content: Dict,
-                 conversation_id: str = None):
+
+    def __init__(
+        self,
+        sender: str,
+        recipient: str,
+        message_type: str,
+        content: Dict,
+        conversation_id: str = None,
+    ):
         self.id = str(uuid.uuid4())
         self.sender = sender
         self.recipient = recipient
@@ -24,67 +27,69 @@ class A2AMessage:
         self.content = content
         self.conversation_id = conversation_id or str(uuid.uuid4())
         self.timestamp = datetime.utcnow().isoformat()
-        
+
     def to_dict(self) -> Dict:
         return {
-            'id': self.id,
-            'sender': self.sender,
-            'recipient': self.recipient,
-            'message_type': self.message_type,
-            'content': self.content,
-            'conversation_id': self.conversation_id,
-            'timestamp': self.timestamp
+            "id": self.id,
+            "sender": self.sender,
+            "recipient": self.recipient,
+            "message_type": self.message_type,
+            "content": self.content,
+            "conversation_id": self.conversation_id,
+            "timestamp": self.timestamp,
         }
-    
+
     @classmethod
-    def from_dict(cls, data: Dict) -> 'A2AMessage':
+    def from_dict(cls, data: Dict) -> "A2AMessage":
         msg = cls(
-            sender=data['sender'],
-            recipient=data['recipient'],
-            message_type=data['message_type'],
-            content=data['content'],
-            conversation_id=data.get('conversation_id')
+            sender=data["sender"],
+            recipient=data["recipient"],
+            message_type=data["message_type"],
+            content=data["content"],
+            conversation_id=data.get("conversation_id"),
         )
-        msg.id = data['id']
-        msg.timestamp = data['timestamp']
+        msg.id = data["id"]
+        msg.timestamp = data["timestamp"]
         return msg
 
 
 class BaseAgent(ABC):
     """Base class for all agents with A2A capabilities"""
-    
+
     def __init__(self, agent_id: str, capabilities: List[str]):
         self.agent_id = agent_id
         self.capabilities = capabilities
         self.conversations = {}
         self.message_handlers = {}
         self.state = {}
-        
+
     @abstractmethod
     async def process_intent(self, intent: Dict) -> Dict:
         """Process an intent and return result"""
         pass
-    
-    async def send_message(self, recipient: str, message_type: str, content: Dict) -> A2AMessage:
+
+    async def send_message(
+        self, recipient: str, message_type: str, content: Dict
+    ) -> A2AMessage:
         """Send message to another agent"""
         msg = A2AMessage(
             sender=self.agent_id,
             recipient=recipient,
             message_type=message_type,
-            content=content
+            content=content,
         )
-        
+
         # Send through message bus
         await message_bus.send(msg)
         return msg
-    
+
     async def receive_message(self, message: A2AMessage):
         """Receive and process message from another agent"""
         # Store in conversation history
         if message.conversation_id not in self.conversations:
             self.conversations[message.conversation_id] = []
         self.conversations[message.conversation_id].append(message)
-        
+
         # Process based on message type
         handler = self.message_handlers.get(message.message_type)
         if handler:
@@ -93,9 +98,9 @@ class BaseAgent(ABC):
                 await self.send_message(
                     recipient=message.sender,
                     message_type=f"{message.message_type}_response",
-                    content=response
+                    content=response,
                 )
-    
+
     def register_handler(self, message_type: str, handler: Callable):
         """Register handler for specific message type"""
         self.message_handlers[message_type] = handler
@@ -103,25 +108,25 @@ class BaseAgent(ABC):
 
 class NegotiationAgent(BaseAgent):
     """Agent specialized in negotiating between other agents"""
-    
+
     def __init__(self):
         super().__init__("negotiator", ["negotiate", "mediate", "coordinate"])
         self.register_handler("negotiate_request", self.handle_negotiation)
-        
+
     async def process_intent(self, intent: Dict) -> Dict:
         """Process negotiation intent"""
-        if intent.get('action') == 'negotiate':
+        if intent.get("action") == "negotiate":
             return await self.negotiate_between_agents(
-                intent['agents'],
-                intent['topic'],
-                intent.get('constraints', {})
+                intent["agents"], intent["topic"], intent.get("constraints", {})
             )
-        return {'error': 'Unknown intent'}
-    
-    async def negotiate_between_agents(self, agents: List[str], topic: str, constraints: Dict) -> Dict:
+        return {"error": "Unknown intent"}
+
+    async def negotiate_between_agents(
+        self, agents: List[str], topic: str, constraints: Dict
+    ) -> Dict:
         """Coordinate negotiation between multiple agents"""
         negotiation_id = str(uuid.uuid4())
-        
+
         # Start negotiation with each agent
         proposals = {}
         for agent in agents:
@@ -129,120 +134,106 @@ class NegotiationAgent(BaseAgent):
                 recipient=agent,
                 message_type="proposal_request",
                 content={
-                    'negotiation_id': negotiation_id,
-                    'topic': topic,
-                    'constraints': constraints
-                }
+                    "negotiation_id": negotiation_id,
+                    "topic": topic,
+                    "constraints": constraints,
+                },
             )
             # Wait for proposals (simplified - real implementation would be async)
             proposals[agent] = None
-        
+
         # Analyze proposals and find optimal solution
         solution = await self.find_optimal_solution(proposals, constraints)
-        
+
         # Communicate decision
         for agent in agents:
             await self.send_message(
                 recipient=agent,
                 message_type="negotiation_result",
-                content={
-                    'negotiation_id': negotiation_id,
-                    'solution': solution
-                }
+                content={"negotiation_id": negotiation_id, "solution": solution},
             )
-        
+
         return {
-            'negotiation_id': negotiation_id,
-            'participants': agents,
-            'solution': solution,
-            'status': 'completed'
+            "negotiation_id": negotiation_id,
+            "participants": agents,
+            "solution": solution,
+            "status": "completed",
         }
-    
+
     async def find_optimal_solution(self, proposals: Dict, constraints: Dict) -> Dict:
         """Find optimal solution from proposals"""
         # This would use optimization algorithms
         # For now, return a simple solution
-        return {
-            'agreed_terms': {},
-            'consensus_level': 0.85
-        }
-    
+        return {"agreed_terms": {}, "consensus_level": 0.85}
+
     async def handle_negotiation(self, message: A2AMessage) -> Dict:
         """Handle incoming negotiation request"""
         # Process negotiation request
-        return {
-            'status': 'accepted',
-            'terms': {}
-        }
+        return {"status": "accepted", "terms": {}}
 
 
 class DataAnalysisAgent(BaseAgent):
     """Agent specialized in data analysis"""
-    
+
     def __init__(self):
         super().__init__("data_analyst", ["analyze", "process", "insights"])
         self.register_handler("analysis_request", self.handle_analysis_request)
-        
+
     async def process_intent(self, intent: Dict) -> Dict:
         """Process data analysis intent"""
-        if intent.get('action') == 'analyze':
+        if intent.get("action") == "analyze":
             return await self.analyze_data(
-                intent['data_source'],
-                intent.get('analysis_type', 'general')
+                intent["data_source"], intent.get("analysis_type", "general")
             )
-        return {'error': 'Unknown intent'}
-    
+        return {"error": "Unknown intent"}
+
     async def analyze_data(self, data_source: str, analysis_type: str) -> Dict:
         """Perform data analysis"""
         # This would connect to actual data sources
         # For now, return mock analysis
         return {
-            'data_source': data_source,
-            'analysis_type': analysis_type,
-            'insights': [
-                'Pattern detected in time series',
-                'Anomaly found at timestamp X',
-                'Correlation between A and B'
+            "data_source": data_source,
+            "analysis_type": analysis_type,
+            "insights": [
+                "Pattern detected in time series",
+                "Anomaly found at timestamp X",
+                "Correlation between A and B",
             ],
-            'confidence': 0.92
+            "confidence": 0.92,
         }
-    
+
     async def handle_analysis_request(self, message: A2AMessage) -> Dict:
         """Handle incoming analysis request"""
         content = message.content
         result = await self.analyze_data(
-            content.get('data_source'),
-            content.get('analysis_type', 'general')
+            content.get("data_source"), content.get("analysis_type", "general")
         )
         return result
 
 
 class A2AMessageBus:
     """Central message bus for agent communication"""
-    
+
     def __init__(self):
         self.agents = {}
         self.message_queue = asyncio.Queue()
         self.running = False
-        
+
     def register_agent(self, agent: BaseAgent):
         """Register agent with message bus"""
         self.agents[agent.agent_id] = agent
-        
+
     async def send(self, message: A2AMessage):
         """Send message through bus"""
         await self.message_queue.put(message)
-        
+
     async def start(self):
         """Start message processing"""
         self.running = True
         while self.running:
             try:
-                message = await asyncio.wait_for(
-                    self.message_queue.get(), 
-                    timeout=1.0
-                )
-                
+                message = await asyncio.wait_for(self.message_queue.get(), timeout=1.0)
+
                 # Deliver to recipient
                 recipient = self.agents.get(message.recipient)
                 if recipient:
@@ -250,12 +241,12 @@ class A2AMessageBus:
                 else:
                     # Log undeliverable message
                     print(f"Agent {message.recipient} not found")
-                    
+
             except asyncio.TimeoutError:
                 continue
             except Exception as e:
                 print(f"Error processing message: {e}")
-    
+
     def stop(self):
         """Stop message processing"""
         self.running = False
@@ -268,34 +259,31 @@ message_bus = A2AMessageBus()
 # Example usage
 async def demonstrate_a2a():
     """Demonstrate A2A communication"""
-    
+
     # Create agents
     negotiator = NegotiationAgent()
     analyst = DataAnalysisAgent()
-    
+
     # Register with message bus
     message_bus.register_agent(negotiator)
     message_bus.register_agent(analyst)
-    
+
     # Start message bus
     bus_task = asyncio.create_task(message_bus.start())
-    
+
     # Example: Analyst requests negotiation
     await analyst.send_message(
         recipient="negotiator",
         message_type="negotiate_request",
         content={
-            'topic': 'resource_allocation',
-            'requirements': {
-                'cpu': '4 cores',
-                'memory': '16GB'
-            }
-        }
+            "topic": "resource_allocation",
+            "requirements": {"cpu": "4 cores", "memory": "16GB"},
+        },
     )
-    
+
     # Let messages process
     await asyncio.sleep(1)
-    
+
     # Stop message bus
     message_bus.stop()
-    await bus_task 
\ No newline at end of file
+    await bus_task
diff --git a/agents/a2a_mcp_integration.py b/agents/a2a_mcp_integration.py
index d59bc17..bc6c51c 100644
--- a/agents/a2a_mcp_integration.py
+++ b/agents/a2a_mcp_integration.py
@@ -32,220 +32,250 @@ from connectors.mcp_base import MCPContext, MCPConnector
 
 logger = logging.getLogger(__name__)
 
+
 class MessagePriority(Enum):
     """Message priority levels for intelligent routing"""
+
     LOW = 0
     NORMAL = 1
     HIGH = 2
     CRITICAL = 3
 
+
 class TransportStrategy(Enum):
     """Transport strategies based on message characteristics"""
-    ZERO_COPY = "zero_copy"           # Same process, < 1MB
-    SHARED_MEMORY = "shared_memory"    # Same/different process, > 1MB
-    MCP_PIPE = "mcp_pipe"             # MCP-optimized transport
-    STANDARD = "standard"             # Fallback transport
+
+    ZERO_COPY = "zero_copy"  # Same process, < 1MB
+    SHARED_MEMORY = "shared_memory"  # Same/different process, > 1MB
+    MCP_PIPE = "mcp_pipe"  # MCP-optimized transport
+    STANDARD = "standard"  # Fallback transport
+
 
 @dataclass
 class A2AMCPMessage:
     """Enhanced message that combines A2A protocol with MCP context"""
+
     # A2A Layer
     a2a_message: A2AMessage
-    
+
     # MCP Layer
     mcp_context: MCPContext
-    
+
     # Transport Layer
     priority: MessagePriority = MessagePriority.NORMAL
     transport_strategy: TransportStrategy = TransportStrategy.STANDARD
     deadline_ms: Optional[float] = None
     performance_requirements: Dict[str, Any] = field(default_factory=dict)
-    
+
     def to_dict(self) -> Dict[str, Any]:
         return {
-            'a2a': self.a2a_message.to_dict(),
-            'mcp': self.mcp_context.to_dict(),
-            'transport': {
-                'priority': self.priority.value,
-                'strategy': self.transport_strategy.value,
-                'deadline_ms': self.deadline_ms,
-                'requirements': self.performance_requirements
-            }
+            "a2a": self.a2a_message.to_dict(),
+            "mcp": self.mcp_context.to_dict(),
+            "transport": {
+                "priority": self.priority.value,
+                "strategy": self.transport_strategy.value,
+                "deadline_ms": self.deadline_ms,
+                "requirements": self.performance_requirements,
+            },
         }
-    
+
     @classmethod
-    def from_dict(cls, data: Dict[str, Any]) -> 'A2AMCPMessage':
-        a2a_msg = A2AMessage.from_dict(data['a2a'])
+    def from_dict(cls, data: Dict[str, Any]) -> "A2AMCPMessage":
+        a2a_msg = A2AMessage.from_dict(data["a2a"])
         mcp_context = MCPContext()
-        mcp_context.from_dict(data['mcp'])
-        
+        mcp_context.from_dict(data["mcp"])
+
         return cls(
             a2a_message=a2a_msg,
             mcp_context=mcp_context,
-            priority=MessagePriority(data['transport']['priority']),
-            transport_strategy=TransportStrategy(data['transport']['strategy']),
-            deadline_ms=data['transport'].get('deadline_ms'),
-            performance_requirements=data['transport'].get('requirements', {})
+            priority=MessagePriority(data["transport"]["priority"]),
+            transport_strategy=TransportStrategy(data["transport"]["strategy"]),
+            deadline_ms=data["transport"].get("deadline_ms"),
+            performance_requirements=data["transport"].get("requirements", {}),
         )
 
+
 class MCPEnabledA2AAgent(BaseAgent):
     """
     Enhanced agent that uses MCP for context and A2A for communication.
     Integrates with the existing MCP server for tool access and context sharing.
     """
-    
-    def __init__(self, agent_id: str, capabilities: List[str], mcp_server_url: str = "http://localhost:8080"):
+
+    def __init__(
+        self,
+        agent_id: str,
+        capabilities: List[str],
+        mcp_server_url: str = "http://localhost:8080",
+    ):
         super().__init__(agent_id, capabilities)
         self.mcp_context = MCPContext()
         self.mcp_server_url = mcp_server_url
-        self.message_bus: Optional[A2AMessageBus] = None # Allow message bus injection
+        self.message_bus: Optional[A2AMessageBus] = None  # Allow message bus injection
         self.performance_stats = {
-            'messages_sent': 0,
-            'messages_received': 0,
-            'negotiations_completed': 0,
-            'avg_response_time_ms': 0.0,
-            'sla_violations': 0
+            "messages_sent": 0,
+            "messages_received": 0,
+            "negotiations_completed": 0,
+            "avg_response_time_ms": 0.0,
+            "sla_violations": 0,
         }
         self.sla_requirements = {
-            'max_latency_ms': 100,
-            'min_throughput_msgs_per_sec': 10,
-            'max_error_rate': 0.01
+            "max_latency_ms": 100,
+            "min_throughput_msgs_per_sec": 10,
+            "max_error_rate": 0.01,
         }
-        
+
         # Register handlers for common message types
         self.register_handler("negotiate_request", self.handle_negotiation_request)
         self.register_handler("context_share", self.handle_context_share)
         self.register_handler("tool_request", self.handle_tool_request)
-        self.register_handler("collaboration_request", self.handle_collaboration_request)
+        self.register_handler(
+            "collaboration_request", self.handle_collaboration_request
+        )
 
         # Initialize context attributes
         self.mcp_context.task = {}
         self.mcp_context.intent = {}
         self.mcp_context.history = []
-    
+
     async def process_intent(self, intent: Dict) -> Dict:
         """
         Process an intent and return result - required implementation of abstract method
         """
         try:
-            action = intent.get('action', 'unknown')
-            
-            if action == 'send_message':
-                recipient = intent.get('recipient')
+            action = intent.get("action", "unknown")
+
+            if action == "send_message":
+                recipient = intent.get("recipient")
                 if not recipient:
-                    return {'status': 'error', 'message': "recipient not specified for send_message intent"}
+                    return {
+                        "status": "error",
+                        "message": "recipient not specified for send_message intent",
+                    }
                 return await self.send_contextualized_message(
                     recipient=recipient,
                     intent=intent,
-                    priority=MessagePriority(intent.get('priority', 1)),
-                    deadline_ms=intent.get('deadline_ms')
+                    priority=MessagePriority(intent.get("priority", 1)),
+                    deadline_ms=intent.get("deadline_ms"),
                 )
-            elif action == 'analyze_data':
-                return await self._analyze_data(intent.get('data', {}))
-            elif action == 'generate_code':
-                return await self._generate_code(intent.get('data', {}))
-            elif action == 'negotiate':
+            elif action == "analyze_data":
+                return await self._analyze_data(intent.get("data", {}))
+            elif action == "generate_code":
+                return await self._generate_code(intent.get("data", {}))
+            elif action == "negotiate":
                 return await self.handle_negotiation_request(
                     A2AMessage(
-                        sender=intent.get('sender', 'unknown'),
+                        sender=intent.get("sender", "unknown"),
                         recipient=self.agent_id,
-                        message_type='negotiate_request',
-                        content=intent.get('data', {})
+                        message_type="negotiate_request",
+                        content=intent.get("data", {}),
                     )
                 )
-            elif action == 'tool_request':
-                tool_name = intent.get('tool_name')
+            elif action == "tool_request":
+                tool_name = intent.get("tool_name")
                 if not tool_name:
-                    return {'status': 'error', 'message': "tool_name not specified for tool_request intent"}
-                return await self._execute_mcp_tool(
-                    tool_name,
-                    intent.get('params', {})
-                )
+                    return {
+                        "status": "error",
+                        "message": "tool_name not specified for tool_request intent",
+                    }
+                return await self._execute_mcp_tool(tool_name, intent.get("params", {}))
             else:
                 return {
-                    'status': 'error',
-                    'message': f'Unknown intent action: {action}',
-                    'available_actions': ['send_message', 'analyze_data', 'generate_code', 'negotiate', 'tool_request']
+                    "status": "error",
+                    "message": f"Unknown intent action: {action}",
+                    "available_actions": [
+                        "send_message",
+                        "analyze_data",
+                        "generate_code",
+                        "negotiate",
+                        "tool_request",
+                    ],
                 }
-                
+
         except Exception as e:
             logger.error(f"Error processing intent: {e}")
-            return {
-                'status': 'error',
-                'message': str(e),
-                'intent': intent
-            }
-    
-    async def send_contextualized_message(self, 
-                                        recipient: str, 
-                                        intent: Dict[str, Any],
-                                        priority: MessagePriority = MessagePriority.NORMAL,
-                                        deadline_ms: Optional[float] = None) -> Dict[str, Any]:
+            return {"status": "error", "message": str(e), "intent": intent}
+
+    async def send_contextualized_message(
+        self,
+        recipient: str,
+        intent: Dict[str, Any],
+        priority: MessagePriority = MessagePriority.NORMAL,
+        deadline_ms: Optional[float] = None,
+    ) -> Dict[str, Any]:
         """
         Send message with MCP context and intelligent routing
         """
         start_time = time.time()
-        
+
         # 1. Build MCP context
-        self.mcp_context.task = {'intent': intent, 'recipient': recipient}
+        self.mcp_context.task = {"intent": intent, "recipient": recipient}
         self.mcp_context.intent = intent
-        self.mcp_context.history.append({
-            'timestamp': datetime.utcnow().isoformat(),
-            'action': 'send_message',
-            'recipient': recipient,
-            'intent': intent
-        })
-        
+        self.mcp_context.history.append(
+            {
+                "timestamp": datetime.utcnow().isoformat(),
+                "action": "send_message",
+                "recipient": recipient,
+                "intent": intent,
+            }
+        )
+
         # 2. Create A2A message
         message_type = f"{intent.get('action', 'message')}_request"
         a2a_msg = A2AMessage(
             sender=self.agent_id,
             recipient=recipient,
             message_type=message_type,
-            content=intent.get('data', {})
+            content=intent.get("data", {}),
         )
-        
+
         # 3. Create unified message
         unified_msg = A2AMCPMessage(
             a2a_message=a2a_msg,
             mcp_context=self.mcp_context,
             priority=priority,
             deadline_ms=deadline_ms,
-            performance_requirements=self.sla_requirements
+            performance_requirements=self.sla_requirements,
         )
-        
+
         # 4. Send through intelligent transport
         result = await self._send_with_intelligent_routing(unified_msg)
-        
+
         # 5. Update performance stats
         latency_ms = (time.time() - start_time) * 1000
-        self.performance_stats['messages_sent'] += 1
-        self.performance_stats['avg_response_time_ms'] = (
-            (self.performance_stats['avg_response_time_ms'] * (self.performance_stats['messages_sent'] - 1) + latency_ms) 
-            / self.performance_stats['messages_sent']
-        )
-        
+        self.performance_stats["messages_sent"] += 1
+        self.performance_stats["avg_response_time_ms"] = (
+            self.performance_stats["avg_response_time_ms"]
+            * (self.performance_stats["messages_sent"] - 1)
+            + latency_ms
+        ) / self.performance_stats["messages_sent"]
+
         # 6. Check SLA compliance
-        if latency_ms > self.sla_requirements['max_latency_ms']:
-            self.performance_stats['sla_violations'] += 1
-            logger.warning(f"SLA violation: {latency_ms:.2f}ms > {self.sla_requirements['max_latency_ms']}ms")
-        
+        if latency_ms > self.sla_requirements["max_latency_ms"]:
+            self.performance_stats["sla_violations"] += 1
+            logger.warning(
+                f"SLA violation: {latency_ms:.2f}ms > {self.sla_requirements['max_latency_ms']}ms"
+            )
+
         return {
-            'message_id': a2a_msg.id,
-            'recipient': recipient,
-            'latency_ms': latency_ms,
-            'transport_strategy': result.get('strategy'),
-            'status': result.get('status', 'sent')
+            "message_id": a2a_msg.id,
+            "recipient": recipient,
+            "latency_ms": latency_ms,
+            "transport_strategy": result.get("strategy"),
+            "status": result.get("status", "sent"),
         }
-    
-    async def _send_with_intelligent_routing(self, message: A2AMCPMessage) -> Dict[str, Any]:
+
+    async def _send_with_intelligent_routing(
+        self, message: A2AMCPMessage
+    ) -> Dict[str, Any]:
         """
         Intelligently route messages based on priority, size, and requirements
         """
         message_size = len(str(message.to_dict()))
-        
+
         # Determine optimal transport strategy
-        if message.priority == MessagePriority.CRITICAL or (message.deadline_ms and message.deadline_ms < 10):
+        if message.priority == MessagePriority.CRITICAL or (
+            message.deadline_ms and message.deadline_ms < 10
+        ):
             strategy = TransportStrategy.ZERO_COPY
         elif message_size > 1024 * 1024:  # > 1MB
             strategy = TransportStrategy.SHARED_MEMORY
@@ -253,9 +283,9 @@ class MCPEnabledA2AAgent(BaseAgent):
             strategy = TransportStrategy.ZERO_COPY
         else:
             strategy = TransportStrategy.MCP_PIPE
-        
+
         message.transport_strategy = strategy
-        
+
         # Send through appropriate transport
         if strategy == TransportStrategy.ZERO_COPY:
             return await self._send_zero_copy(message)
@@ -265,30 +295,22 @@ class MCPEnabledA2AAgent(BaseAgent):
             return await self._send_mcp_pipe(message)
         else:
             return await self._send_standard(message)
-    
+
     async def _send_zero_copy(self, message: A2AMCPMessage) -> Dict[str, Any]:
         """Zero-copy transfer for high-performance scenarios"""
         # In real implementation, this would use direct memory transfer
         # For now, simulate zero-copy behavior by directly calling receive on the bus
         if self.message_bus:
             await self.message_bus.send(message.a2a_message)
-        return {
-            'strategy': 'zero_copy',
-            'status': 'delivered',
-            'latency_ms': 0.1
-        }
-    
+        return {"strategy": "zero_copy", "status": "delivered", "latency_ms": 0.1}
+
     async def _send_shared_memory(self, message: A2AMCPMessage) -> Dict[str, Any]:
         """Shared memory transfer for large messages"""
         # Simulate shared memory transfer
         if self.message_bus:
             await self.message_bus.send(message.a2a_message)
-        return {
-            'strategy': 'shared_memory',
-            'status': 'delivered',
-            'latency_ms': 5.0
-        }
-    
+        return {"strategy": "shared_memory", "status": "delivered", "latency_ms": 5.0}
+
     async def _send_mcp_pipe(self, message: A2AMCPMessage) -> Dict[str, Any]:
         """MCP-optimized pipe transfer"""
         # Use MCP server for transport
@@ -296,346 +318,333 @@ class MCPEnabledA2AAgent(BaseAgent):
             # Send through MCP server (simulated)
             if self.message_bus:
                 await self.message_bus.send(message.a2a_message)
-            return {
-                'strategy': 'mcp_pipe',
-                'status': 'delivered',
-                'latency_ms': 2.0
-            }
+            return {"strategy": "mcp_pipe", "status": "delivered", "latency_ms": 2.0}
         except Exception as e:
             logger.error(f"MCP pipe transfer failed: {e}")
             return await self._send_standard(message)
-    
+
     async def _send_standard(self, message: A2AMCPMessage) -> Dict[str, Any]:
         """Standard transport fallback"""
         if self.message_bus:
             await self.message_bus.send(message.a2a_message)
-        return {
-            'strategy': 'standard',
-            'status': 'delivered',
-            'latency_ms': 10.0
-        }
-    
+        return {"strategy": "standard", "status": "delivered", "latency_ms": 10.0}
+
     async def handle_negotiation_request(self, message: A2AMessage) -> Dict[str, Any]:
         """Handle incoming negotiation request"""
         content = message.content
-        
+
         # Use MCP context to understand negotiation context
-        self.mcp_context.task = {'type': 'negotiation', 'topic': content.get('topic')}
-        
+        self.mcp_context.task = {"type": "negotiation", "topic": content.get("topic")}
+
         # Generate response based on agent capabilities
         response = {
-            'status': 'accepted',
-            'proposal': self._generate_negotiation_proposal(content),
-            'constraints': self._get_agent_constraints(),
-            'preferences': self._get_agent_preferences()
+            "status": "accepted",
+            "proposal": self._generate_negotiation_proposal(content),
+            "constraints": self._get_agent_constraints(),
+            "preferences": self._get_agent_preferences(),
         }
-        
-        self.performance_stats['negotiations_completed'] += 1
+
+        self.performance_stats["negotiations_completed"] += 1
         return response
-    
+
     async def handle_context_share(self, message: A2AMessage) -> Dict[str, Any]:
         """Handle MCP context sharing"""
         # Merge incoming context with local context
-        incoming_context = message.content.get('context', {})
-        
+        incoming_context = message.content.get("context", {})
+
         # Manually merge context fields
-        if isinstance(incoming_context.get('task'), dict):
-            self.mcp_context.task.update(incoming_context['task'])
-        if isinstance(incoming_context.get('intent'), dict):
-            self.mcp_context.intent.update(incoming_context['intent'])
-        if isinstance(incoming_context.get('history'), list):
-            self.mcp_context.history.extend(incoming_context['history'])
-            
+        if isinstance(incoming_context.get("task"), dict):
+            self.mcp_context.task.update(incoming_context["task"])
+        if isinstance(incoming_context.get("intent"), dict):
+            self.mcp_context.intent.update(incoming_context["intent"])
+        if isinstance(incoming_context.get("history"), list):
+            self.mcp_context.history.extend(incoming_context["history"])
+
         return {
-            'status': 'context_merged',
-            'local_context_size': len(str(self.mcp_context.to_dict()))
+            "status": "context_merged",
+            "local_context_size": len(str(self.mcp_context.to_dict())),
         }
-    
+
     async def handle_tool_request(self, message: A2AMessage) -> Dict[str, Any]:
         """Handle tool execution requests through MCP"""
-        tool_name = message.content.get('tool')
-        tool_params = message.content.get('params', {})
+        tool_name = message.content.get("tool")
+        tool_params = message.content.get("params", {})
 
         if not tool_name:
             return {
-                'status': 'tool_error',
-                'tool': None,
-                'error': 'Tool name not provided in request'
+                "status": "tool_error",
+                "tool": None,
+                "error": "Tool name not provided in request",
             }
-        
+
         # Execute tool through MCP server
         try:
             result = await self._execute_mcp_tool(tool_name, tool_params)
-            return {
-                'status': 'tool_executed',
-                'tool': tool_name,
-                'result': result
-            }
+            return {"status": "tool_executed", "tool": tool_name, "result": result}
         except Exception as e:
-            return {
-                'status': 'tool_error',
-                'tool': tool_name,
-                'error': str(e)
-            }
-    
+            return {"status": "tool_error", "tool": tool_name, "error": str(e)}
+
     async def handle_collaboration_request(self, message: A2AMessage) -> Dict[str, Any]:
         """Handle collaboration requests"""
-        collaboration_type = message.content.get('type')
-        data = message.content.get('data', {})
-        
+        collaboration_type = message.content.get("type")
+        data = message.content.get("data", {})
+
         # Process collaboration based on agent capabilities
-        if collaboration_type == 'data_analysis' and 'analyze' in self.capabilities:
+        if collaboration_type == "data_analysis" and "analyze" in self.capabilities:
             result = await self._analyze_data(data)
-        elif collaboration_type == 'code_generation' and 'generate' in self.capabilities:
+        elif (
+            collaboration_type == "code_generation" and "generate" in self.capabilities
+        ):
             result = await self._generate_code(data)
         else:
-            result = {'status': 'capability_not_available'}
-        
+            result = {"status": "capability_not_available"}
+
         return result
-    
-    async def _execute_mcp_tool(self, tool_name: str, params: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def _execute_mcp_tool(
+        self, tool_name: str, params: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Execute tool through MCP server"""
         # This would make actual HTTP calls to the MCP server
         # For now, simulate tool execution
-        if tool_name == 'code_analyzer':
-            return {'lines_of_code': 10, 'complexity': 'low'}
-        elif tool_name == 'protocol_validator':
-            return {'valid': True, 'issues': []}
+        if tool_name == "code_analyzer":
+            return {"lines_of_code": 10, "complexity": "low"}
+        elif tool_name == "protocol_validator":
+            return {"valid": True, "issues": []}
         else:
-            return {'status': 'unknown_tool'}
-    
+            return {"status": "unknown_tool"}
+
     def _generate_negotiation_proposal(self, content: Dict[str, Any]) -> Dict[str, Any]:
         """Generate negotiation proposal based on agent capabilities"""
         return {
-            'resources': self._get_available_resources(),
-            'capabilities': self.capabilities,
-            'terms': {'duration': 'flexible', 'priority': 'normal'}
+            "resources": self._get_available_resources(),
+            "capabilities": self.capabilities,
+            "terms": {"duration": "flexible", "priority": "normal"},
         }
-    
+
     def _get_agent_constraints(self) -> Dict[str, Any]:
         """Get agent-specific constraints"""
         return {
-            'max_concurrent_tasks': 5,
-            'memory_limit_mb': 1024,
-            'cpu_limit_cores': 2
+            "max_concurrent_tasks": 5,
+            "memory_limit_mb": 1024,
+            "cpu_limit_cores": 2,
         }
-    
+
     def _get_agent_preferences(self) -> Dict[str, Any]:
         """Get agent preferences"""
         return {
-            'preferred_transport': 'mcp_pipe',
-            'max_latency_ms': self.sla_requirements['max_latency_ms'],
-            'error_tolerance': 'low'
+            "preferred_transport": "mcp_pipe",
+            "max_latency_ms": self.sla_requirements["max_latency_ms"],
+            "error_tolerance": "low",
         }
-    
+
     def _get_available_resources(self) -> Dict[str, Any]:
         """Get available resources for negotiation"""
-        return {
-            'cpu_cores': 4,
-            'memory_mb': 2048,
-            'storage_gb': 100
-        }
-    
+        return {"cpu_cores": 4, "memory_mb": 2048, "storage_gb": 100}
+
     async def _analyze_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
         """Analyze data (placeholder for specialized agents)"""
         return {
-            'analysis_type': 'basic',
-            'insights': ['Pattern detected', 'Anomaly found'],
-            'confidence': 0.85
+            "analysis_type": "basic",
+            "insights": ["Pattern detected", "Anomaly found"],
+            "confidence": 0.85,
         }
-    
+
     async def _generate_code(self, data: Dict[str, Any]) -> Dict[str, Any]:
         """Generate code (placeholder for specialized agents)"""
         return {
-            'code_type': 'function',
-            'language': 'python',
-            'code': 'def example(): pass'
+            "code_type": "function",
+            "language": "python",
+            "code": "def example(): pass",
         }
 
+
 class A2AMCPOrchestrator:
     """
     Orchestrates A2A communication with MCP integration.
     Manages agent registration, message routing, and performance monitoring.
     """
-    
+
     def __init__(self):
         self.agents: Dict[str, MCPEnabledA2AAgent] = {}
         self.message_bus = A2AMessageBus()
         self.performance_monitor = PerformanceMonitor()
         self.negotiation_manager = NegotiationManager()
-        
-        self.message_bus.agents = self.agents # Link agents to message bus
-        
+
+        self.message_bus.agents = self.agents  # Link agents to message bus
+
     def register_agent(self, agent: MCPEnabledA2AAgent):
         """Register agent with orchestrator"""
         self.agents[agent.agent_id] = agent
-        agent.message_bus = self.message_bus # Inject message bus into agent
+        agent.message_bus = self.message_bus  # Inject message bus into agent
         logger.info(f"Registered agent: {agent.agent_id}")
-    
+
     async def start(self):
         """Start the A2A MCP orchestrator"""
         logger.info("Starting A2A MCP Orchestrator...")
-        
+
         # Start message bus
         bus_task = asyncio.create_task(self.message_bus.start())
-        
+
         # Start performance monitoring
         monitor_task = asyncio.create_task(self.performance_monitor.start())
-        
+
         # Start negotiation manager
         negotiation_task = asyncio.create_task(self.negotiation_manager.start())
-        
+
         return bus_task, monitor_task, negotiation_task
-    
+
     async def stop(self):
         """Stop the orchestrator"""
         self.message_bus.stop()
         self.performance_monitor.stop()
         self.negotiation_manager.stop()
         logger.info("A2A MCP Orchestrator stopped")
-    
+
     def get_agent(self, agent_id: str) -> Optional[MCPEnabledA2AAgent]:
         """Get registered agent by ID"""
         return self.agents.get(agent_id)
-    
+
     def list_agents(self) -> List[str]:
         """List all registered agent IDs"""
         return list(self.agents.keys())
-    
+
     def get_performance_stats(self) -> Dict[str, Any]:
         """Get overall performance statistics"""
         return self.performance_monitor.get_stats()
 
+
 class PerformanceMonitor:
     """Monitors performance metrics across all agents"""
-    
+
     def __init__(self):
         self.stats = {
-            'total_messages': 0,
-            'avg_latency_ms': 0.0,
-            'sla_violations': 0,
-            'active_agents': 0
+            "total_messages": 0,
+            "avg_latency_ms": 0.0,
+            "sla_violations": 0,
+            "active_agents": 0,
         }
         self.running = False
-    
+
     async def start(self):
         """Start performance monitoring"""
         self.running = True
         while self.running:
             await self._update_stats()
             await asyncio.sleep(5)  # Update every 5 seconds
-    
+
     def stop(self):
         """Stop performance monitoring"""
         self.running = False
-    
+
     async def _update_stats(self):
         """Update performance statistics"""
         # This would collect stats from all agents
         pass
-    
+
     def get_stats(self) -> Dict[str, Any]:
         """Get current performance statistics"""
         return self.stats.copy()
 
+
 class NegotiationManager:
     """Manages multi-agent negotiations"""
-    
+
     def __init__(self):
         self.active_negotiations: Dict[str, Dict[str, Any]] = {}
         self.running = False
-    
+
     async def start(self):
         """Start negotiation manager"""
         self.running = True
         while self.running:
             await self._process_negotiations()
             await asyncio.sleep(1)
-    
+
     def stop(self):
         """Stop negotiation manager"""
         self.running = False
-    
+
     async def _process_negotiations(self):
         """Process active negotiations"""
         # This would handle ongoing negotiations
         pass
 
+
 # Global orchestrator instance
 a2a_mcp_orchestrator = A2AMCPOrchestrator()
 
+
 # Example usage and demonstration
 async def demonstrate_a2a_mcp_integration():
     """Demonstrate A2A MCP integration"""
-    
+
     print("=== A2A MCP Integration Demo ===\n")
-    
+
     # Create agents
     analyzer = MCPEnabledA2AAgent("data_analyzer", ["analyze", "process"])
     generator = MCPEnabledA2AAgent("code_generator", ["generate", "create"])
     negotiator = MCPEnabledA2AAgent("negotiator", ["negotiate", "coordinate"])
-    
+
     # Register agents
     a2a_mcp_orchestrator.register_agent(analyzer)
     a2a_mcp_orchestrator.register_agent(generator)
     a2a_mcp_orchestrator.register_agent(negotiator)
-    
+
     # Start orchestrator
     bus_task, monitor_task, negotiation_task = await a2a_mcp_orchestrator.start()
-    
+
     # Demo 1: Contextualized message sending
     print("1. Sending contextualized message:")
     result = await analyzer.send_contextualized_message(
         recipient="code_generator",
         intent={
-            'action': 'generate_code',
-            'data': {
-                'type': 'api_endpoint',
-                'language': 'python',
-                'framework': 'fastapi'
-            }
+            "action": "generate_code",
+            "data": {
+                "type": "api_endpoint",
+                "language": "python",
+                "framework": "fastapi",
+            },
         },
-        priority=MessagePriority.HIGH
+        priority=MessagePriority.HIGH,
     )
     print(f"   - Latency: {result['latency_ms']:.2f}ms")
     print(f"   - Strategy: {result['transport_strategy']}")
     print(f"   - Status: {result['status']}\n")
-    
+
     # Demo 2: Multi-agent negotiation
     print("2. Multi-agent negotiation:")
     negotiation_result = await negotiator.send_contextualized_message(
         recipient="data_analyzer",
         intent={
-            'action': 'negotiate',
-            'data': {
-                'topic': 'resource_allocation',
-                'participants': ['data_analyzer', 'code_generator'],
-                'requirements': {
-                    'cpu_cores': 4,
-                    'memory_mb': 2048
-                }
-            }
+            "action": "negotiate",
+            "data": {
+                "topic": "resource_allocation",
+                "participants": ["data_analyzer", "code_generator"],
+                "requirements": {"cpu_cores": 4, "memory_mb": 2048},
+            },
         },
         priority=MessagePriority.CRITICAL,
-        deadline_ms=50  # 50ms deadline
+        deadline_ms=50,  # 50ms deadline
     )
     print(f"   - Latency: {negotiation_result['latency_ms']:.2f}ms")
     print(f"   - Strategy: {negotiation_result['transport_strategy']}\n")
-    
+
     # Demo 3: Performance monitoring
     print("3. Performance monitoring:")
     stats = a2a_mcp_orchestrator.get_performance_stats()
     print(f"   - Active agents: {len(a2a_mcp_orchestrator.list_agents())}")
     print(f"   - Total messages: {stats['total_messages']}")
     print(f"   - SLA violations: {stats['sla_violations']}\n")
-    
+
     # Stop orchestrator
     await a2a_mcp_orchestrator.stop()
     await bus_task
     await monitor_task
     await negotiation_task
-    
+
     print("✅ A2A MCP Integration Demo Complete!")
 
+
 if __name__ == "__main__":
-    asyncio.run(demonstrate_a2a_mcp_integration()) 
\ No newline at end of file
+    asyncio.run(demonstrate_a2a_mcp_integration())
diff --git a/agents/executor.py b/agents/executor.py
index 3387c36..607f0dd 100644
--- a/agents/executor.py
+++ b/agents/executor.py
@@ -2,34 +2,35 @@
 from protocols.loader import load_protocol
 from utils.logger import log
 
-def execute_task(protocol_name='default_protocol'):
+
+def execute_task(protocol_name="default_protocol"):
     """Execute a specific protocol and return the outcome"""
     # Try database tracker first, fall back to file tracker
     try:
         from utils.db_tracker import track_outcome
     except:
         from utils.tracker import track_outcome
-    
+
     log(f"Executing protocol: {protocol_name}")
-    
+
     # Load the protocol
     protocol = load_protocol(protocol_name)
     if not protocol:
         log(f"Failed to load protocol: {protocol_name}")
-        return {'success': False, 'error': 'Protocol not found'}
-    
+        return {"success": False, "error": "Protocol not found"}
+
     # Execute the protocol's task function
     try:
-        outcome = protocol['task']()
-        
+        outcome = protocol["task"]()
+
         # Track the outcome
         track_outcome(protocol_name, outcome)
-        
+
         log(f"Protocol {protocol_name} completed with outcome: {outcome}")
         return outcome
-        
+
     except Exception as e:
         log(f"Protocol {protocol_name} failed with error: {e}")
-        error_outcome = {'success': False, 'error': str(e)}
+        error_outcome = {"success": False, "error": str(e)}
         track_outcome(protocol_name, error_outcome)
-        return error_outcome
\ No newline at end of file
+        return error_outcome
diff --git a/agents/mutator.py b/agents/mutator.py
index 269b900..ace5472 100644
--- a/agents/mutator.py
+++ b/agents/mutator.py
@@ -3,43 +3,52 @@ import json
 import os
 from utils.logger import log
 
+
 def mutate_protocol(protocol_name):
     """Analyze protocol outcomes and mutate if failure rate is high"""
     # Try database stats first, fall back to file stats
     try:
         from utils.db_tracker import get_protocol_stats, track_mutation
+
         stats = get_protocol_stats(protocol_name)
     except:
         from utils.tracker import get_protocol_stats
+
         stats = get_protocol_stats(protocol_name)
         track_mutation = None  # No mutation tracking in file mode
-    
+
     if not stats:
         log(f"No stats found for {protocol_name}. Nothing to mutate.")
         return False
-    
-    failure_rate = stats['failure_rate']
-    log(f"Protocol {protocol_name}: {stats['failures']}/{stats['total_executions']} failures (rate: {failure_rate:.2%})")
-    
+
+    failure_rate = stats["failure_rate"]
+    log(
+        f"Protocol {protocol_name}: {stats['failures']}/{stats['total_executions']} failures (rate: {failure_rate:.2%})"
+    )
+
     if failure_rate < 0.5:
         log(f"Mutation not needed. Failure rate acceptable: {failure_rate:.2%}")
         return False
 
     # Rewrite protocol file with improved logic
-    protocol_file = f'protocols/{protocol_name}.py'
+    protocol_file = f"protocols/{protocol_name}.py"
     if not os.path.exists(protocol_file):
         log(f"No protocol source file found for {protocol_name}")
         return False
 
-    log(f"🔄 Mutating protocol: {protocol_name} due to high failure rate ({failure_rate:.2%})")
+    log(
+        f"🔄 Mutating protocol: {protocol_name} due to high failure rate ({failure_rate:.2%})"
+    )
 
     # Read original code for backup
-    with open(protocol_file, 'r') as f:
+    with open(protocol_file, "r") as f:
         original_code = f.read()
 
     # Check if this is a simple random protocol or a complex one
-    is_simple_protocol = any(keyword in original_code for keyword in ['randint', 'random()', 'bool(randint'])
-    
+    is_simple_protocol = any(
+        keyword in original_code for keyword in ["randint", "random()", "bool(randint"]
+    )
+
     if is_simple_protocol:
         # Generate improved random logic
         mutated_logic = f'''# Mutated protocol file - Auto-generated by ProtocolMutator
@@ -60,26 +69,30 @@ def task():
 '''
     else:
         # Don't mutate complex protocols, just add a comment
-        log(f"Protocol {protocol_name} appears to be a complex protocol, skipping automatic mutation")
+        log(
+            f"Protocol {protocol_name} appears to be a complex protocol, skipping automatic mutation"
+        )
         return False
 
     try:
         # Backup original protocol
-        backup_file = f'{protocol_file}.backup'
-        with open(backup_file, 'w') as backup:
+        backup_file = f"{protocol_file}.backup"
+        with open(backup_file, "w") as backup:
             backup.write(original_code)
 
         # Write mutated protocol
-        with open(protocol_file, 'w') as f:
+        with open(protocol_file, "w") as f:
             f.write(mutated_logic)
-        
+
         # Track mutation in database if available
         if track_mutation:
             track_mutation(protocol_name, failure_rate, mutated_logic, original_code)
-        
-        log(f"✅ Protocol {protocol_name} mutated successfully. Backup saved to {backup_file}")
+
+        log(
+            f"✅ Protocol {protocol_name} mutated successfully. Backup saved to {backup_file}"
+        )
         return True
-        
+
     except Exception as e:
         log(f"❌ Failed to mutate protocol {protocol_name}: {e}")
         return False
@@ -90,16 +103,18 @@ def analyze_all_protocols():
     # Try database stats first, fall back to file stats
     try:
         from utils.db_tracker import get_all_stats
+
         all_stats = get_all_stats()
     except:
         from utils.tracker import get_all_stats
+
         all_stats = get_all_stats()
-    
+
     mutated_count = 0
     for stats in all_stats:
-        protocol_name = stats['protocol']
+        protocol_name = stats["protocol"]
         if mutate_protocol(protocol_name):
             mutated_count += 1
-    
+
     log(f"Analysis complete. {mutated_count} protocols were mutated.")
-    return mutated_count 
\ No newline at end of file
+    return mutated_count
diff --git a/agents/specialized/__init__.py b/agents/specialized/__init__.py
index f85dd3f..6e417ad 100644
--- a/agents/specialized/__init__.py
+++ b/agents/specialized/__init__.py
@@ -2,4 +2,4 @@
 from .code_generator import code_generator_agent
 from .filesystem_agent import file_system_agent
 
-__all__ = ['code_generator_agent', 'file_system_agent'] 
\ No newline at end of file
+__all__ = ["code_generator_agent", "file_system_agent"]
diff --git a/agents/specialized/code_generator.py b/agents/specialized/code_generator.py
index 88c63c0..42e80ee 100644
--- a/agents/specialized/code_generator.py
+++ b/agents/specialized/code_generator.py
@@ -6,17 +6,19 @@ import textwrap
 from datetime import datetime
 from typing import Dict, Any
 
+
 class CodeGeneratorAgent:
     """Agent specialized in generating code based on intent"""
-    
+
     def __init__(self):
         self.name = "code_generator"
         self.templates = self._load_templates()
-        
+
     def _load_templates(self) -> Dict[str, str]:
         """Load code generation templates"""
         return {
-            "fastapi_endpoint": textwrap.dedent("""
+            "fastapi_endpoint": textwrap.dedent(
+                """
                 @app.post("/api/v1/{endpoint_name}")
                 async def {function_name}({parameters}):
                     \"\"\"
@@ -39,9 +41,10 @@ class CodeGeneratorAgent:
                         raise HTTPException(status_code=400, detail=str(e))
                     except Exception as e:
                         raise HTTPException(status_code=500, detail=str(e))
-            """),
-            
-            "rest_api": textwrap.dedent("""
+            """
+            ),
+            "rest_api": textwrap.dedent(
+                """
                 # {title}
                 # Generated API endpoint
                 
@@ -53,9 +56,10 @@ class CodeGeneratorAgent:
                 {models}
                 
                 {endpoints}
-            """),
-            
-            "crud_operations": textwrap.dedent("""
+            """
+            ),
+            "crud_operations": textwrap.dedent(
+                """
                 # CRUD operations for {entity}
                 
                 @app.post("/{entity_plural}")
@@ -81,57 +85,58 @@ class CodeGeneratorAgent:
                     \"\"\"Delete {entity}\"\"\"
                     # Implementation here
                     pass
-            """)
+            """
+            ),
         }
-    
+
     async def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         """Execute code generation based on inputs"""
-        intent = inputs.get('intent', '')
-        context = inputs.get('context', {})
-        
+        intent = inputs.get("intent", "")
+        context = inputs.get("context", {})
+
         # Parse the intent to understand what to generate
         generation_type = self._parse_intent(intent)
-        
+
         # Generate appropriate code
-        if generation_type == 'api_endpoint':
+        if generation_type == "api_endpoint":
             code = self._generate_api_endpoint(context)
-        elif generation_type == 'crud':
+        elif generation_type == "crud":
             code = self._generate_crud_api(context)
-        elif generation_type == 'data_model':
+        elif generation_type == "data_model":
             code = self._generate_data_model(context)
         else:
             code = self._generate_generic_api(context)
-            
+
         return {
-            'success': True,
-            'generated_code': code,
-            'generation_type': generation_type,
-            'files_created': self._get_file_list(code),
-            'instructions': self._get_implementation_instructions(generation_type),
-            'timestamp': datetime.utcnow().isoformat()
+            "success": True,
+            "generated_code": code,
+            "generation_type": generation_type,
+            "files_created": self._get_file_list(code),
+            "instructions": self._get_implementation_instructions(generation_type),
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     def _parse_intent(self, intent: str) -> str:
         """Parse intent to determine generation type"""
         intent_lower = intent.lower()
-        
-        if 'crud' in intent_lower:
-            return 'crud'
-        elif 'model' in intent_lower or 'schema' in intent_lower:
-            return 'data_model'
-        elif 'endpoint' in intent_lower or 'api' in intent_lower:
-            return 'api_endpoint'
+
+        if "crud" in intent_lower:
+            return "crud"
+        elif "model" in intent_lower or "schema" in intent_lower:
+            return "data_model"
+        elif "endpoint" in intent_lower or "api" in intent_lower:
+            return "api_endpoint"
         else:
-            return 'generic'
-    
+            return "generic"
+
     def _generate_api_endpoint(self, context: Dict) -> str:
         """Generate a single API endpoint"""
-        endpoint_name = context.get('endpoint_name', 'process')
-        function_name = context.get('function_name', endpoint_name.replace('-', '_'))
-        description = context.get('description', f'Process {endpoint_name} request')
-        
+        endpoint_name = context.get("endpoint_name", "process")
+        function_name = context.get("function_name", endpoint_name.replace("-", "_"))
+        description = context.get("description", f"Process {endpoint_name} request")
+
         # Generate parameter list
-        params = context.get('parameters', {})
+        params = context.get("parameters", {})
         if params:
             param_list = []
             for name, ptype in params.items():
@@ -139,7 +144,7 @@ class CodeGeneratorAgent:
             parameters = ", ".join(param_list)
         else:
             parameters = "request: Dict"
-            
+
         # Generate validation logic
         validation_logic = "# Validate required fields\n        "
         if params:
@@ -147,29 +152,29 @@ class CodeGeneratorAgent:
                 validation_logic += f"if not {param}:\n            raise ValidationError('{param} is required')\n        "
         else:
             validation_logic += "pass"
-            
+
         # Generate processing logic
         processing_logic = """# Main processing logic
         result = {
             'processed': True,
             'data': request
         }"""
-        
-        return self.templates['fastapi_endpoint'].format(
+
+        return self.templates["fastapi_endpoint"].format(
             endpoint_name=endpoint_name,
             function_name=function_name,
             parameters=parameters,
             description=description,
             validation_logic=validation_logic,
-            processing_logic=processing_logic
+            processing_logic=processing_logic,
         )
-    
+
     def _generate_crud_api(self, context: Dict) -> str:
         """Generate CRUD API endpoints"""
-        entity = context.get('entity', 'item')
-        entity_plural = context.get('entity_plural', f"{entity}s")
-        model_name = context.get('model_name', f"{entity.capitalize()}Model")
-        
+        entity = context.get("entity", "item")
+        entity_plural = context.get("entity_plural", f"{entity}s")
+        model_name = context.get("model_name", f"{entity.capitalize()}Model")
+
         # Generate model
         model_code = f"""
 class {model_name}(BaseModel):
@@ -179,47 +184,45 @@ class {model_name}(BaseModel):
     created_at: Optional[datetime] = None
     updated_at: Optional[datetime] = None
 """
-        
+
         # Generate CRUD endpoints
-        crud_code = self.templates['crud_operations'].format(
-            entity=entity,
-            entity_plural=entity_plural,
-            model_name=model_name
+        crud_code = self.templates["crud_operations"].format(
+            entity=entity, entity_plural=entity_plural, model_name=model_name
         )
-        
+
         # Combine into full API
-        return self.templates['rest_api'].format(
+        return self.templates["rest_api"].format(
             title=f"{entity.capitalize()} Management API",
             models=model_code,
-            endpoints=crud_code
+            endpoints=crud_code,
         )
-    
+
     def _generate_data_model(self, context: Dict) -> str:
         """Generate Pydantic data models"""
-        model_name = context.get('model_name', 'DataModel')
-        fields = context.get('fields', {
-            'id': 'int',
-            'name': 'str',
-            'created_at': 'datetime'
-        })
-        
+        model_name = context.get("model_name", "DataModel")
+        fields = context.get(
+            "fields", {"id": "int", "name": "str", "created_at": "datetime"}
+        )
+
         model_code = f"class {model_name}(BaseModel):\n"
         for field_name, field_type in fields.items():
-            optional = "Optional[" if field_name != 'id' else ""
+            optional = "Optional[" if field_name != "id" else ""
             close_bracket = "]" if optional else ""
             default = " = None" if optional else ""
-            model_code += f"    {field_name}: {optional}{field_type}{close_bracket}{default}\n"
-            
+            model_code += (
+                f"    {field_name}: {optional}{field_type}{close_bracket}{default}\n"
+            )
+
         return model_code
-    
+
     def _generate_generic_api(self, context: Dict) -> str:
         """Generate a generic API structure"""
-        return self.templates['rest_api'].format(
+        return self.templates["rest_api"].format(
             title="Generated API",
             models=self._generate_data_model(context),
-            endpoints=self._generate_api_endpoint(context)
+            endpoints=self._generate_api_endpoint(context),
         )
-    
+
     def _get_file_list(self, code: str) -> list:
         """Determine which files would be created"""
         files = []
@@ -228,17 +231,19 @@ class {model_name}(BaseModel):
         if "class" in code and "BaseModel" in code:
             files.append("models.py")
         return files
-    
+
     def _get_implementation_instructions(self, generation_type: str) -> str:
         """Get instructions for implementing generated code"""
         instructions = {
-            'api_endpoint': "Add this endpoint to your main FastAPI application (mcp/main.py)",
-            'crud': "Create a new file 'crud_api.py' and import it in your main application",
-            'data_model': "Add this model to your models.py file or create one if it doesn't exist",
-            'generic': "Integrate this code into your existing API structure"
+            "api_endpoint": "Add this endpoint to your main FastAPI application (mcp/main.py)",
+            "crud": "Create a new file 'crud_api.py' and import it in your main application",
+            "data_model": "Add this model to your models.py file or create one if it doesn't exist",
+            "generic": "Integrate this code into your existing API structure",
         }
-        return instructions.get(generation_type, "Review and integrate the generated code")
+        return instructions.get(
+            generation_type, "Review and integrate the generated code"
+        )
 
 
 # Export the agent
-code_generator_agent = CodeGeneratorAgent() 
\ No newline at end of file
+code_generator_agent = CodeGeneratorAgent()
diff --git a/agents/specialized/filesystem_agent.py b/agents/specialized/filesystem_agent.py
index 4b5745a..95e5c0d 100644
--- a/agents/specialized/filesystem_agent.py
+++ b/agents/specialized/filesystem_agent.py
@@ -5,14 +5,15 @@ import os
 from datetime import datetime
 from typing import Dict, Any
 
+
 class FileSystemAgent:
     """Agent for safe, read-only file system operations."""
-    
+
     def __init__(self, base_path="/app"):
         self.name = "file_system_agent"
         # Security: Ensure all operations are constrained to this base path.
         self.base_path = os.path.abspath(base_path)
-        
+
     def _is_safe_path(self, path: str) -> bool:
         """Security check to prevent path traversal attacks."""
         requested_path = os.path.abspath(os.path.join(self.base_path, path))
@@ -20,76 +21,81 @@ class FileSystemAgent:
 
     async def execute(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         """Execute file system action based on inputs."""
-        action = inputs.get('action')
-        path = inputs.get('path', '.')
+        action = inputs.get("action")
+        path = inputs.get("path", ".")
 
         if not self._is_safe_path(path):
             return {
-                'success': False,
-                'error': 'Access denied: Path is outside the allowed project directory.',
-                'timestamp': datetime.utcnow().isoformat()
+                "success": False,
+                "error": "Access denied: Path is outside the allowed project directory.",
+                "timestamp": datetime.utcnow().isoformat(),
             }
 
         full_path = os.path.join(self.base_path, path)
 
-        if action == 'list_directory':
+        if action == "list_directory":
             return await self._list_directory(full_path)
-        elif action == 'read_file':
+        elif action == "read_file":
             return await self._read_file(full_path)
         else:
             return {
-                'success': False,
-                'error': f"Unknown file system action: {action}",
-                'timestamp': datetime.utcnow().isoformat()
+                "success": False,
+                "error": f"Unknown file system action: {action}",
+                "timestamp": datetime.utcnow().isoformat(),
             }
 
     async def _list_directory(self, path: str) -> Dict[str, Any]:
         """List contents of a directory."""
         try:
             if not os.path.isdir(path):
-                return {'success': False, 'error': 'Not a directory'}
+                return {"success": False, "error": "Not a directory"}
 
             items = []
             for item in os.listdir(path):
                 item_path = os.path.join(path, item)
-                if not item.startswith('.') and not item.startswith('__'): # Exclude hidden files/dirs
+                if not item.startswith(".") and not item.startswith(
+                    "__"
+                ):  # Exclude hidden files/dirs
                     is_dir = os.path.isdir(item_path)
-                    items.append({
-                        'name': item,
-                        'path': os.path.relpath(item_path, self.base_path),
-                        'type': 'directory' if is_dir else 'file'
-                    })
-            
+                    items.append(
+                        {
+                            "name": item,
+                            "path": os.path.relpath(item_path, self.base_path),
+                            "type": "directory" if is_dir else "file",
+                        }
+                    )
+
             # Sort with directories first
-            items.sort(key=lambda x: (x['type'] != 'directory', x['name']))
+            items.sort(key=lambda x: (x["type"] != "directory", x["name"]))
 
             return {
-                'success': True,
-                'path': os.path.relpath(path, self.base_path),
-                'items': items,
-                'timestamp': datetime.utcnow().isoformat()
+                "success": True,
+                "path": os.path.relpath(path, self.base_path),
+                "items": items,
+                "timestamp": datetime.utcnow().isoformat(),
             }
         except Exception as e:
-            return {'success': False, 'error': str(e)}
+            return {"success": False, "error": str(e)}
 
     async def _read_file(self, path: str) -> Dict[str, Any]:
         """Read content of a file."""
         try:
             if not os.path.isfile(path):
-                return {'success': False, 'error': 'Not a file'}
-            
-            with open(path, 'r', encoding='utf-8') as f:
+                return {"success": False, "error": "Not a file"}
+
+            with open(path, "r", encoding="utf-8") as f:
                 content = f.read()
 
             return {
-                'success': True,
-                'path': os.path.relpath(path, self.base_path),
-                'content': content,
-                'size_bytes': os.path.getsize(path),
-                'timestamp': datetime.utcnow().isoformat()
+                "success": True,
+                "path": os.path.relpath(path, self.base_path),
+                "content": content,
+                "size_bytes": os.path.getsize(path),
+                "timestamp": datetime.utcnow().isoformat(),
             }
         except Exception as e:
-            return {'success': False, 'error': str(e)}
+            return {"success": False, "error": str(e)}
+
 
 # Export the agent instance
-file_system_agent = FileSystemAgent() 
\ No newline at end of file
+file_system_agent = FileSystemAgent()
diff --git a/agents/unified/mcp_a2a_mojo_integration.py b/agents/unified/mcp_a2a_mojo_integration.py
index 3f78481..955e51d 100644
--- a/agents/unified/mcp_a2a_mojo_integration.py
+++ b/agents/unified/mcp_a2a_mojo_integration.py
@@ -2,11 +2,11 @@
 Unified MCP-A2A-Mojo Integration
 ================================
 
-This module demonstrates how three complementary technologies create a 
+This module demonstrates how three complementary technologies create a
 sophisticated, high-performance agent runtime:
 
 1. MCP (Model Context Protocol) - The BRAIN - Semantic understanding
-2. A2A (Agent-to-Agent) - The NERVOUS SYSTEM - Agent coordination  
+2. A2A (Agent-to-Agent) - The NERVOUS SYSTEM - Agent coordination
 3. Mojo - The CIRCULATORY SYSTEM - High-speed transport
 
 Together they form an intelligent, integrated system where each layer
@@ -28,9 +28,10 @@ from connectors.mcp_base import MCPContext, MCPConnector
 
 class TransportStrategy(Enum):
     """Intelligent transport selection based on context"""
-    ZERO_COPY = "zero_copy"           # Same process, < 1MB
-    SHARED_MEMORY = "shared_memory"    # Same/different process, > 1MB
-    MOJO_PIPE = "mojo_pipe"           # Different process, < 10KB
+
+    ZERO_COPY = "zero_copy"  # Same process, < 1MB
+    SHARED_MEMORY = "shared_memory"  # Same/different process, > 1MB
+    MOJO_PIPE = "mojo_pipe"  # Different process, < 10KB
     HANDLE_PASSING = "handle_passing"  # Resources/GPU memory
 
 
@@ -42,44 +43,49 @@ class UnifiedMessage:
     - A2A protocol for agent coordination
     - Mojo transport hints for performance
     """
+
     # A2A Layer
     a2a_message: A2AMessage
-    
+
     # MCP Layer
     mcp_context: MCPContext
-    
+
     # Mojo Layer hints
     transport_strategy: TransportStrategy
     priority: int = 0  # 0 = normal, 1 = high, 2 = critical
     deadline_ms: Optional[float] = None
     resource_handles: List[Any] = None
-    
+
     def requires_zero_copy(self) -> bool:
         """Determine if message requires zero-copy transport"""
         # Critical messages or those with tight deadlines
         if self.priority >= 2 or (self.deadline_ms and self.deadline_ms < 1.0):
             return True
-        
+
         # Large contexts benefit from zero-copy
         context_size = len(str(self.mcp_context.to_dict()))
         return context_size > 100_000  # 100KB threshold
-    
-    def select_optimal_transport(self, sender_pid: int, receiver_pid: int) -> TransportStrategy:
+
+    def select_optimal_transport(
+        self, sender_pid: int, receiver_pid: int
+    ) -> TransportStrategy:
         """Intelligently select transport based on all factors"""
         same_process = sender_pid == receiver_pid
-        message_size = len(str(self.a2a_message.to_dict())) + len(str(self.mcp_context.to_dict()))
-        
+        message_size = len(str(self.a2a_message.to_dict())) + len(
+            str(self.mcp_context.to_dict())
+        )
+
         # Resource handles always use handle passing
         if self.resource_handles:
             return TransportStrategy.HANDLE_PASSING
-        
+
         # Same process optimizations
         if same_process:
             if message_size < 1_048_576:  # 1MB
                 return TransportStrategy.ZERO_COPY
             else:
                 return TransportStrategy.SHARED_MEMORY
-        
+
         # Cross-process optimizations
         if message_size < 10_240:  # 10KB
             return TransportStrategy.MOJO_PIPE
@@ -92,20 +98,22 @@ class MojoTransportLayer:
     High-performance transport layer inspired by Chromium's Mojo.
     Provides zero-copy, shared memory, and handle passing capabilities.
     """
-    
+
     def __init__(self):
         self.transport_stats = {
             strategy.value: {"count": 0, "total_latency_ms": 0}
             for strategy in TransportStrategy
         }
-        
-    async def send(self, message: UnifiedMessage, sender_pid: int, receiver_pid: int) -> Dict[str, Any]:
+
+    async def send(
+        self, message: UnifiedMessage, sender_pid: int, receiver_pid: int
+    ) -> Dict[str, Any]:
         """Send message using optimal transport strategy"""
         start_time = time.perf_counter()
-        
+
         # Select transport strategy
         strategy = message.select_optimal_transport(sender_pid, receiver_pid)
-        
+
         # Execute transport
         if strategy == TransportStrategy.ZERO_COPY:
             result = await self._zero_copy_send(message)
@@ -115,69 +123,59 @@ class MojoTransportLayer:
             result = await self._pipe_send(message)
         else:  # HANDLE_PASSING
             result = await self._handle_passing_send(message)
-        
+
         # Record performance
         latency_ms = (time.perf_counter() - start_time) * 1000
         self.transport_stats[strategy.value]["count"] += 1
         self.transport_stats[strategy.value]["total_latency_ms"] += latency_ms
-        
+
         result["transport_latency_ms"] = latency_ms
         result["strategy"] = strategy.value
-        
+
         return result
-    
+
     async def _zero_copy_send(self, message: UnifiedMessage) -> Dict[str, Any]:
         """Zero-copy for ultimate performance"""
         # In Python, we simulate zero-copy by passing object references
         # In real Mojo, this would be direct memory transfer
         await asyncio.sleep(0.00001)  # Simulate 10 microsecond transfer
-        
-        return {
-            "status": "delivered",
-            "method": "zero_copy",
-            "zero_copy": True
-        }
-    
+
+        return {"status": "delivered", "method": "zero_copy", "zero_copy": True}
+
     async def _shared_memory_send(self, message: UnifiedMessage) -> Dict[str, Any]:
         """Shared memory for large transfers"""
         # Simulate shared memory allocation and mapping
         await asyncio.sleep(0.0001)  # Simulate 100 microsecond transfer
-        
+
         return {
-            "status": "delivered", 
+            "status": "delivered",
             "method": "shared_memory",
-            "shm_handle": f"shm_{id(message)}"
+            "shm_handle": f"shm_{id(message)}",
         }
-    
+
     async def _pipe_send(self, message: UnifiedMessage) -> Dict[str, Any]:
         """Mojo pipe for small cross-process messages"""
         # Simulate pipe transfer
         await asyncio.sleep(0.0003)  # Simulate 300 microsecond transfer
-        
+
         return {
             "status": "delivered",
             "method": "mojo_pipe",
-            "pipe_id": f"pipe_{id(message)}"
+            "pipe_id": f"pipe_{id(message)}",
         }
-    
+
     async def _handle_passing_send(self, message: UnifiedMessage) -> Dict[str, Any]:
         """Handle passing for resources (GPU memory, file descriptors, etc)"""
         # Simulate handle duplication and passing
         await asyncio.sleep(0.00005)  # Simulate 50 microsecond transfer
-        
+
         handles = []
-        for handle in (message.resource_handles or []):
-            handles.append({
-                "type": type(handle).__name__,
-                "id": id(handle),
-                "transferred": True
-            })
-        
-        return {
-            "status": "delivered",
-            "method": "handle_passing",
-            "handles": handles
-        }
+        for handle in message.resource_handles or []:
+            handles.append(
+                {"type": type(handle).__name__, "id": id(handle), "transferred": True}
+            )
+
+        return {"status": "delivered", "method": "handle_passing", "handles": handles}
 
 
 class IntelligentUnifiedAgent(BaseAgent):
@@ -187,51 +185,55 @@ class IntelligentUnifiedAgent(BaseAgent):
     - Uses A2A for agent coordination and negotiation
     - Uses Mojo for optimal transport performance
     """
-    
+
     def __init__(self, agent_id: str, capabilities: List[str]):
         super().__init__(agent_id, capabilities)
         self.mcp_context = MCPContext()
         self.mojo_transport = MojoTransportLayer()
         self.pid = os.getpid()  # Real process ID
-        
+
         # Performance requirements
         self.sla = {
             "max_latency_ms": 10,
             "prefer_zero_copy": True,
-            "critical_threshold_ms": 1
+            "critical_threshold_ms": 1,
         }
-    
-    async def send_unified_message(self, 
-                                 recipient: str,
-                                 intent: str,
-                                 data: Dict,
-                                 priority: int = 0,
-                                 deadline_ms: Optional[float] = None,
-                                 resources: Optional[List[Any]] = None) -> Dict[str, Any]:
+
+    async def send_unified_message(
+        self,
+        recipient: str,
+        intent: str,
+        data: Dict,
+        priority: int = 0,
+        deadline_ms: Optional[float] = None,
+        resources: Optional[List[Any]] = None,
+    ) -> Dict[str, Any]:
         """
         Send message using all three layers intelligently:
         1. MCP provides semantic context
         2. A2A handles agent protocol
         3. Mojo optimizes transport
         """
-        
+
         # Layer 1: MCP - Build semantic context
         self.mcp_context.task = {"intent": intent, "data": data}
         self.mcp_context.intent = {"action": intent, "target": recipient}
-        self.mcp_context.history.append({
-            "timestamp": time.time(),
-            "action": f"send_{intent}",
-            "recipient": recipient
-        })
-        
+        self.mcp_context.history.append(
+            {
+                "timestamp": time.time(),
+                "action": f"send_{intent}",
+                "recipient": recipient,
+            }
+        )
+
         # Layer 2: A2A - Create agent message
         a2a_msg = A2AMessage(
             sender=self.agent_id,
             recipient=recipient,
             message_type=f"{intent}_request",
-            content=data
+            content=data,
         )
-        
+
         # Layer 3: Mojo - Optimize transport
         unified_msg = UnifiedMessage(
             a2a_message=a2a_msg,
@@ -239,54 +241,53 @@ class IntelligentUnifiedAgent(BaseAgent):
             transport_strategy=TransportStrategy.ZERO_COPY,  # Will be optimized
             priority=priority,
             deadline_ms=deadline_ms,
-            resource_handles=resources
+            resource_handles=resources,
         )
-        
+
         # Get recipient process (simplified - in reality would lookup)
         recipient_pid = hash(recipient) % 1000
-        
+
         # Send using optimal transport
         transport_result = await self.mojo_transport.send(
-            unified_msg, 
-            self.pid,
-            recipient_pid
+            unified_msg, self.pid, recipient_pid
         )
-        
+
         # Verify SLA compliance
         if transport_result["transport_latency_ms"] > self.sla["max_latency_ms"]:
-            print(f"⚠️  SLA violation: {transport_result['transport_latency_ms']:.2f}ms > {self.sla['max_latency_ms']}ms")
-        
+            print(
+                f"⚠️  SLA violation: {transport_result['transport_latency_ms']:.2f}ms > {self.sla['max_latency_ms']}ms"
+            )
+
         return {
             "message_id": a2a_msg.id,
             "transport": transport_result,
             "mcp_context_size": len(str(self.mcp_context.to_dict())),
-            "a2a_conversation": a2a_msg.conversation_id
+            "a2a_conversation": a2a_msg.conversation_id,
         }
-    
+
     async def process_intent(self, intent: Dict) -> Dict:
         """Process intent using all three layers"""
         # MCP understands the semantic meaning
         self.mcp_context.intent = intent
-        
+
         # A2A coordinates with other agents if needed
         if intent.get("requires_negotiation"):
             negotiation_result = await self.negotiate_with_agents(
-                intent["negotiation_partners"],
-                intent["topic"]
+                intent["negotiation_partners"], intent["topic"]
             )
             intent["negotiation_result"] = negotiation_result
-        
+
         # Process based on intent type
         return {
             "status": "processed",
             "intent": intent,
-            "context": self.mcp_context.to_dict()
+            "context": self.mcp_context.to_dict(),
         }
-    
+
     async def negotiate_with_agents(self, partners: List[str], topic: str) -> Dict:
         """High-performance multi-agent negotiation"""
         tasks = []
-        
+
         for partner in partners:
             # Critical negotiation with tight deadline
             task = self.send_unified_message(
@@ -294,58 +295,58 @@ class IntelligentUnifiedAgent(BaseAgent):
                 intent="negotiate",
                 data={"topic": topic, "proposal": self._generate_proposal()},
                 priority=2,  # Critical
-                deadline_ms=0.5  # 500 microsecond deadline
+                deadline_ms=0.5,  # 500 microsecond deadline
             )
             tasks.append(task)
-        
+
         results = await asyncio.gather(*tasks)
-        
+
         # Analyze transport performance
         total_latency = sum(r["transport"]["transport_latency_ms"] for r in results)
         strategies_used = [r["transport"]["strategy"] for r in results]
-        
+
         return {
             "partners": partners,
             "results": results,
             "performance": {
                 "total_latency_ms": total_latency,
                 "avg_latency_ms": total_latency / len(results),
-                "transport_strategies": strategies_used
-            }
+                "transport_strategies": strategies_used,
+            },
         }
-    
+
     def _generate_proposal(self) -> Dict:
         """Generate negotiation proposal"""
         return {
             "terms": {"resource_allocation": "dynamic"},
             "constraints": {"latency": "sub-millisecond"},
-            "preferences": self.sla
+            "preferences": self.sla,
         }
 
 
 class HighFrequencyTradingAgent(IntelligentUnifiedAgent):
     """Example: Ultra-low latency trading using unified architecture"""
-    
+
     def __init__(self, agent_id: str):
         super().__init__(agent_id, ["trade", "analyze", "execute"])
         self.sla = {
             "max_latency_ms": 0.1,  # 100 microseconds
             "prefer_zero_copy": True,
-            "critical_threshold_ms": 0.05  # 50 microseconds
+            "critical_threshold_ms": 0.05,  # 50 microseconds
         }
-    
+
     async def execute_market_order(self, order: Dict) -> Dict:
         """Execute order with microsecond latency"""
         # MCP Layer: Market context
         self.mcp_context.env = {
             "market": order["exchange"],
             "volatility": await self._get_volatility(),
-            "liquidity": await self._get_liquidity()
+            "liquidity": await self._get_liquidity(),
         }
-        
+
         # GPU handle for ML model inference (if using GPU acceleration)
         gpu_handle = self._get_gpu_model_handle()
-        
+
         # Send order with critical priority
         result = await self.send_unified_message(
             recipient="exchange_connector",
@@ -353,19 +354,19 @@ class HighFrequencyTradingAgent(IntelligentUnifiedAgent):
             data=order,
             priority=2,  # Critical
             deadline_ms=0.08,  # 80 microsecond deadline
-            resources=[gpu_handle] if gpu_handle else None
+            resources=[gpu_handle] if gpu_handle else None,
         )
-        
+
         return result
-    
+
     async def _get_volatility(self) -> float:
         """Get market volatility (would be real calculation)"""
         return 0.23
-    
+
     async def _get_liquidity(self) -> float:
         """Get market liquidity (would be real calculation)"""
         return 0.89
-    
+
     def _get_gpu_model_handle(self) -> Optional[Any]:
         """Get GPU model handle for inference (simulated)"""
         # In real implementation, would return CUDA context or similar
@@ -377,7 +378,7 @@ async def demonstrate_unified_architecture():
     Comprehensive demonstration showing how MCP, A2A, and Mojo create
     a sophisticated, high-performance agent system.
     """
-    
+
     print("=" * 60)
     print("UNIFIED MCP-A2A-MOJO ARCHITECTURE DEMONSTRATION")
     print("=" * 60)
@@ -387,22 +388,22 @@ async def demonstrate_unified_architecture():
     print("• A2A: Agent coordination and negotiation")
     print("• Mojo: Microsecond-scale transport performance")
     print()
-    
+
     # 1. Ultra-Low Latency Trading
     print("1. ULTRA-LOW LATENCY TRADING (Microsecond Scale)")
     print("-" * 50)
-    
+
     trader = HighFrequencyTradingAgent("hft_trader_1")
-    
+
     order = {
         "symbol": "AAPL",
         "quantity": 10000,
         "type": "MARKET",
-        "exchange": "NASDAQ"
+        "exchange": "NASDAQ",
     }
-    
+
     result = await trader.execute_market_order(order)
-    
+
     print(f"✓ Order executed")
     print(f"  - Transport: {result['transport']['strategy']}")
     print(f"  - Latency: {result['transport']['transport_latency_ms']:.3f}ms")
@@ -410,28 +411,33 @@ async def demonstrate_unified_architecture():
     print(f"  - GPU handle passed: {result['transport'].get('handles') is not None}")
     print(f"  - MCP context size: {result['mcp_context_size']} bytes")
     print()
-    
+
     # 2. Multi-Agent Negotiation
     print("2. HIGH-PERFORMANCE MULTI-AGENT NEGOTIATION")
     print("-" * 50)
-    
+
     coordinator = IntelligentUnifiedAgent("coordinator", ["coordinate", "allocate"])
-    
+
     negotiation_result = await coordinator.negotiate_with_agents(
-        ["resource_manager", "scheduler", "optimizer"],
-        "datacenter_resources"
+        ["resource_manager", "scheduler", "optimizer"], "datacenter_resources"
     )
-    
+
     print(f"✓ Negotiation completed with {len(negotiation_result['partners'])} agents")
-    print(f"  - Total latency: {negotiation_result['performance']['total_latency_ms']:.3f}ms")
-    print(f"  - Average latency: {negotiation_result['performance']['avg_latency_ms']:.3f}ms")
-    print(f"  - Transport strategies: {set(negotiation_result['performance']['transport_strategies'])}")
+    print(
+        f"  - Total latency: {negotiation_result['performance']['total_latency_ms']:.3f}ms"
+    )
+    print(
+        f"  - Average latency: {negotiation_result['performance']['avg_latency_ms']:.3f}ms"
+    )
+    print(
+        f"  - Transport strategies: {set(negotiation_result['performance']['transport_strategies'])}"
+    )
     print()
-    
+
     # 3. Performance Summary
     print("3. TRANSPORT LAYER PERFORMANCE SUMMARY")
     print("-" * 50)
-    
+
     # Aggregate stats from all agents
     all_stats = {}
     for agent in [trader, coordinator]:
@@ -440,13 +446,13 @@ async def demonstrate_unified_architecture():
                 all_stats[strategy] = {"count": 0, "total_latency_ms": 0}
             all_stats[strategy]["count"] += stats["count"]
             all_stats[strategy]["total_latency_ms"] += stats["total_latency_ms"]
-    
+
     print("Transport Strategy Usage:")
     for strategy, stats in all_stats.items():
         if stats["count"] > 0:
             avg_latency = stats["total_latency_ms"] / stats["count"]
             print(f"  - {strategy}: {stats['count']} calls, avg {avg_latency:.3f}ms")
-    
+
     print()
     print("KEY INSIGHTS:")
     print("✓ MCP provides semantic understanding for all operations")
diff --git a/agents/unified_transport_layer.py b/agents/unified_transport_layer.py
index c68dae1..46eeee4 100644
--- a/agents/unified_transport_layer.py
+++ b/agents/unified_transport_layer.py
@@ -13,25 +13,30 @@ from dataclasses import dataclass
 from agents.a2a_framework import A2AMessage, BaseAgent, A2AMessageBus
 from connectors.mcp_base import MCPContext, MCPConnector
 
+
 # Mojo-inspired transport abstractions (Python implementation)
 @dataclass
 class MojoMessagePipe:
     """High-performance message pipe inspired by Mojo"""
+
     pipe_id: str
     sender_process: int
     receiver_process: int
     shared_memory: Optional[mmap.mmap] = None
-    
+
     def is_same_process(self) -> bool:
         return self.sender_process == self.receiver_process
 
-@dataclass 
+
+@dataclass
 class MojoHandle:
     """Native handle for resource passing"""
+
     handle_type: str
     resource_id: str
     metadata: Dict[str, Any]
 
+
 class UnifiedTransportLayer:
     """
     Unified transport that intelligently routes messages through:
@@ -39,31 +44,29 @@ class UnifiedTransportLayer:
     - A2A for agent protocol and negotiation
     - MCP for context and semantic understanding
     """
-    
+
     def __init__(self):
         self.mojo_pipes: Dict[str, MojoMessagePipe] = {}
         self.shared_memory_regions: Dict[str, mmap.mmap] = {}
         self.performance_stats = {
-            'mojo_transfers': 0,
-            'mcp_context_shares': 0,
-            'a2a_negotiations': 0,
-            'zero_copy_transfers': 0
+            "mojo_transfers": 0,
+            "mcp_context_shares": 0,
+            "a2a_negotiations": 0,
+            "zero_copy_transfers": 0,
         }
-    
+
     def create_pipe(self, sender: str, receiver: str) -> MojoMessagePipe:
         """Create optimized pipe between agents"""
         pipe_id = f"{sender}->{receiver}"
-        
+
         # Check if same process for zero-copy optimization
         sender_pid = self._get_agent_process(sender)
         receiver_pid = self._get_agent_process(receiver)
-        
+
         pipe = MojoMessagePipe(
-            pipe_id=pipe_id,
-            sender_process=sender_pid,
-            receiver_process=receiver_pid
+            pipe_id=pipe_id, sender_process=sender_pid, receiver_process=receiver_pid
         )
-        
+
         # Create shared memory for large transfers
         if sender_pid == receiver_pid:
             # Same process - use in-memory buffer
@@ -72,283 +75,272 @@ class UnifiedTransportLayer:
             # Different processes - create shared memory
             shm_size = 10 * 1024 * 1024  # 10MB default
             pipe.shared_memory = mmap.mmap(-1, shm_size)
-            
+
         self.mojo_pipes[pipe_id] = pipe
         return pipe
-    
+
     def _get_agent_process(self, agent_id: str) -> int:
         """Get process ID for agent (simplified)"""
         # In real implementation, would track actual process IDs
         return hash(agent_id) % 10  # Simulate different processes
-    
-    async def send_unified_message(self, 
-                                 sender: BaseAgent,
-                                 recipient: str,
-                                 message: A2AMessage,
-                                 context: MCPContext) -> Dict[str, Any]:
+
+    async def send_unified_message(
+        self,
+        sender: BaseAgent,
+        recipient: str,
+        message: A2AMessage,
+        context: MCPContext,
+    ) -> Dict[str, Any]:
         """
         Send message using the most efficient transport based on:
         - Message size
         - Agent locations
         - Performance requirements
         """
-        
+
         start_time = time.time()
         pipe_id = f"{sender.agent_id}->{recipient}"
-        
+
         # Get or create pipe
         if pipe_id not in self.mojo_pipes:
             pipe = self.create_pipe(sender.agent_id, recipient)
         else:
             pipe = self.mojo_pipes[pipe_id]
-        
+
         # Prepare unified payload
         payload = {
-            'a2a_message': message.to_dict(),
-            'mcp_context': context.to_dict(),
-            'transport_metadata': {
-                'sent_at': time.time(),
-                'pipe_id': pipe_id,
-                'optimization': 'auto'
-            }
+            "a2a_message": message.to_dict(),
+            "mcp_context": context.to_dict(),
+            "transport_metadata": {
+                "sent_at": time.time(),
+                "pipe_id": pipe_id,
+                "optimization": "auto",
+            },
         }
-        
+
         # Choose transport strategy
         payload_size = len(str(payload))
-        
+
         if pipe.is_same_process() and payload_size < 1024 * 1024:  # < 1MB
             # Zero-copy for small same-process messages
             result = await self._zero_copy_transfer(pipe, payload)
-            self.performance_stats['zero_copy_transfers'] += 1
-            
+            self.performance_stats["zero_copy_transfers"] += 1
+
         elif pipe.is_same_process() and payload_size >= 1024 * 1024:
             # Shared memory for large same-process messages
             result = await self._shared_memory_transfer(pipe, payload)
-            
+
         elif not pipe.is_same_process() and payload_size < 10 * 1024:  # < 10KB
             # Direct pipe for small cross-process
             result = await self._pipe_transfer(pipe, payload)
-            
+
         else:
             # Shared memory + handle passing for large cross-process
             result = await self._handle_passing_transfer(pipe, payload)
-        
+
         # Update stats
-        self.performance_stats['mojo_transfers'] += 1
-        self.performance_stats['mcp_context_shares'] += 1
-        if message.message_type.startswith('negotiate'):
-            self.performance_stats['a2a_negotiations'] += 1
-        
+        self.performance_stats["mojo_transfers"] += 1
+        self.performance_stats["mcp_context_shares"] += 1
+        if message.message_type.startswith("negotiate"):
+            self.performance_stats["a2a_negotiations"] += 1
+
         # Record performance
         latency = (time.time() - start_time) * 1000  # ms
-        result['transport_latency_ms'] = latency
-        
+        result["transport_latency_ms"] = latency
+
         return result
-    
+
     async def _zero_copy_transfer(self, pipe: MojoMessagePipe, payload: Dict) -> Dict:
         """Zero-copy transfer for same-process communication"""
         # In real Mojo, this would be direct memory transfer
         # Python simulation: direct object passing
         return {
-            'status': 'delivered',
-            'method': 'zero_copy',
-            'payload': payload  # No serialization needed
+            "status": "delivered",
+            "method": "zero_copy",
+            "payload": payload,  # No serialization needed
         }
-    
-    async def _shared_memory_transfer(self, pipe: MojoMessagePipe, payload: Dict) -> Dict:
+
+    async def _shared_memory_transfer(
+        self, pipe: MojoMessagePipe, payload: Dict
+    ) -> Dict:
         """Shared memory transfer for large payloads"""
         # Serialize to shared memory
         serialized = pickle.dumps(payload)
-        
+
         if pipe.shared_memory:
             # Write to shared memory
             pipe.shared_memory.seek(0)
-            pipe.shared_memory.write(len(serialized).to_bytes(8, 'little'))
+            pipe.shared_memory.write(len(serialized).to_bytes(8, "little"))
             pipe.shared_memory.write(serialized)
-            
+
             # Return handle instead of data
             handle = MojoHandle(
-                handle_type='shared_memory',
+                handle_type="shared_memory",
                 resource_id=pipe.pipe_id,
-                metadata={'size': len(serialized)}
+                metadata={"size": len(serialized)},
             )
-            
-            return {
-                'status': 'delivered',
-                'method': 'shared_memory',
-                'handle': handle
-            }
+
+            return {"status": "delivered", "method": "shared_memory", "handle": handle}
         else:
             # Fallback for same-process
             return await self._zero_copy_transfer(pipe, payload)
-    
+
     async def _pipe_transfer(self, pipe: MojoMessagePipe, payload: Dict) -> Dict:
         """Standard pipe transfer for small cross-process messages"""
         # In real Mojo, this would use message pipes
         # Python simulation: asyncio queue
         return {
-            'status': 'delivered', 
-            'method': 'pipe',
-            'serialized_size': len(pickle.dumps(payload))
+            "status": "delivered",
+            "method": "pipe",
+            "serialized_size": len(pickle.dumps(payload)),
         }
-    
-    async def _handle_passing_transfer(self, pipe: MojoMessagePipe, payload: Dict) -> Dict:
+
+    async def _handle_passing_transfer(
+        self, pipe: MojoMessagePipe, payload: Dict
+    ) -> Dict:
         """Handle passing for resources and large data"""
         # Create handle for resource
         handle = MojoHandle(
-            handle_type='composite',
+            handle_type="composite",
             resource_id=f"{pipe.pipe_id}_{time.time()}",
             metadata={
-                'mcp_context_size': len(str(payload['mcp_context'])),
-                'a2a_message_type': payload['a2a_message']['message_type']
-            }
+                "mcp_context_size": len(str(payload["mcp_context"])),
+                "a2a_message_type": payload["a2a_message"]["message_type"],
+            },
         )
-        
+
         # In real Mojo, would pass native handles
-        return {
-            'status': 'delivered',
-            'method': 'handle_passing',
-            'handle': handle
-        }
+        return {"status": "delivered", "method": "handle_passing", "handle": handle}
 
 
 class UnifiedAgent(BaseAgent):
     """Agent that leverages all three layers intelligently"""
-    
+
     def __init__(self, agent_id: str, capabilities: List[str]):
         super().__init__(agent_id, capabilities)
         self.mcp_context = MCPContext()
         self.transport = UnifiedTransportLayer()
-        self.performance_requirements = {
-            'max_latency_ms': 10,
-            'prefer_zero_copy': True
-        }
-    
-    async def send_intelligent_message(self, 
-                                     recipient: str,
-                                     intent: str,
-                                     data: Dict) -> Dict:
+        self.performance_requirements = {"max_latency_ms": 10, "prefer_zero_copy": True}
+
+    async def send_intelligent_message(
+        self, recipient: str, intent: str, data: Dict
+    ) -> Dict:
         """
         Send message using intelligent routing:
         - MCP for context
         - A2A for protocol
         - Mojo for transport
         """
-        
+
         # 1. Build MCP context
-        self.mcp_context.task = {'intent': intent, 'data': data}
-        self.mcp_context.history.append({
-            'action': 'send_message',
-            'to': recipient,
-            'timestamp': time.time()
-        })
-        
-        # 2. Create A2A message  
+        self.mcp_context.task = {"intent": intent, "data": data}
+        self.mcp_context.history.append(
+            {"action": "send_message", "to": recipient, "timestamp": time.time()}
+        )
+
+        # 2. Create A2A message
         message = A2AMessage(
             sender=self.agent_id,
             recipient=recipient,
             message_type=f"{intent}_request",
-            content=data
+            content=data,
         )
-        
+
         # 3. Use unified transport
         result = await self.transport.send_unified_message(
-            sender=self,
-            recipient=recipient,
-            message=message,
-            context=self.mcp_context
+            sender=self, recipient=recipient, message=message, context=self.mcp_context
         )
-        
+
         # 4. Update context with result
-        self.mcp_context.history.append({
-            'action': 'message_sent',
-            'result': result['status'],
-            'method': result['method'],
-            'latency_ms': result.get('transport_latency_ms', 0)
-        })
-        
+        self.mcp_context.history.append(
+            {
+                "action": "message_sent",
+                "result": result["status"],
+                "method": result["method"],
+                "latency_ms": result.get("transport_latency_ms", 0),
+            }
+        )
+
         return result
-    
-    async def negotiate_with_performance(self,
-                                       other_agents: List[str],
-                                       negotiation_topic: str) -> Dict:
+
+    async def negotiate_with_performance(
+        self, other_agents: List[str], negotiation_topic: str
+    ) -> Dict:
         """High-performance multi-agent negotiation"""
-        
+
         # Pre-create Mojo pipes for all agents
         pipes = {}
         for agent in other_agents:
             pipe = self.transport.create_pipe(self.agent_id, agent)
             pipes[agent] = pipe
-        
+
         # Parallel negotiation using all three layers
         tasks = []
         for agent in other_agents:
             task = self.send_intelligent_message(
                 recipient=agent,
-                intent='negotiate',
+                intent="negotiate",
                 data={
-                    'topic': negotiation_topic,
-                    'proposal': self._generate_proposal(negotiation_topic)
-                }
+                    "topic": negotiation_topic,
+                    "proposal": self._generate_proposal(negotiation_topic),
+                },
             )
             tasks.append(task)
-        
+
         # Wait for all responses
         results = await asyncio.gather(*tasks)
-        
+
         # Analyze results
         return {
-            'negotiation_complete': True,
-            'participants': other_agents,
-            'results': results,
-            'total_latency_ms': sum(r.get('transport_latency_ms', 0) for r in results),
-            'transport_methods': [r['method'] for r in results]
+            "negotiation_complete": True,
+            "participants": other_agents,
+            "results": results,
+            "total_latency_ms": sum(r.get("transport_latency_ms", 0) for r in results),
+            "transport_methods": [r["method"] for r in results],
         }
-    
+
     def _generate_proposal(self, topic: str) -> Dict:
         """Generate negotiation proposal"""
         return {
-            'terms': {},
-            'constraints': {},
-            'preferences': self.performance_requirements
+            "terms": {},
+            "constraints": {},
+            "preferences": self.performance_requirements,
         }
 
 
 # Example: High-Performance Trading System
 class TradingAgent(UnifiedAgent):
     """Trading agent using unified architecture"""
-    
+
     def __init__(self, agent_id: str):
-        super().__init__(agent_id, ['trade', 'analyze', 'execute'])
+        super().__init__(agent_id, ["trade", "analyze", "execute"])
         self.performance_requirements = {
-            'max_latency_ms': 0.1,  # 100 microseconds
-            'prefer_zero_copy': True,
-            'require_handle_passing': True  # For order handles
+            "max_latency_ms": 0.1,  # 100 microseconds
+            "prefer_zero_copy": True,
+            "require_handle_passing": True,  # For order handles
         }
-    
+
     async def execute_trade(self, order: Dict) -> Dict:
         """Execute trade with microsecond latency"""
-        
+
         # Update MCP context with market data
         self.mcp_context.env = {
-            'market': order['market'],
-            'volatility': self._get_market_volatility()
+            "market": order["market"],
+            "volatility": self._get_market_volatility(),
         }
-        
+
         # Send to execution agent via fastest path
         result = await self.send_intelligent_message(
-            recipient='execution_engine',
-            intent='execute_order',
-            data=order
+            recipient="execution_engine", intent="execute_order", data=order
         )
-        
+
         # Verify ultra-low latency
-        if result['transport_latency_ms'] > 0.1:
+        if result["transport_latency_ms"] > 0.1:
             # Fallback or alert
             print(f"WARNING: High latency detected: {result['transport_latency_ms']}ms")
-        
+
         return result
-    
+
     def _get_market_volatility(self) -> float:
         """Get current market volatility"""
         return 0.15  # Simplified
@@ -357,61 +349,52 @@ class TradingAgent(UnifiedAgent):
 # Demonstration
 async def demonstrate_unified_architecture():
     """Show how MCP, A2A, and Mojo work together"""
-    
+
     print("=== Unified MCP-A2A-Mojo Architecture Demo ===\n")
-    
+
     # Create agents
     trader = TradingAgent("trader_1")
     analyzer = UnifiedAgent("analyzer_1", ["analyze", "predict"])
     executor = UnifiedAgent("executor_1", ["execute", "confirm"])
-    
+
     # Test 1: Ultra-low latency trading
     print("1. Ultra-low latency trading:")
-    order = {
-        'symbol': 'AAPL',
-        'action': 'BUY',
-        'quantity': 1000,
-        'type': 'MARKET'
-    }
-    
+    order = {"symbol": "AAPL", "action": "BUY", "quantity": 1000, "type": "MARKET"}
+
     result = await trader.execute_trade(order)
     print(f"   - Method: {result['method']}")
     print(f"   - Latency: {result['transport_latency_ms']:.3f}ms")
     print(f"   - Status: {result['status']}\n")
-    
+
     # Test 2: Multi-agent negotiation
     print("2. Multi-agent negotiation:")
     negotiation_result = await analyzer.negotiate_with_performance(
-        ["trader_1", "executor_1"],
-        "resource_allocation"
+        ["trader_1", "executor_1"], "resource_allocation"
     )
     print(f"   - Participants: {negotiation_result['participants']}")
     print(f"   - Total latency: {negotiation_result['total_latency_ms']:.3f}ms")
     print(f"   - Methods used: {set(negotiation_result['transport_methods'])}\n")
-    
+
     # Test 3: Large context transfer
     print("3. Large context transfer:")
     large_context = MCPContext()
-    large_context.code_state = {'data': 'x' * (2 * 1024 * 1024)}  # 2MB
-    
+    large_context.code_state = {"data": "x" * (2 * 1024 * 1024)}  # 2MB
+
     large_message = A2AMessage(
         sender="analyzer_1",
         recipient="executor_1",
         message_type="large_data",
-        content={'size': '2MB'}
+        content={"size": "2MB"},
     )
-    
+
     transport = UnifiedTransportLayer()
     result = await transport.send_unified_message(
-        analyzer,
-        "executor_1", 
-        large_message,
-        large_context
+        analyzer, "executor_1", large_message, large_context
     )
     print(f"   - Method: {result['method']}")
     print(f"   - Handle type: {result.get('handle', {}).get('handle_type', 'N/A')}")
     print(f"   - Status: {result['status']}\n")
-    
+
     # Print performance summary
     print("=== Performance Summary ===")
     stats = trader.transport.performance_stats
@@ -420,4 +403,4 @@ async def demonstrate_unified_architecture():
 
 
 if __name__ == "__main__":
-    asyncio.run(demonstrate_unified_architecture()) 
\ No newline at end of file
+    asyncio.run(demonstrate_unified_architecture())
diff --git a/analyzers/pattern_detector.py b/analyzers/pattern_detector.py
index fc0591f..942fe0d 100644
--- a/analyzers/pattern_detector.py
+++ b/analyzers/pattern_detector.py
@@ -6,125 +6,129 @@ from typing import Dict, List, Tuple, Optional
 from datetime import datetime, timedelta
 import numpy as np
 from collections import defaultdict
+
 # from utils.db_tracker import get_execution_history  # TODO: implement when db_tracker has this function
 
+
 class PatternDetector:
     """Detects patterns in execution data to guide mutations"""
-    
+
     def __init__(self):
         self.patterns = {}
         self.insights = []
         self.mutation_recommendations = []
-        
+
     async def analyze_execution_patterns(self, time_window: timedelta = None) -> Dict:
         """Analyze execution patterns from database"""
         # Get execution history
         history = await self._get_execution_data(time_window)
-        
+
         # Detect various patterns
         failure_patterns = await self._detect_failure_patterns(history)
         performance_patterns = await self._detect_performance_patterns(history)
         usage_patterns = await self._detect_usage_patterns(history)
-        
+
         # Generate insights
         insights = await self._generate_insights(
-            failure_patterns,
-            performance_patterns,
-            usage_patterns
+            failure_patterns, performance_patterns, usage_patterns
         )
-        
+
         # Generate mutation recommendations
         recommendations = await self._generate_mutation_recommendations(insights)
-        
+
         return {
-            'patterns': {
-                'failures': failure_patterns,
-                'performance': performance_patterns,
-                'usage': usage_patterns
+            "patterns": {
+                "failures": failure_patterns,
+                "performance": performance_patterns,
+                "usage": usage_patterns,
             },
-            'insights': insights,
-            'recommendations': recommendations,
-            'analysis_timestamp': datetime.utcnow().isoformat()
+            "insights": insights,
+            "recommendations": recommendations,
+            "analysis_timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     async def _get_execution_data(self, time_window: timedelta = None) -> List[Dict]:
         """Get execution data from database"""
         # In real implementation, would query database
         # For now, return mock data
         return [
             {
-                'protocol': 'data_processor',
-                'success': False,
-                'error': 'FileNotFoundError',
-                'duration': 0.5,
-                'timestamp': datetime.utcnow().isoformat()
+                "protocol": "data_processor",
+                "success": False,
+                "error": "FileNotFoundError",
+                "duration": 0.5,
+                "timestamp": datetime.utcnow().isoformat(),
             },
             {
-                'protocol': 'api_health_checker',
-                'success': True,
-                'duration': 1.2,
-                'timestamp': datetime.utcnow().isoformat()
-            }
+                "protocol": "api_health_checker",
+                "success": True,
+                "duration": 1.2,
+                "timestamp": datetime.utcnow().isoformat(),
+            },
         ]
-    
+
     async def _detect_failure_patterns(self, history: List[Dict]) -> Dict:
         """Detect patterns in failures"""
         failure_patterns = {
-            'by_protocol': defaultdict(int),
-            'by_error_type': defaultdict(int),
-            'by_time_of_day': defaultdict(int),
-            'cascading_failures': [],
-            'repeated_failures': []
+            "by_protocol": defaultdict(int),
+            "by_error_type": defaultdict(int),
+            "by_time_of_day": defaultdict(int),
+            "cascading_failures": [],
+            "repeated_failures": [],
         }
-        
+
         for execution in history:
-            if not execution['success']:
-                protocol = execution['protocol']
-                error = execution.get('error', 'unknown')
-                
-                failure_patterns['by_protocol'][protocol] += 1
-                failure_patterns['by_error_type'][error] += 1
-                
+            if not execution["success"]:
+                protocol = execution["protocol"]
+                error = execution.get("error", "unknown")
+
+                failure_patterns["by_protocol"][protocol] += 1
+                failure_patterns["by_error_type"][error] += 1
+
                 # Time-based analysis
-                hour = datetime.fromisoformat(execution['timestamp']).hour
-                failure_patterns['by_time_of_day'][hour] += 1
-        
+                hour = datetime.fromisoformat(execution["timestamp"]).hour
+                failure_patterns["by_time_of_day"][hour] += 1
+
         # Detect repeated failures (same protocol failing multiple times)
-        for protocol, count in failure_patterns['by_protocol'].items():
+        for protocol, count in failure_patterns["by_protocol"].items():
             if count > 3:
-                failure_patterns['repeated_failures'].append({
-                    'protocol': protocol,
-                    'failure_count': count,
-                    'severity': 'high' if count > 10 else 'medium'
-                })
-        
+                failure_patterns["repeated_failures"].append(
+                    {
+                        "protocol": protocol,
+                        "failure_count": count,
+                        "severity": "high" if count > 10 else "medium",
+                    }
+                )
+
         return failure_patterns
-    
+
     async def _detect_performance_patterns(self, history: List[Dict]) -> Dict:
         """Detect performance patterns"""
         performance_patterns = {
-            'slow_protocols': [],
-            'performance_degradation': [],
-            'resource_bottlenecks': []
+            "slow_protocols": [],
+            "performance_degradation": [],
+            "resource_bottlenecks": [],
         }
-        
+
         # Group by protocol
         protocol_durations = defaultdict(list)
         for execution in history:
-            if 'duration' in execution:
-                protocol_durations[execution['protocol']].append(execution['duration'])
-        
+            if "duration" in execution:
+                protocol_durations[execution["protocol"]].append(execution["duration"])
+
         # Find slow protocols
         for protocol, durations in protocol_durations.items():
             avg_duration = np.mean(durations)
             if avg_duration > 5.0:  # 5 seconds threshold
-                performance_patterns['slow_protocols'].append({
-                    'protocol': protocol,
-                    'avg_duration': avg_duration,
-                    'max_duration': max(durations),
-                    'sample_size': len(durations)
-                })
-        
+                performance_patterns["slow_protocols"].append(
+                    {
+                        "protocol": protocol,
+                        "avg_duration": avg_duration,
+                        "max_duration": max(durations),
+                        "sample_size": len(durations),
+                    }
+                )
+
         # Detect performance degradation (increasing execution times)
         for protocol, durations in protocol_durations.items():
             if len(durations) > 5:
@@ -132,167 +136,184 @@ class PatternDetector:
                 recent = durations[-5:]
                 older = durations[:-5]
                 if np.mean(recent) > np.mean(older) * 1.5:
-                    performance_patterns['performance_degradation'].append({
-                        'protocol': protocol,
-                        'degradation_factor': np.mean(recent) / np.mean(older),
-                        'trend': 'increasing'
-                    })
-        
+                    performance_patterns["performance_degradation"].append(
+                        {
+                            "protocol": protocol,
+                            "degradation_factor": np.mean(recent) / np.mean(older),
+                            "trend": "increasing",
+                        }
+                    )
+
         return performance_patterns
-    
+
     async def _detect_usage_patterns(self, history: List[Dict]) -> Dict:
         """Detect usage patterns"""
         usage_patterns = {
-            'most_used_protocols': [],
-            'unused_protocols': [],
-            'usage_by_time': defaultdict(int),
-            'protocol_combinations': []
+            "most_used_protocols": [],
+            "unused_protocols": [],
+            "usage_by_time": defaultdict(int),
+            "protocol_combinations": [],
         }
-        
+
         # Count protocol usage
         protocol_usage = defaultdict(int)
         for execution in history:
-            protocol_usage[execution['protocol']] += 1
-        
+            protocol_usage[execution["protocol"]] += 1
+
         # Sort by usage
         sorted_usage = sorted(protocol_usage.items(), key=lambda x: x[1], reverse=True)
-        usage_patterns['most_used_protocols'] = [
-            {'protocol': p, 'usage_count': c} 
-            for p, c in sorted_usage[:5]
+        usage_patterns["most_used_protocols"] = [
+            {"protocol": p, "usage_count": c} for p, c in sorted_usage[:5]
         ]
-        
+
         return usage_patterns
-    
-    async def _generate_insights(self, 
-                               failure_patterns: Dict,
-                               performance_patterns: Dict,
-                               usage_patterns: Dict) -> List[Dict]:
+
+    async def _generate_insights(
+        self, failure_patterns: Dict, performance_patterns: Dict, usage_patterns: Dict
+    ) -> List[Dict]:
         """Generate actionable insights from patterns"""
         insights = []
-        
+
         # Failure insights
-        for repeated in failure_patterns['repeated_failures']:
-            insights.append({
-                'type': 'repeated_failure',
-                'severity': repeated['severity'],
-                'message': f"Protocol {repeated['protocol']} has failed {repeated['failure_count']} times",
-                'recommendation': 'Consider mutation or redesign',
-                'data': repeated
-            })
-        
+        for repeated in failure_patterns["repeated_failures"]:
+            insights.append(
+                {
+                    "type": "repeated_failure",
+                    "severity": repeated["severity"],
+                    "message": f"Protocol {repeated['protocol']} has failed {repeated['failure_count']} times",
+                    "recommendation": "Consider mutation or redesign",
+                    "data": repeated,
+                }
+            )
+
         # Performance insights
-        for slow in performance_patterns['slow_protocols']:
-            insights.append({
-                'type': 'performance_issue',
-                'severity': 'medium',
-                'message': f"Protocol {slow['protocol']} averages {slow['avg_duration']:.2f}s execution time",
-                'recommendation': 'Optimize algorithm or add caching',
-                'data': slow
-            })
-        
+        for slow in performance_patterns["slow_protocols"]:
+            insights.append(
+                {
+                    "type": "performance_issue",
+                    "severity": "medium",
+                    "message": f"Protocol {slow['protocol']} averages {slow['avg_duration']:.2f}s execution time",
+                    "recommendation": "Optimize algorithm or add caching",
+                    "data": slow,
+                }
+            )
+
         # Usage insights
-        if usage_patterns['most_used_protocols']:
-            top_protocol = usage_patterns['most_used_protocols'][0]
-            insights.append({
-                'type': 'high_usage',
-                'severity': 'info',
-                'message': f"Protocol {top_protocol['protocol']} is most used ({top_protocol['usage_count']} times)",
-                'recommendation': 'Ensure robustness and consider optimization',
-                'data': top_protocol
-            })
-        
+        if usage_patterns["most_used_protocols"]:
+            top_protocol = usage_patterns["most_used_protocols"][0]
+            insights.append(
+                {
+                    "type": "high_usage",
+                    "severity": "info",
+                    "message": f"Protocol {top_protocol['protocol']} is most used ({top_protocol['usage_count']} times)",
+                    "recommendation": "Ensure robustness and consider optimization",
+                    "data": top_protocol,
+                }
+            )
+
         return insights
-    
-    async def _generate_mutation_recommendations(self, insights: List[Dict]) -> List[Dict]:
+
+    async def _generate_mutation_recommendations(
+        self, insights: List[Dict]
+    ) -> List[Dict]:
         """Generate specific mutation recommendations"""
         recommendations = []
-        
+
         for insight in insights:
-            if insight['type'] == 'repeated_failure':
-                protocol = insight['data']['protocol']
-                recommendations.append({
-                    'protocol': protocol,
-                    'mutation_type': 'error_handling',
-                    'priority': 'high',
-                    'suggested_changes': [
-                        'Add retry logic with exponential backoff',
-                        'Implement better error handling',
-                        'Add input validation',
-                        'Consider circuit breaker pattern'
-                    ],
-                    'reason': insight['message']
-                })
-                
-            elif insight['type'] == 'performance_issue':
-                protocol = insight['data']['protocol']
-                recommendations.append({
-                    'protocol': protocol,
-                    'mutation_type': 'performance_optimization',
-                    'priority': 'medium',
-                    'suggested_changes': [
-                        'Add caching layer',
-                        'Optimize database queries',
-                        'Implement pagination',
-                        'Use async operations'
-                    ],
-                    'reason': insight['message']
-                })
-        
+            if insight["type"] == "repeated_failure":
+                protocol = insight["data"]["protocol"]
+                recommendations.append(
+                    {
+                        "protocol": protocol,
+                        "mutation_type": "error_handling",
+                        "priority": "high",
+                        "suggested_changes": [
+                            "Add retry logic with exponential backoff",
+                            "Implement better error handling",
+                            "Add input validation",
+                            "Consider circuit breaker pattern",
+                        ],
+                        "reason": insight["message"],
+                    }
+                )
+
+            elif insight["type"] == "performance_issue":
+                protocol = insight["data"]["protocol"]
+                recommendations.append(
+                    {
+                        "protocol": protocol,
+                        "mutation_type": "performance_optimization",
+                        "priority": "medium",
+                        "suggested_changes": [
+                            "Add caching layer",
+                            "Optimize database queries",
+                            "Implement pagination",
+                            "Use async operations",
+                        ],
+                        "reason": insight["message"],
+                    }
+                )
+
         return recommendations
 
 
 class InsightDrivenMutator:
     """Mutates components based on insights from pattern detection"""
-    
+
     def __init__(self):
         self.pattern_detector = PatternDetector()
         self.mutation_history = []
-        
+
     async def analyze_and_mutate(self) -> Dict:
         """Analyze patterns and apply mutations"""
         # Get analysis
         analysis = await self.pattern_detector.analyze_execution_patterns()
-        
+
         # Apply mutations based on recommendations
         mutations_applied = []
-        for recommendation in analysis['recommendations']:
+        for recommendation in analysis["recommendations"]:
             mutation_result = await self._apply_mutation(recommendation)
             mutations_applied.append(mutation_result)
-        
+
         return {
-            'analysis': analysis,
-            'mutations_applied': mutations_applied,
-            'timestamp': datetime.utcnow().isoformat()
+            "analysis": analysis,
+            "mutations_applied": mutations_applied,
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     async def _apply_mutation(self, recommendation: Dict) -> Dict:
         """Apply a specific mutation based on recommendation"""
-        protocol = recommendation['protocol']
-        mutation_type = recommendation['mutation_type']
-        
+        protocol = recommendation["protocol"]
+        mutation_type = recommendation["mutation_type"]
+
         # Load current protocol code
         from protocols.loader import load_protocol
+
         current_code = await self._get_protocol_code(protocol)
-        
+
         # Generate mutated code based on type
-        if mutation_type == 'error_handling':
+        if mutation_type == "error_handling":
             mutated_code = await self._add_error_handling(current_code, recommendation)
-        elif mutation_type == 'performance_optimization':
-            mutated_code = await self._add_performance_optimization(current_code, recommendation)
+        elif mutation_type == "performance_optimization":
+            mutated_code = await self._add_performance_optimization(
+                current_code, recommendation
+            )
         else:
             mutated_code = current_code
-        
+
         # Save mutated code
         success = await self._save_mutated_protocol(protocol, mutated_code)
-        
+
         return {
-            'protocol': protocol,
-            'mutation_type': mutation_type,
-            'success': success,
-            'changes_applied': recommendation['suggested_changes'][:2],  # Apply top 2 suggestions
-            'timestamp': datetime.utcnow().isoformat()
+            "protocol": protocol,
+            "mutation_type": mutation_type,
+            "success": success,
+            "changes_applied": recommendation["suggested_changes"][
+                :2
+            ],  # Apply top 2 suggestions
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     async def _get_protocol_code(self, protocol: str) -> str:
         """Get current protocol code"""
         # In real implementation, would read from file
@@ -301,7 +322,7 @@ def task():
     # Original {protocol} code
     return random.random() > 0.5
 """
-    
+
     async def _add_error_handling(self, code: str, recommendation: Dict) -> str:
         """Add error handling to code"""
         # In real implementation, would use AST manipulation
@@ -330,8 +351,10 @@ def _original_task():
     # Original code moved here
     return random.random() > 0.5
 """
-    
-    async def _add_performance_optimization(self, code: str, recommendation: Dict) -> str:
+
+    async def _add_performance_optimization(
+        self, code: str, recommendation: Dict
+    ) -> str:
         """Add performance optimization to code"""
         # In real implementation, would analyze and optimize
         return f"""
@@ -346,10 +369,10 @@ def _compute_result():
     # Original computation
     return random.random() > 0.5
 """
-    
+
     async def _save_mutated_protocol(self, protocol: str, code: str) -> bool:
         """Save mutated protocol code"""
         # In real implementation, would write to file
         # For now, just log
         print(f"Would save mutated {protocol} with new code")
-        return True 
\ No newline at end of file
+        return True
diff --git a/connectors/dwave_quantum_connector.py b/connectors/dwave_quantum_connector.py
index eebb696..61a5cee 100644
--- a/connectors/dwave_quantum_connector.py
+++ b/connectors/dwave_quantum_connector.py
@@ -33,6 +33,7 @@ try:
     import dimod
     from dimod import BinaryQuadraticModel, ConstrainedQuadraticModel
     import dwave.inspector
+
     DWAVE_AVAILABLE = True
 except ImportError:
     DWAVE_AVAILABLE = False
@@ -41,9 +42,11 @@ from connectors.mcp_base import MCPConnector
 
 logger = logging.getLogger(__name__)
 
+
 @dataclass
 class QuantumResult:
     """Real quantum annealing result from D-Wave"""
+
     samples: List[Dict[str, int]]
     energies: List[float]
     num_occurrences: List[int]
@@ -53,47 +56,54 @@ class QuantumResult:
     success: bool
     error_message: Optional[str] = None
 
+
 class DWaveQuantumConnector(MCPConnector):
     """
     Real D-Wave Quantum MCP Connector
-    
+
     Provides authentic quantum annealing capabilities through D-Wave Leap cloud service.
     Uses actual D-Wave Ocean SDK - no simulations or fake results.
     """
-    
-    def __init__(self, api_token: Optional[str] = None, solver_name: Optional[str] = None):
+
+    def __init__(
+        self, api_token: Optional[str] = None, solver_name: Optional[str] = None
+    ):
         super().__init__("dwave_quantum", "quantum_computing")
         self.api_token = api_token
-        self.solver_name = solver_name  # e.g., "Advantage_system6.4" or "Advantage2_prototype"
+        self.solver_name = (
+            solver_name  # e.g., "Advantage_system6.4" or "Advantage2_prototype"
+        )
         self.sampler = None
         self.client = None
         self.solver_info = {}
-        
+
         if not DWAVE_AVAILABLE:
-            logger.error("D-Wave Ocean SDK not installed. Run: pip install dwave-ocean-sdk")
-    
+            logger.error(
+                "D-Wave Ocean SDK not installed. Run: pip install dwave-ocean-sdk"
+            )
+
     async def connect(self, config: Dict[str, Any]) -> bool:
         """Connect to D-Wave Leap cloud service"""
         if not DWAVE_AVAILABLE:
             logger.error("D-Wave Ocean SDK not available")
             return False
-        
+
         try:
             # Get configuration
-            self.api_token = config.get('api_token', self.api_token)
-            self.solver_name = config.get('solver_name', self.solver_name)
-            
+            self.api_token = config.get("api_token", self.api_token)
+            self.solver_name = config.get("solver_name", self.solver_name)
+
             # Initialize D-Wave client
             if self.api_token:
                 self.client = Client.from_config(token=self.api_token)
             else:
                 # Try to use default configuration
                 self.client = Client.from_config()
-            
+
             # Get available solvers
             solvers = self.client.get_solvers()
-            qpu_solvers = [s for s in solvers if hasattr(s, 'qubits')]
-            
+            qpu_solvers = [s for s in solvers if hasattr(s, "qubits")]
+
             if not qpu_solvers:
                 logger.warning("No QPU solvers available, using simulated annealing")
                 self.sampler = SimulatedAnnealingSampler()
@@ -101,65 +111,74 @@ class DWaveQuantumConnector(MCPConnector):
                     "name": "SimulatedAnnealingSampler",
                     "type": "software",
                     "num_qubits": "unlimited",
-                    "connectivity": "complete"
+                    "connectivity": "complete",
                 }
             else:
                 # Use specified solver or first available QPU
                 if self.solver_name:
-                    solver = next((s for s in qpu_solvers if self.solver_name in s.id), qpu_solvers[0])
+                    solver = next(
+                        (s for s in qpu_solvers if self.solver_name in s.id),
+                        qpu_solvers[0],
+                    )
                 else:
                     solver = qpu_solvers[0]
-                
+
                 self.sampler = EmbeddingComposite(DWaveSampler(solver=solver.id))
                 self.solver_info = {
                     "name": solver.id,
                     "type": "QPU",
                     "num_qubits": len(solver.nodes),
                     "num_couplers": len(solver.edges),
-                    "topology": getattr(solver, 'topology', 'Unknown'),
-                    "programming_thermalization": solver.properties.get('programming_thermalization_range'),
-                    "annealing_time_range": solver.properties.get('annealing_time_range')
+                    "topology": getattr(solver, "topology", "Unknown"),
+                    "programming_thermalization": solver.properties.get(
+                        "programming_thermalization_range"
+                    ),
+                    "annealing_time_range": solver.properties.get(
+                        "annealing_time_range"
+                    ),
                 }
-            
+
             self.connected = True
             logger.info(f"Connected to D-Wave solver: {self.solver_info['name']}")
             return True
-            
+
         except Exception as e:
             logger.error(f"Failed to connect to D-Wave: {e}")
             self.connected = False
             return False
-    
+
     async def disconnect(self) -> bool:
         """Disconnect from D-Wave service"""
         if self.client:
             self.client.close()
         self.connected = False
         return True
-    
+
     async def get_context(self):
         """Get quantum system context"""
         return self.context
-    
+
     async def send_context(self, context) -> bool:
         """Send context to quantum system"""
         self.context = context
         return True
-    
-    async def execute_action(self, action: str, params: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def execute_action(
+        self, action: str, params: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Execute quantum action"""
         if not self.connected:
             return {"error": "Not connected to D-Wave service"}
-        
+
         actions = {
-            'solve_qubo': self.solve_qubo,
-            'solve_ising': self.solve_ising,
-            'traveling_salesman': self.solve_tsp,
-            'max_cut': self.solve_max_cut,
-            'knapsack': self.solve_knapsack,
-            'get_solver_info': self.get_solver_info
+            "solve_qubo": self.solve_qubo,
+            "solve_ising": self.solve_ising,
+            "traveling_salesman": self.solve_tsp,
+            "max_cut": self.solve_max_cut,
+            "knapsack": self.solve_knapsack,
+            "get_solver_info": self.get_solver_info,
         }
-        
+
         handler = actions.get(action)
         if handler:
             try:
@@ -167,160 +186,174 @@ class DWaveQuantumConnector(MCPConnector):
                 return result
             except Exception as e:
                 return {"error": str(e), "action": action}
-        
+
         return {"error": f"Unknown action: {action}"}
-    
+
     async def solve_qubo(self, params: Dict[str, Any]) -> Dict[str, Any]:
         """
         Solve Quadratic Unconstrained Binary Optimization (QUBO) problem
-        
+
         Based on D-Wave examples: https://github.com/dwave-examples
         """
         try:
-            qubo_dict = params.get('qubo', {})
-            num_reads = params.get('num_reads', 100)
-            annealing_time = params.get('annealing_time', 20)  # microseconds
-            
+            qubo_dict = params.get("qubo", {})
+            num_reads = params.get("num_reads", 100)
+            annealing_time = params.get("annealing_time", 20)  # microseconds
+
             if not qubo_dict:
                 return {"error": "QUBO dictionary required"}
-            
+
             # Convert string keys to tuples if needed
             if isinstance(list(qubo_dict.keys())[0], str):
                 # Handle string representation like "x0*x1": coeff
                 processed_qubo = {}
                 for key, value in qubo_dict.items():
-                    if '*' in key:
-                        vars = key.split('*')
+                    if "*" in key:
+                        vars = key.split("*")
                         i, j = int(vars[0][1:]), int(vars[1][1:])
                         processed_qubo[(i, j)] = value
                     else:
                         i = int(key[1:])
                         processed_qubo[(i, i)] = value
                 qubo_dict = processed_qubo
-            
+
             # Create BQM
             bqm = dimod.BinaryQuadraticModel.from_qubo(qubo_dict)
-            
+
             # Sample using real D-Wave hardware or simulator
             sampleset = self.sampler.sample(
                 bqm,
                 num_reads=num_reads,
-                annealing_time=annealing_time if hasattr(self.sampler, 'annealing_time') else None,
-                return_embedding=True
+                annealing_time=(
+                    annealing_time if hasattr(self.sampler, "annealing_time") else None
+                ),
+                return_embedding=True,
             )
-            
+
             # Process results
             best_sample = sampleset.first.sample
             best_energy = sampleset.first.energy
-            
+
             # Calculate chain break information if available
             chain_break_fraction = 0.0
-            if hasattr(sampleset, 'data_vectors') and 'chain_break_fraction' in sampleset.data_vectors:
-                chain_break_fraction = np.mean(sampleset.data_vectors['chain_break_fraction'])
-            
+            if (
+                hasattr(sampleset, "data_vectors")
+                and "chain_break_fraction" in sampleset.data_vectors
+            ):
+                chain_break_fraction = np.mean(
+                    sampleset.data_vectors["chain_break_fraction"]
+                )
+
             return {
                 "success": True,
                 "best_solution": best_sample,
                 "best_energy": best_energy,
                 "num_solutions": len(sampleset),
                 "chain_break_fraction": chain_break_fraction,
-                "timing": sampleset.info.get('timing', {}),
+                "timing": sampleset.info.get("timing", {}),
                 "solver_info": self.solver_info,
-                "all_samples": [dict(sample) for sample in sampleset.samples()[:10]],  # First 10
-                "energies": list(sampleset.data_vectors['energy'][:10])
+                "all_samples": [
+                    dict(sample) for sample in sampleset.samples()[:10]
+                ],  # First 10
+                "energies": list(sampleset.data_vectors["energy"][:10]),
             }
-            
+
         except Exception as e:
             logger.error(f"QUBO solving failed: {e}")
             return {"success": False, "error": str(e)}
-    
+
     async def solve_ising(self, params: Dict[str, Any]) -> Dict[str, Any]:
         """
         Solve Ising model problem
-        
+
         Based on real D-Wave Ising formulations
         """
         try:
-            h = params.get('h', {})  # Linear terms
-            J = params.get('J', {})  # Quadratic terms
-            num_reads = params.get('num_reads', 100)
-            
+            h = params.get("h", {})  # Linear terms
+            J = params.get("J", {})  # Quadratic terms
+            num_reads = params.get("num_reads", 100)
+
             # Create BQM from Ising model
             bqm = dimod.BinaryQuadraticModel.from_ising(h, J)
-            
+
             # Sample using D-Wave
             sampleset = self.sampler.sample(bqm, num_reads=num_reads)
-            
+
             best_sample = sampleset.first.sample
             best_energy = sampleset.first.energy
-            
+
             return {
                 "success": True,
                 "best_solution": best_sample,
                 "best_energy": best_energy,
                 "solver_info": self.solver_info,
-                "timing": sampleset.info.get('timing', {})
+                "timing": sampleset.info.get("timing", {}),
             }
-            
+
         except Exception as e:
             return {"success": False, "error": str(e)}
-    
+
     async def solve_tsp(self, params: Dict[str, Any]) -> Dict[str, Any]:
         """
         Solve Traveling Salesman Problem using quantum annealing
-        
+
         Based on: https://github.com/dwave-examples/tsp
         """
         try:
-            cities = params.get('cities', [])
-            distances = params.get('distances', {})
-            
+            cities = params.get("cities", [])
+            distances = params.get("distances", {})
+
             if len(cities) < 3:
                 return {"error": "Need at least 3 cities for TSP"}
-            
+
             n = len(cities)
-            
+
             # Create QUBO formulation for TSP
             # Variables: x_i_t = 1 if city i is visited at time t
             Q = {}
-            
+
             # Constraint: Each city visited exactly once
             for i in range(n):
                 for t1 in range(n):
-                    for t2 in range(t1+1, n):
-                        Q[(i*n + t1, i*n + t2)] = 2  # Penalty for visiting city i at multiple times
-            
+                    for t2 in range(t1 + 1, n):
+                        Q[(i * n + t1, i * n + t2)] = (
+                            2  # Penalty for visiting city i at multiple times
+                        )
+
             # Constraint: Each time slot has exactly one city
             for t in range(n):
                 for i1 in range(n):
-                    for i2 in range(i1+1, n):
-                        Q[(i1*n + t, i2*n + t)] = 2  # Penalty for multiple cities at time t
-            
+                    for i2 in range(i1 + 1, n):
+                        Q[(i1 * n + t, i2 * n + t)] = (
+                            2  # Penalty for multiple cities at time t
+                        )
+
             # Objective: Minimize total distance
             for i in range(n):
                 for j in range(n):
                     if i != j:
-                        dist = distances.get(f"{cities[i]}-{cities[j]}", distances.get((i, j), 1))
+                        dist = distances.get(
+                            f"{cities[i]}-{cities[j]}", distances.get((i, j), 1)
+                        )
                         for t in range(n):
                             t_next = (t + 1) % n
-                            Q[(i*n + t, j*n + t_next)] = dist
-            
+                            Q[(i * n + t, j * n + t_next)] = dist
+
             # Solve QUBO
-            result = await self.solve_qubo({
-                'qubo': Q,
-                'num_reads': params.get('num_reads', 100)
-            })
-            
-            if result.get('success'):
+            result = await self.solve_qubo(
+                {"qubo": Q, "num_reads": params.get("num_reads", 100)}
+            )
+
+            if result.get("success"):
                 # Convert solution back to route
-                solution = result['best_solution']
-                route = [''] * n
+                solution = result["best_solution"]
+                route = [""] * n
                 for var, val in solution.items():
                     if val == 1:
                         city_idx = var // n
                         time_idx = var % n
                         route[time_idx] = cities[city_idx]
-                
+
                 # Calculate total distance
                 total_distance = 0
                 for i in range(n):
@@ -328,31 +361,33 @@ class DWaveQuantumConnector(MCPConnector):
                     next_city = route[(i + 1) % n]
                     current_idx = cities.index(current_city)
                     next_idx = cities.index(next_city)
-                    total_distance += distances.get(f"{current_city}-{next_city}", 
-                                                  distances.get((current_idx, next_idx), 1))
-                
-                result['route'] = route
-                result['total_distance'] = total_distance
-                result['problem_type'] = 'TSP'
-            
+                    total_distance += distances.get(
+                        f"{current_city}-{next_city}",
+                        distances.get((current_idx, next_idx), 1),
+                    )
+
+                result["route"] = route
+                result["total_distance"] = total_distance
+                result["problem_type"] = "TSP"
+
             return result
-            
+
         except Exception as e:
             return {"success": False, "error": str(e), "problem_type": "TSP"}
-    
+
     async def solve_max_cut(self, params: Dict[str, Any]) -> Dict[str, Any]:
         """
         Solve Maximum Cut problem
-        
+
         Based on D-Wave graph partitioning examples
         """
         try:
-            graph_edges = params.get('edges', [])
-            weights = params.get('weights', {})
-            
+            graph_edges = params.get("edges", [])
+            weights = params.get("weights", {})
+
             # Create QUBO for Max-Cut
             Q = {}
-            
+
             for i, (u, v) in enumerate(graph_edges):
                 weight = weights.get((u, v), weights.get((v, u), 1))
                 # Max-Cut: maximize sum of weights for cut edges
@@ -360,133 +395,136 @@ class DWaveQuantumConnector(MCPConnector):
                 Q[(u, u)] = Q.get((u, u), 0) - weight
                 Q[(v, v)] = Q.get((v, v), 0) - weight
                 Q[(u, v)] = Q.get((u, v), 0) + 2 * weight
-            
-            result = await self.solve_qubo({
-                'qubo': Q,
-                'num_reads': params.get('num_reads', 100)
-            })
-            
-            if result.get('success'):
-                solution = result['best_solution']
+
+            result = await self.solve_qubo(
+                {"qubo": Q, "num_reads": params.get("num_reads", 100)}
+            )
+
+            if result.get("success"):
+                solution = result["best_solution"]
                 set_a = [node for node, val in solution.items() if val == 0]
                 set_b = [node for node, val in solution.items() if val == 1]
-                
+
                 # Calculate cut value
                 cut_value = 0
                 for u, v in graph_edges:
                     if (u in set_a and v in set_b) or (u in set_b and v in set_a):
                         cut_value += weights.get((u, v), weights.get((v, u), 1))
-                
-                result['partition_a'] = set_a
-                result['partition_b'] = set_b
-                result['cut_value'] = cut_value
-                result['problem_type'] = 'Max-Cut'
-            
+
+                result["partition_a"] = set_a
+                result["partition_b"] = set_b
+                result["cut_value"] = cut_value
+                result["problem_type"] = "Max-Cut"
+
             return result
-            
+
         except Exception as e:
             return {"success": False, "error": str(e), "problem_type": "Max-Cut"}
-    
+
     async def solve_knapsack(self, params: Dict[str, Any]) -> Dict[str, Any]:
         """
         Solve 0-1 Knapsack problem using quantum annealing
         """
         try:
-            items = params.get('items', [])  # List of {'weight': w, 'value': v}
-            capacity = params.get('capacity', 10)
-            
+            items = params.get("items", [])  # List of {'weight': w, 'value': v}
+            capacity = params.get("capacity", 10)
+
             n = len(items)
             if n == 0:
                 return {"error": "No items provided"}
-            
+
             # QUBO formulation for knapsack
             Q = {}
-            penalty = max(item['value'] for item in items) * 2  # Large penalty
-            
+            penalty = max(item["value"] for item in items) * 2  # Large penalty
+
             # Objective: maximize value (minimize negative value)
             for i, item in enumerate(items):
-                Q[(i, i)] = -item['value']
-            
+                Q[(i, i)] = -item["value"]
+
             # Constraint: weight <= capacity
             # (sum(w_i * x_i) - capacity)^2 penalty term
             for i in range(n):
                 for j in range(i, n):
-                    weight_product = items[i]['weight'] * items[j]['weight']
+                    weight_product = items[i]["weight"] * items[j]["weight"]
                     if i == j:
-                        Q[(i, i)] += penalty * (weight_product - 2 * capacity * items[i]['weight'])
+                        Q[(i, i)] += penalty * (
+                            weight_product - 2 * capacity * items[i]["weight"]
+                        )
                     else:
                         Q[(i, j)] = Q.get((i, j), 0) + penalty * weight_product
-            
+
             # Add capacity^2 term (constant, doesn't affect optimization)
-            
-            result = await self.solve_qubo({
-                'qubo': Q,
-                'num_reads': params.get('num_reads', 100)
-            })
-            
-            if result.get('success'):
-                solution = result['best_solution']
+
+            result = await self.solve_qubo(
+                {"qubo": Q, "num_reads": params.get("num_reads", 100)}
+            )
+
+            if result.get("success"):
+                solution = result["best_solution"]
                 selected_items = [i for i, val in solution.items() if val == 1]
-                total_weight = sum(items[i]['weight'] for i in selected_items)
-                total_value = sum(items[i]['value'] for i in selected_items)
-                
-                result['selected_items'] = selected_items
-                result['total_weight'] = total_weight
-                result['total_value'] = total_value
-                result['capacity_used'] = total_weight / capacity
-                result['feasible'] = total_weight <= capacity
-                result['problem_type'] = 'Knapsack'
-            
+                total_weight = sum(items[i]["weight"] for i in selected_items)
+                total_value = sum(items[i]["value"] for i in selected_items)
+
+                result["selected_items"] = selected_items
+                result["total_weight"] = total_weight
+                result["total_value"] = total_value
+                result["capacity_used"] = total_weight / capacity
+                result["feasible"] = total_weight <= capacity
+                result["problem_type"] = "Knapsack"
+
             return result
-            
+
         except Exception as e:
             return {"success": False, "error": str(e), "problem_type": "Knapsack"}
-    
+
     async def get_solver_info(self, params: Dict[str, Any] = None) -> Dict[str, Any]:
         """Get information about the connected D-Wave solver"""
         if not self.connected:
             return {"error": "Not connected to D-Wave service"}
-        
+
         return {
             "success": True,
             "solver_info": self.solver_info,
             "available": DWAVE_AVAILABLE,
-            "connection_status": "connected" if self.connected else "disconnected"
+            "connection_status": "connected" if self.connected else "disconnected",
         }
 
+
 # Example usage and testing
 async def example_usage():
     """Example of using the real D-Wave quantum connector"""
-    
+
     # NOTE: Requires valid D-Wave Leap account and API token
     connector = DWaveQuantumConnector()
-    
+
     # Connect (will use default config or environment variables)
     success = await connector.connect({})
-    
+
     if success:
         print("✅ Connected to D-Wave quantum system")
-        
+
         # Get solver information
-        solver_info = await connector.execute_action('get_solver_info', {})
+        solver_info = await connector.execute_action("get_solver_info", {})
         print(f"Solver: {solver_info}")
-        
+
         # Solve a simple QUBO problem
         # Example: x0 + x1 - 2*x0*x1 (prefer x0=1, x1=0 or x0=0, x1=1)
-        qubo_result = await connector.execute_action('solve_qubo', {
-            'qubo': {(0, 0): 1, (1, 1): 1, (0, 1): -2},
-            'num_reads': 100
-        })
+        qubo_result = await connector.execute_action(
+            "solve_qubo", {"qubo": {(0, 0): 1, (1, 1): 1, (0, 1): -2}, "num_reads": 100}
+        )
         print(f"QUBO Result: {qubo_result}")
-        
+
         # Solve TSP
-        tsp_result = await connector.execute_action('traveling_salesman', {
-            'cities': ['A', 'B', 'C'],
-            'distances': {('A', 'B'): 2, ('B', 'C'): 3, ('C', 'A'): 1},
-            'num_reads': 50
-        })
+        tsp_result = await connector.execute_action(
+            "traveling_salesman",
+            {
+                "cities": ["A", "B", "C"],
+                "distances": {("A", "B"): 2, ("B", "C"): 3, ("C", "A"): 1},
+                "num_reads": 50,
+            },
+        )
         print(f"TSP Result: {tsp_result}")
-        
+
         await connector.disconnect()
     else:
         print("❌ Failed to connect to D-Wave")
@@ -494,10 +532,11 @@ async def example_usage():
         print("Install: pip install dwave-ocean-sdk")
         print("Setup: https://cloud.dwavesys.com/leap/")
 
+
 if __name__ == "__main__":
     if DWAVE_AVAILABLE:
         asyncio.run(example_usage())
     else:
         print("D-Wave Ocean SDK not installed")
         print("Install with: pip install dwave-ocean-sdk")
-        print("Sign up for D-Wave Leap: https://cloud.dwavesys.com/leap/")
\ No newline at end of file
+        print("Sign up for D-Wave Leap: https://cloud.dwavesys.com/leap/")
diff --git a/connectors/llm_connector.py b/connectors/llm_connector.py
index 27a8c90..ad6cbc2 100644
--- a/connectors/llm_connector.py
+++ b/connectors/llm_connector.py
@@ -8,15 +8,16 @@ from typing import Dict, List, Any, Optional
 from datetime import datetime
 import aiohttp
 
+
 class LLMConnector:
     """
     Real LLM connector with actual API integration
     """
-    
+
     def __init__(self):
-        self.openai_key = os.environ.get('OPENAI_API_KEY', '')
-        self.anthropic_key = os.environ.get('ANTHROPIC_API_KEY', '')
-        
+        self.openai_key = os.environ.get("OPENAI_API_KEY", "")
+        self.anthropic_key = os.environ.get("ANTHROPIC_API_KEY", "")
+
     async def analyze_multimodal(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         """Real multi-modal analysis using OpenAI GPT-4V"""
         if self.openai_key:
@@ -24,15 +25,15 @@ class LLMConnector:
         else:
             # Use local analysis if no API key
             return await self._local_analyze(inputs)
-    
+
     async def _openai_analyze(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         """Real OpenAI API call"""
         url = "https://api.openai.com/v1/chat/completions"
         headers = {
             "Authorization": f"Bearer {self.openai_key}",
-            "Content-Type": "application/json"
+            "Content-Type": "application/json",
         }
-        
+
         prompt = f"""
 Analyze this system data and provide actionable insights:
 
@@ -47,96 +48,112 @@ Return a JSON response with:
 
 Format as valid JSON only.
 """
-        
+
         payload = {
             "model": "gpt-4-turbo-preview",
             "messages": [
-                {"role": "system", "content": "You are a system analyst. Return only valid JSON."},
-                {"role": "user", "content": prompt}
+                {
+                    "role": "system",
+                    "content": "You are a system analyst. Return only valid JSON.",
+                },
+                {"role": "user", "content": prompt},
             ],
             "temperature": 0.3,
-            "max_tokens": 2000
+            "max_tokens": 2000,
         }
-        
+
         try:
             async with aiohttp.ClientSession() as session:
                 async with session.post(url, headers=headers, json=payload) as response:
                     if response.status == 200:
                         data = await response.json()
-                        content = data['choices'][0]['message']['content']
+                        content = data["choices"][0]["message"]["content"]
                         return json.loads(content)
                     else:
                         return await self._local_analyze(inputs)
         except Exception as e:
             print(f"OpenAI API error: {e}")
             return await self._local_analyze(inputs)
-    
+
     async def _local_analyze(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
         """Real local analysis without external APIs"""
         patterns = []
         optimizations = []
         new_ideas = []
-        
+
         # Analyze execution history
-        exec_history = inputs.get('execution_history', {})
-        if exec_history.get('success_patterns'):
-            for pattern in exec_history['success_patterns']:
-                if pattern['success_rate'] > 0.8:
-                    patterns.append({
-                        'type': 'high_performance',
-                        'protocol': pattern['protocol'],
-                        'success_rate': pattern['success_rate'],
-                        'insight': f"Protocol {pattern['protocol']} shows {pattern['success_rate']*100:.1f}% success rate",
-                        'recommendation': f"Use {pattern['protocol']} as template for similar tasks"
-                    })
-        
+        exec_history = inputs.get("execution_history", {})
+        if exec_history.get("success_patterns"):
+            for pattern in exec_history["success_patterns"]:
+                if pattern["success_rate"] > 0.8:
+                    patterns.append(
+                        {
+                            "type": "high_performance",
+                            "protocol": pattern["protocol"],
+                            "success_rate": pattern["success_rate"],
+                            "insight": f"Protocol {pattern['protocol']} shows {pattern['success_rate']*100:.1f}% success rate",
+                            "recommendation": f"Use {pattern['protocol']} as template for similar tasks",
+                        }
+                    )
+
         # Analyze system metrics
-        metrics = inputs.get('system_metrics', {})
+        metrics = inputs.get("system_metrics", {})
         if metrics:
-            if metrics.get('memory_usage', 0) > 0.7:
-                optimizations.append({
-                    'area': 'memory',
-                    'current': f"{metrics['memory_usage']*100:.1f}%",
-                    'action': 'Implement memory pooling and garbage collection optimization',
-                    'priority': 'high'
-                })
-            
-            if metrics.get('cache_hit_rate', 1.0) < 0.8:
-                optimizations.append({
-                    'area': 'caching',
-                    'current': f"{metrics.get('cache_hit_rate', 0)*100:.1f}%",
-                    'action': 'Implement predictive cache warming based on usage patterns',
-                    'priority': 'medium'
-                })
-        
+            if metrics.get("memory_usage", 0) > 0.7:
+                optimizations.append(
+                    {
+                        "area": "memory",
+                        "current": f"{metrics['memory_usage']*100:.1f}%",
+                        "action": "Implement memory pooling and garbage collection optimization",
+                        "priority": "high",
+                    }
+                )
+
+            if metrics.get("cache_hit_rate", 1.0) < 0.8:
+                optimizations.append(
+                    {
+                        "area": "caching",
+                        "current": f"{metrics.get('cache_hit_rate', 0)*100:.1f}%",
+                        "action": "Implement predictive cache warming based on usage patterns",
+                        "priority": "medium",
+                    }
+                )
+
         # Generate new protocol ideas based on real data
-        mutation_data = inputs.get('protocol_mutations', [])
-        successful_mutations = [m for m in mutation_data if m.get('improvement', 0) > 0.2]
-        
+        mutation_data = inputs.get("protocol_mutations", [])
+        successful_mutations = [
+            m for m in mutation_data if m.get("improvement", 0) > 0.2
+        ]
+
         if successful_mutations:
-            new_ideas.append({
-                'name': 'auto_mutation_engine',
-                'description': 'Automatically apply successful mutation patterns to underperforming protocols',
-                'rationale': f"Found {len(successful_mutations)} mutations with >20% improvement",
-                'implementation': 'Create ML model to predict beneficial mutations'
-            })
-        
+            new_ideas.append(
+                {
+                    "name": "auto_mutation_engine",
+                    "description": "Automatically apply successful mutation patterns to underperforming protocols",
+                    "rationale": f"Found {len(successful_mutations)} mutations with >20% improvement",
+                    "implementation": "Create ML model to predict beneficial mutations",
+                }
+            )
+
         # Add real protocol ideas based on actual system needs
-        if exec_history.get('total_executions', 0) > 100:
-            new_ideas.append({
-                'name': 'execution_pattern_predictor',
-                'description': 'Predict optimal protocol selection based on historical patterns',
-                'rationale': f"System has {exec_history.get('total_executions')} executions to learn from",
-                'implementation': 'Train lightweight ML model on execution history'
-            })
-        
+        if exec_history.get("total_executions", 0) > 100:
+            new_ideas.append(
+                {
+                    "name": "execution_pattern_predictor",
+                    "description": "Predict optimal protocol selection based on historical patterns",
+                    "rationale": f"System has {exec_history.get('total_executions')} executions to learn from",
+                    "implementation": "Train lightweight ML model on execution history",
+                }
+            )
+
         return {
-            'patterns': patterns,
-            'optimizations': optimizations,
-            'new_ideas': new_ideas,
-            'analysis_confidence': 0.85,
-            'timestamp': datetime.utcnow().isoformat()
+            "patterns": patterns,
+            "optimizations": optimizations,
+            "new_ideas": new_ideas,
+            "analysis_confidence": 0.85,
+            "timestamp": datetime.utcnow().isoformat(),
         }
 
+
 # Global instance
-llm_connector = LLMConnector() 
\ No newline at end of file
+llm_connector = LLMConnector()
diff --git a/connectors/mcp_base.py b/connectors/mcp_base.py
index 8b7e546..0327123 100644
--- a/connectors/mcp_base.py
+++ b/connectors/mcp_base.py
@@ -7,9 +7,10 @@ import json
 import asyncio
 from datetime import datetime
 
+
 class MCPContext:
     """Universal context object for MCP"""
-    
+
     def __init__(self):
         self.user = {}
         self.task = {}
@@ -18,75 +19,75 @@ class MCPContext:
         self.code_state = {}
         self.history = []
         self.metadata = {
-            'created_at': datetime.utcnow().isoformat(),
-            'version': '1.0',
-            'protocol': 'MCP'
+            "created_at": datetime.utcnow().isoformat(),
+            "version": "1.0",
+            "protocol": "MCP",
         }
-    
+
     def to_dict(self) -> Dict:
         return {
-            'user': self.user,
-            'task': self.task,
-            'intent': self.intent,
-            'env': self.env,
-            'code_state': self.code_state,
-            'history': self.history,
-            'metadata': self.metadata
+            "user": self.user,
+            "task": self.task,
+            "intent": self.intent,
+            "env": self.env,
+            "code_state": self.code_state,
+            "history": self.history,
+            "metadata": self.metadata,
         }
-    
+
     @classmethod
-    def from_dict(cls, data: Dict) -> 'MCPContext':
+    def from_dict(cls, data: Dict) -> "MCPContext":
         context = cls()
-        context.user = data.get('user', {})
-        context.task = data.get('task', {})
-        context.intent = data.get('intent', {})
-        context.env = data.get('env', {})
-        context.code_state = data.get('code_state', {})
-        context.history = data.get('history', [])
-        context.metadata = data.get('metadata', context.metadata)
+        context.user = data.get("user", {})
+        context.task = data.get("task", {})
+        context.intent = data.get("intent", {})
+        context.env = data.get("env", {})
+        context.code_state = data.get("code_state", {})
+        context.history = data.get("history", [])
+        context.metadata = data.get("metadata", context.metadata)
         return context
 
 
 class MCPConnector(ABC):
     """Base class for all MCP-compliant connectors"""
-    
+
     def __init__(self, connector_id: str, service_type: str):
         self.connector_id = connector_id
         self.service_type = service_type
         self.connected = False
         self.context = MCPContext()
         self.capabilities = []
-        
+
     @abstractmethod
     async def connect(self, config: Dict) -> bool:
         """Establish connection to external service"""
         pass
-    
+
     @abstractmethod
     async def disconnect(self) -> bool:
         """Disconnect from external service"""
         pass
-    
+
     @abstractmethod
     async def get_context(self) -> MCPContext:
         """Get current context from service"""
         pass
-    
+
     @abstractmethod
     async def send_context(self, context: MCPContext) -> bool:
         """Send context to service"""
         pass
-    
+
     @abstractmethod
     async def execute_action(self, action: str, params: Dict) -> Dict:
         """Execute action on external service"""
         pass
-    
+
     async def sync_context(self, local_context: MCPContext) -> MCPContext:
         """Synchronize context between local and remote"""
         # Get remote context
         remote_context = await self.get_context()
-        
+
         # Merge contexts (simplified - real implementation would handle conflicts)
         merged = MCPContext()
         merged.user = {**remote_context.user, **local_context.user}
@@ -94,291 +95,288 @@ class MCPConnector(ABC):
         merged.intent = {**remote_context.intent, **local_context.intent}
         merged.env = {**remote_context.env, **local_context.env}
         merged.code_state = {**remote_context.code_state, **local_context.code_state}
-        
+
         # Update history
-        merged.history = remote_context.history + [{
-            'action': 'context_sync',
-            'timestamp': datetime.utcnow().isoformat(),
-            'source': self.connector_id
-        }]
-        
+        merged.history = remote_context.history + [
+            {
+                "action": "context_sync",
+                "timestamp": datetime.utcnow().isoformat(),
+                "source": self.connector_id,
+            }
+        ]
+
         # Send merged context back
         await self.send_context(merged)
-        
+
         return merged
 
 
 class GitHubMCPConnector(MCPConnector):
     """MCP connector for GitHub"""
-    
+
     def __init__(self):
         super().__init__("github_mcp", "version_control")
         self.capabilities = [
             "code_retrieval",
             "issue_tracking",
             "pr_management",
-            "context_extraction"
+            "context_extraction",
         ]
-        
+
     async def connect(self, config: Dict) -> bool:
         """Connect to GitHub API"""
-        self.api_token = config.get('api_token')
-        self.repo = config.get('repository')
-        
+        self.api_token = config.get("api_token")
+        self.repo = config.get("repository")
+
         if not self.api_token or not self.repo:
             return False
-            
+
         # Test connection
         # In real implementation, would make API call
         self.connected = True
         return True
-    
+
     async def disconnect(self) -> bool:
         """Disconnect from GitHub"""
         self.connected = False
         return True
-    
+
     async def get_context(self) -> MCPContext:
         """Extract context from GitHub"""
         context = MCPContext()
-        
+
         if not self.connected:
             return context
-            
+
         # Extract repository context
         context.code_state = {
-            'repository': self.repo,
-            'branch': 'main',  # Would get actual branch
-            'last_commit': 'abc123',  # Would get actual commit
-            'open_issues': 5,  # Would count actual issues
-            'open_prs': 2  # Would count actual PRs
+            "repository": self.repo,
+            "branch": "main",  # Would get actual branch
+            "last_commit": "abc123",  # Would get actual commit
+            "open_issues": 5,  # Would count actual issues
+            "open_prs": 2,  # Would count actual PRs
         }
-        
+
         context.env = {
-            'platform': 'github',
-            'api_version': 'v3',
-            'rate_limit': 5000  # Would get actual rate limit
+            "platform": "github",
+            "api_version": "v3",
+            "rate_limit": 5000,  # Would get actual rate limit
         }
-        
+
         return context
-    
+
     async def send_context(self, context: MCPContext) -> bool:
         """Send context to GitHub (e.g., as issue comment)"""
         if not self.connected:
             return False
-            
+
         # In real implementation, might create issue/PR comment with context
         return True
-    
+
     async def execute_action(self, action: str, params: Dict) -> Dict:
         """Execute GitHub action"""
         if not self.connected:
-            return {'error': 'Not connected'}
-            
+            return {"error": "Not connected"}
+
         actions = {
-            'get_code': self._get_code,
-            'create_issue': self._create_issue,
-            'get_pr_context': self._get_pr_context
+            "get_code": self._get_code,
+            "create_issue": self._create_issue,
+            "get_pr_context": self._get_pr_context,
         }
-        
+
         handler = actions.get(action)
         if handler:
             return await handler(params)
-            
-        return {'error': f'Unknown action: {action}'}
-    
+
+        return {"error": f"Unknown action: {action}"}
+
     async def _get_code(self, params: Dict) -> Dict:
         """Get code from repository"""
-        file_path = params.get('file_path')
+        file_path = params.get("file_path")
         # In real implementation, would fetch from GitHub API
         return {
-            'file_path': file_path,
-            'content': '# Example code',
-            'language': 'python'
+            "file_path": file_path,
+            "content": "# Example code",
+            "language": "python",
         }
-    
+
     async def _create_issue(self, params: Dict) -> Dict:
         """Create GitHub issue"""
         # In real implementation, would create via API
         return {
-            'issue_number': 123,
-            'url': f'https://github.com/{self.repo}/issues/123'
+            "issue_number": 123,
+            "url": f"https://github.com/{self.repo}/issues/123",
         }
-    
+
     async def _get_pr_context(self, params: Dict) -> Dict:
         """Get PR context"""
-        pr_number = params.get('pr_number')
+        pr_number = params.get("pr_number")
         # In real implementation, would fetch PR details
         return {
-            'pr_number': pr_number,
-            'title': 'Example PR',
-            'description': 'PR description',
-            'files_changed': 5,
-            'additions': 100,
-            'deletions': 50
+            "pr_number": pr_number,
+            "title": "Example PR",
+            "description": "PR description",
+            "files_changed": 5,
+            "additions": 100,
+            "deletions": 50,
         }
 
 
 class ClaudeMCPConnector(MCPConnector):
     """MCP connector for Claude AI"""
-    
+
     def __init__(self):
         super().__init__("claude_mcp", "ai_assistant")
         self.capabilities = [
             "natural_language_processing",
             "code_generation",
             "context_understanding",
-            "reasoning"
+            "reasoning",
         ]
-        
+
     async def connect(self, config: Dict) -> bool:
         """Connect to Claude API"""
-        self.api_key = config.get('api_key')
-        self.model = config.get('model', 'claude-3-opus')
-        
+        self.api_key = config.get("api_key")
+        self.model = config.get("model", "claude-3-opus")
+
         if not self.api_key:
             return False
-            
+
         self.connected = True
         return True
-    
+
     async def disconnect(self) -> bool:
         """Disconnect from Claude"""
         self.connected = False
         return True
-    
+
     async def get_context(self) -> MCPContext:
         """Get context from Claude conversation"""
         context = MCPContext()
-        
+
         if not self.connected:
             return context
-            
+
         # In real implementation, would maintain conversation context
         context.task = {
-            'current_conversation': 'active',
-            'messages_count': 10,
-            'tokens_used': 1500
+            "current_conversation": "active",
+            "messages_count": 10,
+            "tokens_used": 1500,
         }
-        
-        context.intent = {
-            'detected_intent': 'code_generation',
-            'confidence': 0.95
-        }
-        
+
+        context.intent = {"detected_intent": "code_generation", "confidence": 0.95}
+
         return context
-    
+
     async def send_context(self, context: MCPContext) -> bool:
         """Send context to Claude"""
         if not self.connected:
             return False
-            
+
         # In real implementation, would include context in prompts
         return True
-    
+
     async def execute_action(self, action: str, params: Dict) -> Dict:
         """Execute Claude action"""
         if not self.connected:
-            return {'error': 'Not connected'}
-            
+            return {"error": "Not connected"}
+
         actions = {
-            'generate_code': self._generate_code,
-            'analyze_intent': self._analyze_intent,
-            'reason_about': self._reason_about
+            "generate_code": self._generate_code,
+            "analyze_intent": self._analyze_intent,
+            "reason_about": self._reason_about,
         }
-        
+
         handler = actions.get(action)
         if handler:
             return await handler(params)
-            
-        return {'error': f'Unknown action: {action}'}
-    
+
+        return {"error": f"Unknown action: {action}"}
+
     async def _generate_code(self, params: Dict) -> Dict:
         """Generate code using Claude"""
-        prompt = params.get('prompt')
-        language = params.get('language', 'python')
-        
+        prompt = params.get("prompt")
+        language = params.get("language", "python")
+
         # In real implementation, would call Claude API
         return {
-            'code': f'# Generated {language} code\nprint("Hello from Claude")',
-            'language': language,
-            'confidence': 0.92
+            "code": f'# Generated {language} code\nprint("Hello from Claude")',
+            "language": language,
+            "confidence": 0.92,
         }
-    
+
     async def _analyze_intent(self, params: Dict) -> Dict:
         """Analyze user intent"""
-        text = params.get('text')
-        
+        text = params.get("text")
+
         # In real implementation, would use Claude for analysis
         return {
-            'intent': 'create_function',
-            'entities': ['user_management', 'authentication'],
-            'confidence': 0.88
+            "intent": "create_function",
+            "entities": ["user_management", "authentication"],
+            "confidence": 0.88,
         }
-    
+
     async def _reason_about(self, params: Dict) -> Dict:
         """Use Claude's reasoning capabilities"""
-        problem = params.get('problem')
-        
+        problem = params.get("problem")
+
         # In real implementation, would use Claude
         return {
-            'reasoning': 'Based on the problem...',
-            'solution': 'Proposed solution...',
-            'alternatives': ['Alternative 1', 'Alternative 2']
+            "reasoning": "Based on the problem...",
+            "solution": "Proposed solution...",
+            "alternatives": ["Alternative 1", "Alternative 2"],
         }
 
 
 class MCPConnectorRegistry:
     """Registry for all MCP connectors"""
-    
+
     def __init__(self):
         self.connectors = {}
         self.active_connections = {}
-        
+
     def register_connector(self, connector: MCPConnector):
         """Register a connector"""
         self.connectors[connector.connector_id] = connector
-        
+
     async def connect(self, connector_id: str, config: Dict) -> bool:
         """Connect a specific connector"""
         connector = self.connectors.get(connector_id)
         if not connector:
             return False
-            
+
         success = await connector.connect(config)
         if success:
             self.active_connections[connector_id] = connector
-            
+
         return success
-    
-    async def execute_cross_service_action(self, 
-                                         source_connector: str,
-                                         target_connector: str,
-                                         action: str,
-                                         params: Dict) -> Dict:
+
+    async def execute_cross_service_action(
+        self, source_connector: str, target_connector: str, action: str, params: Dict
+    ) -> Dict:
         """Execute action across services using MCP context sharing"""
-        
+
         # Get source context
         source = self.active_connections.get(source_connector)
         target = self.active_connections.get(target_connector)
-        
+
         if not source or not target:
-            return {'error': 'Connectors not connected'}
-            
+            return {"error": "Connectors not connected"}
+
         # Get context from source
         context = await source.get_context()
-        
+
         # Send context to target
         await target.send_context(context)
-        
+
         # Execute action on target with context
         result = await target.execute_action(action, params)
-        
+
         # Update source with results
         result_context = MCPContext()
-        result_context.task = {'last_action': action, 'result': result}
+        result_context.task = {"last_action": action, "result": result}
         await source.send_context(result_context)
-        
+
         return result
 
 
@@ -387,4 +385,4 @@ mcp_registry = MCPConnectorRegistry()
 
 # Register available connectors
 mcp_registry.register_connector(GitHubMCPConnector())
-mcp_registry.register_connector(ClaudeMCPConnector()) 
\ No newline at end of file
+mcp_registry.register_connector(ClaudeMCPConnector())
diff --git a/connectors/mcp_debug_tool.py b/connectors/mcp_debug_tool.py
index 784626e..1d1825f 100644
--- a/connectors/mcp_debug_tool.py
+++ b/connectors/mcp_debug_tool.py
@@ -15,9 +15,11 @@ from pathlib import Path
 
 from connectors.mcp_base import MCPConnector
 
+
 @dataclass
 class MCPDebugContext:
     """MCP-compatible debug context structure"""
+
     file: str
     line: int
     timestamp: str
@@ -25,13 +27,15 @@ class MCPDebugContext:
     variables: Optional[Dict[str, Any]] = None
     stack_trace: Optional[List[str]] = None
     quantum_state: Optional[Dict[str, Any]] = None
-    
+
     def to_dict(self) -> Dict[str, Any]:
         return asdict(self)
 
+
 @dataclass
 class DebugResponse:
     """Structured debug response following MCP schema"""
+
     status: str
     reasoning: str
     suggestions: List[str]
@@ -39,10 +43,11 @@ class DebugResponse:
     quantum_insights: Optional[Dict[str, Any]] = None
     performance_metrics: Optional[Dict[str, Any]] = None
 
+
 class MCPDebugTool(MCPConnector):
     """
     Advanced MCP Debug Tool with GCP integration and quantum agent support
-    
+
     Features:
     - Real-time code analysis and error detection
     - Quantum state debugging for quantum agents
@@ -50,204 +55,208 @@ class MCPDebugTool(MCPConnector):
     - Performance metrics and optimization insights
     - MCP-compliant context sharing
     """
-    
+
     def __init__(self, gcp_endpoint: str, auth_token: str = None):
         super().__init__("mcp_debug_tool", "debug_analysis")
         self.gcp_endpoint = gcp_endpoint
         self.auth_token = auth_token
         self.session: Optional[aiohttp.ClientSession] = None
         self.logger = logging.getLogger(__name__)
-        
+
         # Quantum debugging capabilities
         self.quantum_analyzers = {
-            'qubit_state': self._analyze_qubit_state,
-            'entanglement': self._analyze_entanglement,
-            'decoherence': self._analyze_decoherence,
-            'gate_fidelity': self._analyze_gate_fidelity
+            "qubit_state": self._analyze_qubit_state,
+            "entanglement": self._analyze_entanglement,
+            "decoherence": self._analyze_decoherence,
+            "gate_fidelity": self._analyze_gate_fidelity,
         }
-        
+
     async def __aenter__(self):
         """Async context manager entry"""
         self.session = aiohttp.ClientSession(
             timeout=aiohttp.ClientTimeout(total=30),
             headers={
-                'Authorization': f'Bearer {self.auth_token}' if self.auth_token else '',
-                'Content-Type': 'application/json',
-                'User-Agent': 'MCP-Debug-Tool/1.0.0'
-            }
+                "Authorization": f"Bearer {self.auth_token}" if self.auth_token else "",
+                "Content-Type": "application/json",
+                "User-Agent": "MCP-Debug-Tool/1.0.0",
+            },
         )
         return self
-        
+
     async def __aexit__(self, exc_type, exc_val, exc_tb):
         """Async context manager exit"""
         if self.session:
             await self.session.close()
-    
+
     # Implement required MCPConnector abstract methods
     async def connect(self, config: Dict[str, Any]) -> bool:
         """Establish connection to GCP debug service"""
-        self.gcp_endpoint = config.get('gcp_endpoint', self.gcp_endpoint)
-        self.auth_token = config.get('auth_token', self.auth_token)
+        self.gcp_endpoint = config.get("gcp_endpoint", self.gcp_endpoint)
+        self.auth_token = config.get("auth_token", self.auth_token)
         self.connected = True
         return True
-    
+
     async def disconnect(self) -> bool:
         """Disconnect from GCP debug service"""
         if self.session:
             await self.session.close()
         self.connected = False
         return True
-    
+
     async def get_context(self):
         """Get current debug context"""
         return self.context
-    
+
     async def send_context(self, context) -> bool:
         """Send context to debug service"""
         self.context = context
         return True
-    
-    async def execute_action(self, action: str, params: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def execute_action(
+        self, action: str, params: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Execute debug action"""
         if action == "debug_code":
             result = await self.debug_code(
-                code=params.get('code', ''),
-                error=params.get('error'),
-                mcp_data=params.get('mcp_data'),
-                quantum_context=params.get('quantum_context')
+                code=params.get("code", ""),
+                error=params.get("error"),
+                mcp_data=params.get("mcp_data"),
+                quantum_context=params.get("quantum_context"),
             )
             return result.__dict__
-        return {'error': f'Unknown action: {action}'}
-            
+        return {"error": f"Unknown action: {action}"}
+
     async def debug_code(
-        self, 
-        code: str, 
+        self,
+        code: str,
         error: str = None,
         mcp_data: Dict[str, Any] = None,
-        quantum_context: Dict[str, Any] = None
+        quantum_context: Dict[str, Any] = None,
     ) -> DebugResponse:
         """
         Main debug method following MCP tool schema
-        
+
         Args:
             code: Code snippet or file content to debug
             error: Error message or stack trace
             mcp_data: MCP context data (file, line, timestamp)
             quantum_context: Quantum-specific debugging context
-            
+
         Returns:
             DebugResponse with analysis, suggestions, and fixes
         """
         try:
             # Create MCP debug context
             debug_context = self._create_debug_context(mcp_data, error)
-            
+
             # Analyze code structure and patterns
             code_analysis = await self._analyze_code_structure(code)
-            
+
             # Perform quantum-specific analysis if applicable
             quantum_insights = None
             if quantum_context:
                 quantum_insights = await self._analyze_quantum_context(
                     code, quantum_context
                 )
-            
+
             # Get GCP-powered reasoning and suggestions
             gcp_response = await self._get_gcp_reasoning(
                 code, error, debug_context, quantum_insights
             )
-            
+
             # Generate fix suggestions
             fixes = await self._generate_fixes(
                 code, error, code_analysis, quantum_insights
             )
-            
+
             # Calculate performance metrics
             performance_metrics = await self._calculate_performance_metrics(
                 code, debug_context
             )
-            
+
             return DebugResponse(
                 status="success",
-                reasoning=gcp_response.get('reasoning', 'Analysis completed'),
-                suggestions=gcp_response.get('suggestions', []),
+                reasoning=gcp_response.get("reasoning", "Analysis completed"),
+                suggestions=gcp_response.get("suggestions", []),
                 fixes=fixes,
                 quantum_insights=quantum_insights,
-                performance_metrics=performance_metrics
+                performance_metrics=performance_metrics,
             )
-            
+
         except Exception as e:
             self.logger.error(f"Debug analysis failed: {str(e)}")
             return DebugResponse(
                 status="error",
                 reasoning=f"Debug analysis failed: {str(e)}",
-                suggestions=["Check debug tool configuration", "Verify GCP connectivity"],
-                fixes=[]
+                suggestions=[
+                    "Check debug tool configuration",
+                    "Verify GCP connectivity",
+                ],
+                fixes=[],
             )
-    
-    def _create_debug_context(self, mcp_data: Dict[str, Any], error: str) -> MCPDebugContext:
+
+    def _create_debug_context(
+        self, mcp_data: Dict[str, Any], error: str
+    ) -> MCPDebugContext:
         """Create standardized MCP debug context"""
         if not mcp_data:
             mcp_data = {}
-            
+
         return MCPDebugContext(
-            file=mcp_data.get('file', 'unknown'),
-            line=mcp_data.get('line', 0),
-            timestamp=mcp_data.get('timestamp', datetime.now(timezone.utc).isoformat()),
-            stack_trace=error.split('\n') if error else None
+            file=mcp_data.get("file", "unknown"),
+            line=mcp_data.get("line", 0),
+            timestamp=mcp_data.get("timestamp", datetime.now(timezone.utc).isoformat()),
+            stack_trace=error.split("\n") if error else None,
         )
-    
+
     async def _analyze_code_structure(self, code: str) -> Dict[str, Any]:
         """Analyze code structure for patterns and potential issues"""
         analysis = {
-            'complexity': self._calculate_complexity(code),
-            'patterns': self._detect_patterns(code),
-            'imports': self._extract_imports(code),
-            'functions': self._extract_functions(code),
-            'quantum_elements': self._detect_quantum_elements(code)
+            "complexity": self._calculate_complexity(code),
+            "patterns": self._detect_patterns(code),
+            "imports": self._extract_imports(code),
+            "functions": self._extract_functions(code),
+            "quantum_elements": self._detect_quantum_elements(code),
         }
         return analysis
-    
+
     async def _analyze_quantum_context(
-        self, 
-        code: str, 
-        quantum_context: Dict[str, Any]
+        self, code: str, quantum_context: Dict[str, Any]
     ) -> Dict[str, Any]:
         """Analyze quantum-specific debugging context"""
         insights = {}
-        
+
         for analyzer_name, analyzer_func in self.quantum_analyzers.items():
             try:
                 result = await analyzer_func(code, quantum_context)
                 insights[analyzer_name] = result
             except Exception as e:
-                insights[analyzer_name] = {'error': str(e)}
-        
+                insights[analyzer_name] = {"error": str(e)}
+
         return insights
-    
+
     async def _get_gcp_reasoning(
         self,
         code: str,
         error: str,
         debug_context: MCPDebugContext,
-        quantum_insights: Dict[str, Any] = None
+        quantum_insights: Dict[str, Any] = None,
     ) -> Dict[str, Any]:
         """Get GCP-powered reasoning and analysis"""
         if not self.session:
-            return {'reasoning': 'GCP session not available', 'suggestions': []}
-        
+            return {"reasoning": "GCP session not available", "suggestions": []}
+
         payload = {
-            'code': code,
-            'error': error,
-            'context': debug_context.to_dict(),
-            'quantum_insights': quantum_insights,
-            'analysis_type': 'comprehensive_debug'
+            "code": code,
+            "error": error,
+            "context": debug_context.to_dict(),
+            "quantum_insights": quantum_insights,
+            "analysis_type": "comprehensive_debug",
         }
-        
+
         try:
             async with self.session.post(
-                f"{self.gcp_endpoint}/v1/reason",
-                json=payload
+                f"{self.gcp_endpoint}/v1/reason", json=payload
             ) as response:
                 if response.status == 200:
                     return await response.json()
@@ -257,289 +266,316 @@ class MCPDebugTool(MCPConnector):
         except Exception as e:
             self.logger.error(f"GCP API call failed: {str(e)}")
             return await self._fallback_reasoning(code, error)
-    
+
     async def _fallback_reasoning(self, code: str, error: str) -> Dict[str, Any]:
         """Fallback reasoning when GCP is unavailable"""
         suggestions = []
-        
+
         if error:
-            if 'quantum' in error.lower():
-                suggestions.extend([
-                    "Check quantum circuit initialization",
-                    "Verify qubit count and gate operations",
-                    "Review quantum measurement procedures"
-                ])
-            if 'import' in error.lower():
+            if "quantum" in error.lower():
+                suggestions.extend(
+                    [
+                        "Check quantum circuit initialization",
+                        "Verify qubit count and gate operations",
+                        "Review quantum measurement procedures",
+                    ]
+                )
+            if "import" in error.lower():
                 suggestions.append("Check import statements and dependencies")
-            if 'syntax' in error.lower():
+            if "syntax" in error.lower():
                 suggestions.append("Review code syntax and indentation")
-        
+
         return {
-            'reasoning': 'Local analysis performed (GCP unavailable)',
-            'suggestions': suggestions or ['Review code logic and error patterns']
+            "reasoning": "Local analysis performed (GCP unavailable)",
+            "suggestions": suggestions or ["Review code logic and error patterns"],
         }
-    
+
     async def _generate_fixes(
         self,
         code: str,
         error: str,
         code_analysis: Dict[str, Any],
-        quantum_insights: Dict[str, Any] = None
+        quantum_insights: Dict[str, Any] = None,
     ) -> List[Dict[str, Any]]:
         """Generate specific fix suggestions"""
         fixes = []
-        
+
         # Quantum-specific fixes
-        if quantum_insights and 'quantum_elements' in code_analysis:
+        if quantum_insights and "quantum_elements" in code_analysis:
             quantum_fixes = await self._generate_quantum_fixes(
-                code, quantum_insights, code_analysis['quantum_elements']
+                code, quantum_insights, code_analysis["quantum_elements"]
             )
             fixes.extend(quantum_fixes)
-        
+
         # General code fixes
         if error:
             general_fixes = await self._generate_general_fixes(code, error)
             fixes.extend(general_fixes)
-        
+
         # Performance optimization fixes
-        if code_analysis.get('complexity', 0) > 10:
-            fixes.append({
-                'type': 'optimization',
-                'description': 'Reduce code complexity',
-                'suggestion': 'Break down complex functions into smaller ones',
-                'priority': 'medium'
-            })
-        
+        if code_analysis.get("complexity", 0) > 10:
+            fixes.append(
+                {
+                    "type": "optimization",
+                    "description": "Reduce code complexity",
+                    "suggestion": "Break down complex functions into smaller ones",
+                    "priority": "medium",
+                }
+            )
+
         return fixes
-    
+
     async def _generate_quantum_fixes(
-        self,
-        code: str,
-        quantum_insights: Dict[str, Any],
-        quantum_elements: List[str]
+        self, code: str, quantum_insights: Dict[str, Any], quantum_elements: List[str]
     ) -> List[Dict[str, Any]]:
         """Generate quantum-specific fix suggestions"""
         fixes = []
-        
+
         for insight_type, insight_data in quantum_insights.items():
-            if isinstance(insight_data, dict) and 'error' not in insight_data:
-                if insight_type == 'qubit_state' and insight_data.get('issues'):
-                    fixes.append({
-                        'type': 'quantum_state',
-                        'description': 'Qubit state management issue detected',
-                        'suggestion': 'Initialize qubits properly and check measurement timing',
-                        'priority': 'high',
-                        'quantum_specific': True
-                    })
-                
-                if insight_type == 'entanglement' and insight_data.get('warning'):
-                    fixes.append({
-                        'type': 'quantum_entanglement',
-                        'description': 'Entanglement pattern may cause decoherence',
-                        'suggestion': 'Review gate sequence and timing',
-                        'priority': 'medium',
-                        'quantum_specific': True
-                    })
-        
+            if isinstance(insight_data, dict) and "error" not in insight_data:
+                if insight_type == "qubit_state" and insight_data.get("issues"):
+                    fixes.append(
+                        {
+                            "type": "quantum_state",
+                            "description": "Qubit state management issue detected",
+                            "suggestion": "Initialize qubits properly and check measurement timing",
+                            "priority": "high",
+                            "quantum_specific": True,
+                        }
+                    )
+
+                if insight_type == "entanglement" and insight_data.get("warning"):
+                    fixes.append(
+                        {
+                            "type": "quantum_entanglement",
+                            "description": "Entanglement pattern may cause decoherence",
+                            "suggestion": "Review gate sequence and timing",
+                            "priority": "medium",
+                            "quantum_specific": True,
+                        }
+                    )
+
         return fixes
-    
-    async def _generate_general_fixes(self, code: str, error: str) -> List[Dict[str, Any]]:
+
+    async def _generate_general_fixes(
+        self, code: str, error: str
+    ) -> List[Dict[str, Any]]:
         """Generate general fix suggestions based on error patterns"""
         fixes = []
-        
+
         error_patterns = {
-            'NameError': {
-                'description': 'Variable or function not defined',
-                'suggestion': 'Check variable names and import statements',
-                'priority': 'high'
+            "NameError": {
+                "description": "Variable or function not defined",
+                "suggestion": "Check variable names and import statements",
+                "priority": "high",
+            },
+            "TypeError": {
+                "description": "Type mismatch in operation",
+                "suggestion": "Verify data types and conversion operations",
+                "priority": "high",
             },
-            'TypeError': {
-                'description': 'Type mismatch in operation',
-                'suggestion': 'Verify data types and conversion operations',
-                'priority': 'high'
+            "IndexError": {
+                "description": "List or array index out of range",
+                "suggestion": "Add bounds checking before accessing elements",
+                "priority": "medium",
             },
-            'IndexError': {
-                'description': 'List or array index out of range',
-                'suggestion': 'Add bounds checking before accessing elements',
-                'priority': 'medium'
-            }
         }
-        
+
         for pattern, fix_info in error_patterns.items():
             if pattern in error:
-                fixes.append({
-                    'type': 'syntax_error',
-                    **fix_info,
-                    'quantum_specific': False
-                })
-        
+                fixes.append(
+                    {"type": "syntax_error", **fix_info, "quantum_specific": False}
+                )
+
         return fixes
-    
+
     async def _calculate_performance_metrics(
-        self,
-        code: str,
-        debug_context: MCPDebugContext
+        self, code: str, debug_context: MCPDebugContext
     ) -> Dict[str, Any]:
         """Calculate performance metrics for the code"""
         return {
-            'complexity_score': self._calculate_complexity(code),
-            'line_count': len(code.split('\n')),
-            'estimated_runtime': 'low' if len(code.split('\n')) < 100 else 'medium',
-            'memory_usage': 'estimated_low',
-            'quantum_efficiency': self._estimate_quantum_efficiency(code)
+            "complexity_score": self._calculate_complexity(code),
+            "line_count": len(code.split("\n")),
+            "estimated_runtime": "low" if len(code.split("\n")) < 100 else "medium",
+            "memory_usage": "estimated_low",
+            "quantum_efficiency": self._estimate_quantum_efficiency(code),
         }
-    
+
     # Quantum analysis methods
-    async def _analyze_qubit_state(self, code: str, context: Dict[str, Any]) -> Dict[str, Any]:
+    async def _analyze_qubit_state(
+        self, code: str, context: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Analyze qubit state management in quantum code"""
         qubit_operations = []
         issues = []
-        
-        lines = code.split('\n')
+
+        lines = code.split("\n")
         for i, line in enumerate(lines):
-            if 'qubits' in line.lower() or 'qubit' in line.lower():
-                qubit_operations.append({'line': i+1, 'operation': line.strip()})
-            
-            if 'measure' in line.lower() and 'before' not in line.lower():
-                if i > 0 and 'gate' not in lines[i-1].lower():
+            if "qubits" in line.lower() or "qubit" in line.lower():
+                qubit_operations.append({"line": i + 1, "operation": line.strip()})
+
+            if "measure" in line.lower() and "before" not in line.lower():
+                if i > 0 and "gate" not in lines[i - 1].lower():
                     issues.append(f"Potential premature measurement at line {i+1}")
-        
+
         return {
-            'operations': qubit_operations,
-            'issues': issues,
-            'state_quality': 'good' if len(issues) == 0 else 'needs_review'
+            "operations": qubit_operations,
+            "issues": issues,
+            "state_quality": "good" if len(issues) == 0 else "needs_review",
         }
-    
-    async def _analyze_entanglement(self, code: str, context: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def _analyze_entanglement(
+        self, code: str, context: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Analyze quantum entanglement patterns"""
-        entanglement_gates = ['cnot', 'cx', 'cz', 'bell']
+        entanglement_gates = ["cnot", "cx", "cz", "bell"]
         entanglement_ops = []
-        
-        for line_num, line in enumerate(code.split('\n'), 1):
+
+        for line_num, line in enumerate(code.split("\n"), 1):
             for gate in entanglement_gates:
                 if gate in line.lower():
-                    entanglement_ops.append({
-                        'line': line_num,
-                        'gate': gate,
-                        'operation': line.strip()
-                    })
-        
+                    entanglement_ops.append(
+                        {"line": line_num, "gate": gate, "operation": line.strip()}
+                    )
+
         return {
-            'entanglement_operations': entanglement_ops,
-            'count': len(entanglement_ops),
-            'warning': 'High entanglement density' if len(entanglement_ops) > 5 else None
+            "entanglement_operations": entanglement_ops,
+            "count": len(entanglement_ops),
+            "warning": (
+                "High entanglement density" if len(entanglement_ops) > 5 else None
+            ),
         }
-    
-    async def _analyze_decoherence(self, code: str, context: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def _analyze_decoherence(
+        self, code: str, context: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Analyze potential decoherence issues"""
         decoherence_risks = []
-        
-        if 'sleep' in code or 'wait' in code:
-            decoherence_risks.append('Timing delays detected - may cause decoherence')
-        
-        if code.count('\n') > 50:  # Long quantum programs
-            decoherence_risks.append('Long quantum program - consider circuit optimization')
-        
+
+        if "sleep" in code or "wait" in code:
+            decoherence_risks.append("Timing delays detected - may cause decoherence")
+
+        if code.count("\n") > 50:  # Long quantum programs
+            decoherence_risks.append(
+                "Long quantum program - consider circuit optimization"
+            )
+
         return {
-            'risks': decoherence_risks,
-            'severity': 'high' if len(decoherence_risks) > 1 else 'low'
+            "risks": decoherence_risks,
+            "severity": "high" if len(decoherence_risks) > 1 else "low",
         }
-    
-    async def _analyze_gate_fidelity(self, code: str, context: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def _analyze_gate_fidelity(
+        self, code: str, context: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Analyze quantum gate fidelity patterns"""
         gate_count = 0
         gate_types = set()
-        
-        common_gates = ['h', 'x', 'y', 'z', 'rx', 'ry', 'rz', 'cnot', 'cx']
-        
-        for line in code.split('\n'):
+
+        common_gates = ["h", "x", "y", "z", "rx", "ry", "rz", "cnot", "cx"]
+
+        for line in code.split("\n"):
             for gate in common_gates:
                 if gate in line.lower():
                     gate_count += 1
                     gate_types.add(gate)
-        
+
         return {
-            'total_gates': gate_count,
-            'gate_types': list(gate_types),
-            'estimated_fidelity': 'high' if gate_count < 20 else 'medium'
+            "total_gates": gate_count,
+            "gate_types": list(gate_types),
+            "estimated_fidelity": "high" if gate_count < 20 else "medium",
         }
-    
+
     # Helper methods
     def _calculate_complexity(self, code: str) -> int:
         """Calculate cyclomatic complexity approximation"""
-        complexity_keywords = ['if', 'elif', 'else', 'for', 'while', 'try', 'except']
+        complexity_keywords = ["if", "elif", "else", "for", "while", "try", "except"]
         complexity = 1  # Base complexity
-        
-        for line in code.split('\n'):
+
+        for line in code.split("\n"):
             for keyword in complexity_keywords:
                 if keyword in line.strip():
                     complexity += 1
-        
+
         return complexity
-    
+
     def _detect_patterns(self, code: str) -> List[str]:
         """Detect common code patterns"""
         patterns = []
-        
-        if 'quantum' in code.lower():
-            patterns.append('quantum_computing')
-        if 'async' in code or 'await' in code:
-            patterns.append('asynchronous')
-        if 'class' in code:
-            patterns.append('object_oriented')
-        if 'def' in code:
-            patterns.append('functional')
-        
+
+        if "quantum" in code.lower():
+            patterns.append("quantum_computing")
+        if "async" in code or "await" in code:
+            patterns.append("asynchronous")
+        if "class" in code:
+            patterns.append("object_oriented")
+        if "def" in code:
+            patterns.append("functional")
+
         return patterns
-    
+
     def _extract_imports(self, code: str) -> List[str]:
         """Extract import statements"""
         imports = []
-        for line in code.split('\n'):
+        for line in code.split("\n"):
             stripped = line.strip()
-            if stripped.startswith('import ') or stripped.startswith('from '):
+            if stripped.startswith("import ") or stripped.startswith("from "):
                 imports.append(stripped)
         return imports
-    
+
     def _extract_functions(self, code: str) -> List[str]:
         """Extract function definitions"""
         functions = []
-        for line in code.split('\n'):
+        for line in code.split("\n"):
             stripped = line.strip()
-            if stripped.startswith('def ') or stripped.startswith('async def '):
+            if stripped.startswith("def ") or stripped.startswith("async def "):
                 functions.append(stripped)
         return functions
-    
+
     def _detect_quantum_elements(self, code: str) -> List[str]:
         """Detect quantum computing elements in code"""
         quantum_keywords = [
-            'qubit', 'quantum', 'circuit', 'gate', 'measurement',
-            'superposition', 'entanglement', 'qiskit', 'cirq', 'dwave'
+            "qubit",
+            "quantum",
+            "circuit",
+            "gate",
+            "measurement",
+            "superposition",
+            "entanglement",
+            "qiskit",
+            "cirq",
+            "dwave",
         ]
-        
+
         detected = []
         code_lower = code.lower()
-        
+
         for keyword in quantum_keywords:
             if keyword in code_lower:
                 detected.append(keyword)
-        
+
         return detected
-    
+
     def _estimate_quantum_efficiency(self, code: str) -> str:
         """Estimate quantum algorithm efficiency"""
         quantum_elements = self._detect_quantum_elements(code)
-        gate_density = len([line for line in code.split('\n') 
-                           if any(gate in line.lower() for gate in ['h', 'x', 'y', 'z', 'cnot'])])
-        
+        gate_density = len(
+            [
+                line
+                for line in code.split("\n")
+                if any(gate in line.lower() for gate in ["h", "x", "y", "z", "cnot"])
+            ]
+        )
+
         if not quantum_elements:
-            return 'n/a'
+            return "n/a"
         elif gate_density < 10:
-            return 'high'
+            return "high"
         elif gate_density < 30:
-            return 'medium'
+            return "medium"
         else:
-            return 'needs_optimization'
+            return "needs_optimization"
+
 
 # MCP Tool Registration Schema
 MCP_DEBUG_TOOL_SCHEMA = {
@@ -551,33 +587,33 @@ MCP_DEBUG_TOOL_SCHEMA = {
             "schema": {
                 "code": {
                     "type": "string",
-                    "description": "The code snippet or file content to debug, formatted as a string."
+                    "description": "The code snippet or file content to debug, formatted as a string.",
                 },
                 "context": {
                     "type": "object",
                     "properties": {
                         "error": {
                             "type": "string",
-                            "description": "The error message or stack trace encountered, if any."
+                            "description": "The error message or stack trace encountered, if any.",
                         },
                         "mcp_data": {
                             "type": "object",
                             "properties": {
                                 "file": {
                                     "type": "string",
-                                    "description": "The filepath of the code being debugged (e.g., app/models.py)."
+                                    "description": "The filepath of the code being debugged (e.g., app/models.py).",
                                 },
                                 "line": {
                                     "type": "integer",
-                                    "description": "The line number where the error or debug point occurs."
+                                    "description": "The line number where the error or debug point occurs.",
                                 },
                                 "timestamp": {
                                     "type": "string",
                                     "format": "date-time",
-                                    "description": "ISO 8601 timestamp of when the debug event occurred."
-                                }
+                                    "description": "ISO 8601 timestamp of when the debug event occurred.",
+                                },
                             },
-                            "required": ["file", "line"]
+                            "required": ["file", "line"],
                         },
                         "quantum_context": {
                             "type": "object",
@@ -586,38 +622,38 @@ MCP_DEBUG_TOOL_SCHEMA = {
                                 "circuit_depth": {"type": "integer"},
                                 "qubit_count": {"type": "integer"},
                                 "gate_sequence": {"type": "array"},
-                                "measurement_results": {"type": "object"}
-                            }
-                        }
+                                "measurement_results": {"type": "object"},
+                            },
+                        },
                     },
-                    "required": ["error", "mcp_data"]
-                }
+                    "required": ["error", "mcp_data"],
+                },
             },
             "description": "A debugging tool integrated with GCP to analyze code issues, provide reasoning, and suggest fixes, leveraging MCP for context sharing. Supports quantum agent applications.",
             "version": "1.0.0",
             "authentication": {
                 "type": "oauth2",
                 "token_url": "https://your-gcp-api/oauth2/token",
-                "scopes": ["https://www.googleapis.com/auth/cloud-platform"]
+                "scopes": ["https://www.googleapis.com/auth/cloud-platform"],
             },
             "timeout": 30000,
             "retry_policy": {
                 "max_retries": 3,
                 "backoff": "exponential",
-                "initial_delay_ms": 1000
-            }
+                "initial_delay_ms": 1000,
+            },
         }
     ]
 }
 
+
 # Usage Example
 async def example_usage():
     """Example usage of the MCP Debug Tool"""
     async with MCPDebugTool(
-        gcp_endpoint="https://your-gcp-api",
-        auth_token="your-oauth-token"
+        gcp_endpoint="https://your-gcp-api", auth_token="your-oauth-token"
     ) as debug_tool:
-        
+
         # Debug quantum code
         quantum_code = """
         import qiskit
@@ -634,25 +670,26 @@ async def example_usage():
             qc.measure([0, 1], [0, 1])
             return qc
         """
-        
+
         result = await debug_tool.debug_code(
             code=quantum_code,
             error="Quantum circuit execution failed",
             mcp_data={
                 "file": "quantum_teleportation.py",
                 "line": 12,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             },
             quantum_context={
                 "circuit_depth": 4,
                 "qubit_count": 3,
-                "gate_sequence": ["h", "cx", "cx", "h", "measure"]
-            }
+                "gate_sequence": ["h", "cx", "cx", "h", "measure"],
+            },
         )
-        
+
         print(f"Debug Status: {result.status}")
         print(f"Reasoning: {result.reasoning}")
         print(f"Quantum Insights: {result.quantum_insights}")
 
+
 if __name__ == "__main__":
-    asyncio.run(example_usage())
\ No newline at end of file
+    asyncio.run(example_usage())
diff --git a/connectors/xai_connector.py b/connectors/xai_connector.py
index 691b3c0..37b1e9b 100644
--- a/connectors/xai_connector.py
+++ b/connectors/xai_connector.py
@@ -20,25 +20,25 @@ logger = logging.getLogger(__name__)
 class XAIConnector:
     """
     Production-ready connector for xAI's Grok models with MCP integration.
-    
+
     Features:
     - Grok-3 model access
     - Live search capabilities
     - Streaming support
     - Material Design 3 compliant responses
     """
-    
+
     def __init__(self, api_key: Optional[str] = None):
         self.api_key = api_key or os.getenv("XAI_API_KEY")
         if not self.api_key:
             raise ValueError("XAI API key required")
-        
+
         self.base_url = "https://api.x.ai/v1"
         self.headers = {
             "Authorization": f"Bearer {self.api_key}",
-            "Content-Type": "application/json"
+            "Content-Type": "application/json",
         }
-        
+
         # Material Design 3 color tokens for responses
         self.md3_colors = {
             "primary": "#6750A4",
@@ -48,25 +48,27 @@ class XAIConnector:
             "error": "#BA1A1A",
             "error_container": "#FFDAD6",
             "success": "#006E1C",
-            "success_container": "#C6F181"
+            "success_container": "#C6F181",
         }
-    
-    async def chat_completion(self, 
-                            messages: List[Dict[str, str]], 
-                            model: str = "grok-3-latest",
-                            temperature: float = 0.7,
-                            stream: bool = False,
-                            enable_search: bool = False) -> Dict[str, Any]:
+
+    async def chat_completion(
+        self,
+        messages: List[Dict[str, str]],
+        model: str = "grok-3-latest",
+        temperature: float = 0.7,
+        stream: bool = False,
+        enable_search: bool = False,
+    ) -> Dict[str, Any]:
         """
         Send chat completion request to xAI.
-        
+
         Args:
             messages: List of message dictionaries
             model: Model to use (default: grok-3-latest)
             temperature: Sampling temperature
             stream: Whether to stream responses
             enable_search: Enable live search capabilities
-            
+
         Returns:
             API response or formatted result
         """
@@ -74,52 +76,51 @@ class XAIConnector:
             "messages": messages,
             "model": model,
             "temperature": temperature,
-            "stream": stream
+            "stream": stream,
         }
-        
+
         # Add search parameters if enabled
         if enable_search:
-            payload["search"] = {
-                "enabled": True,
-                "max_results": 5
-            }
-        
+            payload["search"] = {"enabled": True, "max_results": 5}
+
         async with httpx.AsyncClient() as client:
             try:
                 response = await client.post(
                     f"{self.base_url}/chat/completions",
                     headers=self.headers,
                     json=payload,
-                    timeout=30.0
+                    timeout=30.0,
                 )
                 response.raise_for_status()
-                
+
                 result = response.json()
-                
+
                 # Format response with Material Design 3 structure
                 return self._format_md3_response(result)
-                
+
             except httpx.HTTPStatusError as e:
-                logger.error(f"xAI API error: {e.response.status_code} - {e.response.text}")
+                logger.error(
+                    f"xAI API error: {e.response.status_code} - {e.response.text}"
+                )
                 return {
                     "success": False,
                     "error": str(e),
-                    "color": self.md3_colors["error"]
+                    "color": self.md3_colors["error"],
                 }
             except Exception as e:
                 logger.error(f"Unexpected error: {e}")
                 return {
                     "success": False,
                     "error": str(e),
-                    "color": self.md3_colors["error"]
+                    "color": self.md3_colors["error"],
                 }
-    
+
     def _format_md3_response(self, api_response: Dict[str, Any]) -> Dict[str, Any]:
         """
         Format API response following Material Design 3 principles.
         """
         content = api_response["choices"][0]["message"]["content"]
-        
+
         return {
             "success": True,
             "content": content,
@@ -127,22 +128,19 @@ class XAIConnector:
                 "model": api_response["model"],
                 "usage": api_response["usage"],
                 "sources_used": api_response["usage"].get("num_sources_used", 0),
-                "timestamp": datetime.now().isoformat()
+                "timestamp": datetime.now().isoformat(),
             },
             "ui": {
                 "color_scheme": self.md3_colors,
                 "typography": {
                     "headline": "Roboto",
                     "body": "Roboto",
-                    "code": "Roboto Mono"
+                    "code": "Roboto Mono",
                 },
-                "elevation": {
-                    "card": 1,
-                    "modal": 3
-                }
-            }
+                "elevation": {"card": 1, "modal": 3},
+            },
         }
-    
+
     async def search_and_summarize(self, query: str) -> Dict[str, Any]:
         """
         Use Grok's live search to find and summarize current information.
@@ -150,40 +148,37 @@ class XAIConnector:
         messages = [
             {
                 "role": "system",
-                "content": "You are a helpful assistant with access to current information. Search for and summarize the requested information."
+                "content": "You are a helpful assistant with access to current information. Search for and summarize the requested information.",
             },
-            {
-                "role": "user",
-                "content": query
-            }
+            {"role": "user", "content": query},
         ]
-        
+
         return await self.chat_completion(
             messages=messages,
             enable_search=True,
-            temperature=0.3  # Lower temperature for factual search
+            temperature=0.3,  # Lower temperature for factual search
         )
-    
-    async def code_generation(self, 
-                            task: str, 
-                            language: str = "python",
-                            follow_md3: bool = True) -> Dict[str, Any]:
+
+    async def code_generation(
+        self, task: str, language: str = "python", follow_md3: bool = True
+    ) -> Dict[str, Any]:
         """
         Generate code using Grok with optional Material Design 3 compliance.
         """
         system_prompt = f"You are an expert {language} developer."
-        
+
         if follow_md3 and language in ["javascript", "typescript", "dart"]:
-            system_prompt += " Follow Material Design 3 guidelines for any UI components."
-        
+            system_prompt += (
+                " Follow Material Design 3 guidelines for any UI components."
+            )
+
         messages = [
             {"role": "system", "content": system_prompt},
-            {"role": "user", "content": task}
+            {"role": "user", "content": task},
         ]
-        
+
         return await self.chat_completion(
-            messages=messages,
-            temperature=0.2  # Lower for code generation
+            messages=messages, temperature=0.2  # Lower for code generation
         )
 
 
@@ -191,16 +186,16 @@ class XAIMCPTool:
     """
     MCP Tool wrapper for xAI functionality.
     """
-    
+
     def __init__(self):
         self.connector = XAIConnector()
         self.name = "xai_grok"
         self.description = "Access xAI's Grok models with live search"
-    
+
     async def execute(self, params: Dict[str, Any]) -> Dict[str, Any]:
         """
         Execute xAI tool based on parameters.
-        
+
         Expected params:
         - action: "chat", "search", "code"
         - query: The user query
@@ -209,47 +204,46 @@ class XAIMCPTool:
         action = params.get("action", "chat")
         query = params.get("query", "")
         options = params.get("options", {})
-        
+
         if action == "search":
             return await self.connector.search_and_summarize(query)
         elif action == "code":
             return await self.connector.code_generation(
                 task=query,
                 language=options.get("language", "python"),
-                follow_md3=options.get("follow_md3", True)
+                follow_md3=options.get("follow_md3", True),
             )
         else:  # Default chat
             messages = [{"role": "user", "content": query}]
-            return await self.connector.chat_completion(
-                messages=messages,
-                **options
-            )
+            return await self.connector.chat_completion(messages=messages, **options)
 
 
 # Example usage
 async def demo_xai_integration():
     """Demonstrate xAI integration with Material Design 3."""
-    
+
     connector = XAIConnector()
-    
+
     # Test live search
     print("Testing live search...")
     search_result = await connector.search_and_summarize(
         "What are the latest updates in MCP (Model Context Protocol)?"
     )
     print(f"Search result: {json.dumps(search_result, indent=2)}")
-    
+
     # Test code generation with MD3
     print("\nTesting MD3-compliant code generation...")
     code_result = await connector.code_generation(
         task="Create a Material Design 3 card component in React",
         language="javascript",
-        follow_md3=True
+        follow_md3=True,
     )
     print(f"Code result: {json.dumps(code_result, indent=2)}")
 
 
 if __name__ == "__main__":
     # Set API key for testing
-    os.environ["XAI_API_KEY"] = "xai-BEQ4Au7tyMj3NDROcMShfCH8dd1o90upMbBZ8aOjaDeMnjkfESQzznlKDnAQf0anrTLsKrYTdTAQYvLp"
-    asyncio.run(demo_xai_integration()) 
\ No newline at end of file
+    os.environ["XAI_API_KEY"] = (
+        "xai-BEQ4Au7tyMj3NDROcMShfCH8dd1o90upMbBZ8aOjaDeMnjkfESQzznlKDnAQf0anrTLsKrYTdTAQYvLp"
+    )
+    asyncio.run(demo_xai_integration())
diff --git a/deploy_mcp_llm_integration.py b/deploy_mcp_llm_integration.py
index d843148..6f6226e 100644
--- a/deploy_mcp_llm_integration.py
+++ b/deploy_mcp_llm_integration.py
@@ -16,8 +16,7 @@ from datetime import datetime
 
 # Production logging
 logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger(__name__)
 
@@ -25,187 +24,185 @@ logger = logging.getLogger(__name__)
 class MCPLLMIntegration:
     """
     Production integration of MCP with LLM processes.
-    
+
     Key differentiators:
     1. Uses mcp-use for standard MCP protocol (commodity)
     2. Adds State Continuity Fabric (our innovation)
     3. Implements Edge-Cloud Continuum (our innovation)
     4. Provides cross-device/app context (our innovation)
     """
-    
+
     def __init__(self):
         self.mcp_client = None
         self.state_fabric = None
         self.execution_metrics = {
-            'total_calls': 0,
-            'successful_calls': 0,
-            'failed_calls': 0,
-            'avg_latency': 0
+            "total_calls": 0,
+            "successful_calls": 0,
+            "failed_calls": 0,
+            "avg_latency": 0,
         }
-    
+
     async def initialize(self) -> bool:
         """Initialize MCP and State Continuity Fabric"""
         try:
             # Import mcp-use
             from mcp_use import MCPClient
-            
+
             # Connect to MCP server
             self.mcp_client = MCPClient()
             # Note: mcp-use requires different initialization
             # We'll use their session-based approach
-            
+
             # Verify connection
             tools = await self.mcp_client.list_tools()
             logger.info(f"Connected to MCP with {len(tools)} tools available")
-            
+
             # Initialize State Fabric (our unique value)
             try:
                 from fabric.integrated_mcp_fabric import MCPStateFabric
+
                 self.state_fabric = MCPStateFabric("llm_fabric")
-                await self.state_fabric.initialize([
-                    {'name': 'local', 'url': 'http://localhost:8080'}
-                ])
+                await self.state_fabric.initialize(
+                    [{"name": "local", "url": "http://localhost:8080"}]
+                )
                 logger.info("State Continuity Fabric initialized")
             except Exception as e:
                 logger.warning(f"State Fabric not available: {e}")
                 # Continue without state fabric - graceful degradation
-            
+
             return True
-            
+
         except Exception as e:
             logger.error(f"Initialization failed: {e}")
             return False
-    
-    async def process_llm_request(self, 
-                                request: Dict[str, Any],
-                                context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+
+    async def process_llm_request(
+        self, request: Dict[str, Any], context: Optional[Dict[str, Any]] = None
+    ) -> Dict[str, Any]:
         """
         Process LLM request with MCP tools and state continuity.
-        
+
         This is where we add value:
         - Use MCP tools to enhance LLM capabilities
         - Track state across requests for continuity
         - Enable cross-device context awareness
         """
         start_time = asyncio.get_event_loop().time()
-        
+
         try:
             # Extract intent from LLM request
-            intent = request.get('intent', 'unknown')
-            parameters = request.get('parameters', {})
-            
+            intent = request.get("intent", "unknown")
+            parameters = request.get("parameters", {})
+
             # Determine which MCP tool to use
             tool_mapping = {
-                'analyze_code': 'code_analyzer',
-                'validate_protocol': 'protocol_validator',
-                'correct_errors': 'self_corrector'
+                "analyze_code": "code_analyzer",
+                "validate_protocol": "protocol_validator",
+                "correct_errors": "self_corrector",
             }
-            
+
             tool_name = tool_mapping.get(intent)
             if not tool_name:
-                return {
-                    'success': False,
-                    'error': f'Unknown intent: {intent}'
-                }
-            
+                return {"success": False, "error": f"Unknown intent: {intent}"}
+
             # Execute via MCP with state tracking
             if self.state_fabric:
                 # Use our enhanced execution with state continuity
                 result = await self.state_fabric.execute_with_context(
-                    server_name='local',
+                    server_name="local",
                     tool_name=tool_name,
                     params=parameters,
-                    context=context or {
-                        'device_id': 'llm_server',
-                        'app_id': 'llm_integration',
-                        'session_id': request.get('session_id', 'default')
-                    }
+                    context=context
+                    or {
+                        "device_id": "llm_server",
+                        "app_id": "llm_integration",
+                        "session_id": request.get("session_id", "default"),
+                    },
                 )
             else:
                 # Fallback to basic MCP execution
                 result = await self.mcp_client.call_tool(tool_name, parameters)
-                result = {
-                    'success': True,
-                    'result': result,
-                    'tool': tool_name
-                }
-            
+                result = {"success": True, "result": result, "tool": tool_name}
+
             # Update metrics
             latency = asyncio.get_event_loop().time() - start_time
             self._update_metrics(success=True, latency=latency)
-            
+
             # Add execution metadata
-            result['metadata'] = {
-                'latency_ms': latency * 1000,
-                'timestamp': datetime.utcnow().isoformat(),
-                'has_state_continuity': self.state_fabric is not None
+            result["metadata"] = {
+                "latency_ms": latency * 1000,
+                "timestamp": datetime.utcnow().isoformat(),
+                "has_state_continuity": self.state_fabric is not None,
             }
-            
+
             return result
-            
+
         except Exception as e:
             logger.error(f"Request processing failed: {e}")
             self._update_metrics(success=False, latency=0)
             return {
-                'success': False,
-                'error': str(e),
-                'tool': tool_name if 'tool_name' in locals() else 'unknown'
+                "success": False,
+                "error": str(e),
+                "tool": tool_name if "tool_name" in locals() else "unknown",
             }
-    
+
     def _update_metrics(self, success: bool, latency: float):
         """Update execution metrics"""
-        self.execution_metrics['total_calls'] += 1
+        self.execution_metrics["total_calls"] += 1
         if success:
-            self.execution_metrics['successful_calls'] += 1
+            self.execution_metrics["successful_calls"] += 1
             # Update rolling average latency
-            current_avg = self.execution_metrics['avg_latency']
-            total_calls = self.execution_metrics['total_calls']
-            self.execution_metrics['avg_latency'] = (
-                (current_avg * (total_calls - 1) + latency) / total_calls
-            )
+            current_avg = self.execution_metrics["avg_latency"]
+            total_calls = self.execution_metrics["total_calls"]
+            self.execution_metrics["avg_latency"] = (
+                current_avg * (total_calls - 1) + latency
+            ) / total_calls
         else:
-            self.execution_metrics['failed_calls'] += 1
-    
+            self.execution_metrics["failed_calls"] += 1
+
     async def get_cross_device_context(self, session_id: str) -> Dict[str, Any]:
         """
         Get context from other devices/sessions - our unique capability.
-        
+
         This demonstrates how we go beyond mcp-use by providing
         cross-device state continuity.
         """
         if not self.state_fabric:
-            return {'available': False, 'reason': 'State fabric not initialized'}
-        
+            return {"available": False, "reason": "State fabric not initialized"}
+
         try:
             # Get execution history across all devices
             history = await self.state_fabric.get_execution_history()
-            
+
             # Filter by session if provided
             if session_id:
-                history = [h for h in history if h.get('session_id') == session_id]
-            
+                history = [h for h in history if h.get("session_id") == session_id]
+
             return {
-                'available': True,
-                'history': history,
-                'device_count': len(self.state_fabric.state_fabric.device_registry)
-                if self.state_fabric.state_fabric else 0
+                "available": True,
+                "history": history,
+                "device_count": (
+                    len(self.state_fabric.state_fabric.device_registry)
+                    if self.state_fabric.state_fabric
+                    else 0
+                ),
             }
-            
+
         except Exception as e:
             logger.error(f"Failed to get cross-device context: {e}")
-            return {'available': False, 'error': str(e)}
-    
+            return {"available": False, "error": str(e)}
+
     def get_metrics(self) -> Dict[str, Any]:
         """Get execution metrics"""
         return {
             **self.execution_metrics,
-            'success_rate': (
-                self.execution_metrics['successful_calls'] / 
-                max(self.execution_metrics['total_calls'], 1)
+            "success_rate": (
+                self.execution_metrics["successful_calls"]
+                / max(self.execution_metrics["total_calls"], 1)
             ),
-            'avg_latency_ms': self.execution_metrics['avg_latency'] * 1000
+            "avg_latency_ms": self.execution_metrics["avg_latency"] * 1000,
         }
-    
+
     async def close(self):
         """Clean shutdown"""
         if self.mcp_client:
@@ -218,64 +215,58 @@ class MCPLLMIntegration:
 async def deploy_llm_integration():
     """
     Deploy MCP-LLM integration in production.
-    
+
     This shows:
     1. Real integration with mcp-use
     2. Our State Continuity value-add
     3. Production error handling
     4. Metrics and monitoring
     """
-    
+
     integration = MCPLLMIntegration()
-    
+
     # Initialize
     initialized = await integration.initialize()
     if not initialized:
         logger.error("Failed to initialize MCP-LLM integration")
         return
-    
+
     # Simulate LLM requests
     test_requests = [
         {
-            'intent': 'analyze_code',
-            'parameters': {
-                'code': 'def hello(): print("world")',
-                'language': 'python'
-            },
-            'session_id': 'session_123'
+            "intent": "analyze_code",
+            "parameters": {"code": 'def hello(): print("world")', "language": "python"},
+            "session_id": "session_123",
         },
         {
-            'intent': 'validate_protocol',
-            'parameters': {
-                'protocol': 'test_protocol',
-                'data': {'key': 'value'}
-            },
-            'session_id': 'session_123'
+            "intent": "validate_protocol",
+            "parameters": {"protocol": "test_protocol", "data": {"key": "value"}},
+            "session_id": "session_123",
         },
         {
-            'intent': 'correct_errors',
-            'parameters': {
-                'errors': ['undefined variable x'],
-                'context': 'python function'
+            "intent": "correct_errors",
+            "parameters": {
+                "errors": ["undefined variable x"],
+                "context": "python function",
             },
-            'session_id': 'session_456'
-        }
+            "session_id": "session_456",
+        },
     ]
-    
+
     # Process requests
     for request in test_requests:
         logger.info(f"Processing request: {request['intent']}")
         result = await integration.process_llm_request(request)
         logger.info(f"Result: {json.dumps(result, indent=2)}")
-    
+
     # Show cross-device context capability
-    context = await integration.get_cross_device_context('session_123')
+    context = await integration.get_cross_device_context("session_123")
     logger.info(f"Cross-device context: {json.dumps(context, indent=2)}")
-    
+
     # Show metrics
     metrics = integration.get_metrics()
     logger.info(f"Execution metrics: {json.dumps(metrics, indent=2)}")
-    
+
     # Clean shutdown
     await integration.close()
 
@@ -285,7 +276,7 @@ async def analyze_competitive_advantage():
     """
     Analyze how our approach differs from plain mcp-use.
     """
-    
+
     logger.info("=== Competitive Analysis ===")
     logger.info("mcp-use provides: Basic MCP protocol, tool discovery, execution")
     logger.info("We add:")
@@ -294,32 +285,31 @@ async def analyze_competitive_advantage():
     logger.info("3. Execution metrics - Production monitoring")
     logger.info("4. Privacy-aware state management - Filter sensitive data")
     logger.info("5. Vector clock synchronization - Resolve distributed conflicts")
-    
+
     # Show concrete example
     integration = MCPLLMIntegration()
     if await integration.initialize():
         # Execute same tool from different "devices"
-        for device in ['laptop', 'phone', 'tablet']:
+        for device in ["laptop", "phone", "tablet"]:
             await integration.process_llm_request(
-                {
-                    'intent': 'analyze_code',
-                    'parameters': {'code': f'# From {device}'}
-                },
-                {'device_id': device, 'app_id': 'demo'}
+                {"intent": "analyze_code", "parameters": {"code": f"# From {device}"}},
+                {"device_id": device, "app_id": "demo"},
             )
-        
+
         # Show we can see all executions
         context = await integration.get_cross_device_context(None)
-        logger.info(f"Tracked {len(context.get('history', []))} executions across devices")
-        
+        logger.info(
+            f"Tracked {len(context.get('history', []))} executions across devices"
+        )
+
         await integration.close()
 
 
 if __name__ == "__main__":
     logger.info("Starting MCP-LLM Integration Deployment")
-    
+
     # Run deployment
     asyncio.run(deploy_llm_integration())
-    
+
     # Run competitive analysis
-    asyncio.run(analyze_competitive_advantage()) 
\ No newline at end of file
+    asyncio.run(analyze_competitive_advantage())
diff --git a/deploy_production_mcp.py b/deploy_production_mcp.py
index 8e6adb2..a6acd3b 100644
--- a/deploy_production_mcp.py
+++ b/deploy_production_mcp.py
@@ -16,8 +16,7 @@ from datetime import datetime
 
 # Configure production logging
 logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger(__name__)
 
@@ -29,150 +28,153 @@ class ProductionMCPDeployment:
     - Our State Continuity innovations
     - Unified transport layer (MCP + A2A + Mojo)
     """
-    
+
     def __init__(self):
         self.mcp_agent = None
         self.state_fabric = None
         self.unified_transport = None
-        
+
     async def initialize(self):
         """Initialize all components with production error handling"""
         try:
             # Step 1: Initialize mcp-use properly
             from mcp_use import MCPAgent, MCPClient
             from langchain_openai import ChatOpenAI
-            
+
             # Create MCP client with proper configuration
             config = {
                 "mcpServers": {
                     "local": {
                         "command": "node",
                         "args": ["/app/mcp_server/main.js"],
-                        "env": {"NODE_ENV": "production"}
+                        "env": {"NODE_ENV": "production"},
                     },
-                    "github": {
-                        "command": "npx",
-                        "args": ["@github/mcp-server-github"]
-                    }
+                    "github": {"command": "npx", "args": ["@github/mcp-server-github"]},
                 }
             }
-            
+
             # Use MCPClient.from_dict as shown in mcp-use docs
             mcp_client = MCPClient.from_dict(config)
-            
+
             # Create LLM
             llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
-            
+
             # Create agent with proper parameters
             self.mcp_agent = MCPAgent(
                 llm=llm,
                 client=mcp_client,
                 max_steps=30,
                 use_server_manager=True,  # Enable intelligent server selection
-                verbose=True
+                verbose=True,
             )
-            
+
             logger.info("MCP Agent initialized successfully")
-            
+
             # Step 2: Initialize our State Continuity Fabric
             try:
                 from fabric.state_continuity_core import StateContinuityFabric
+
                 self.state_fabric = StateContinuityFabric("production")
                 await self.state_fabric.initialize()
                 logger.info("State Continuity Fabric initialized")
             except ImportError:
-                logger.warning("State Continuity Fabric not available - continuing without it")
-            
+                logger.warning(
+                    "State Continuity Fabric not available - continuing without it"
+                )
+
             # Step 3: Initialize Unified Transport Layer
             try:
                 from agents.unified_transport_layer import UnifiedTransportLayer
+
                 self.unified_transport = UnifiedTransportLayer()
                 await self.unified_transport.initialize()
                 logger.info("Unified Transport Layer initialized")
             except ImportError:
-                logger.warning("Unified Transport not available - using standard transport")
-            
+                logger.warning(
+                    "Unified Transport not available - using standard transport"
+                )
+
             return True
-            
+
         except Exception as e:
             logger.error(f"Initialization failed: {e}")
             return False
-    
-    async def execute_with_state_continuity(self, query: str, context: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def execute_with_state_continuity(
+        self, query: str, context: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """
         Execute query with our unique state continuity features.
         This is where we add value beyond mcp-use.
         """
         start_time = asyncio.get_event_loop().time()
-        
+
         try:
             # Capture pre-execution state
             if self.state_fabric:
                 pre_state = await self.state_fabric.capture_context(
-                    device_id=context.get('device_id', 'server'),
-                    app_id=context.get('app_id', 'production'),
+                    device_id=context.get("device_id", "server"),
+                    app_id=context.get("app_id", "production"),
                     context={
-                        'query': query,
-                        'timestamp': datetime.utcnow().isoformat(),
-                        'session_id': context.get('session_id')
-                    }
+                        "query": query,
+                        "timestamp": datetime.utcnow().isoformat(),
+                        "session_id": context.get("session_id"),
+                    },
                 )
                 logger.info(f"Captured pre-execution state: {pre_state.id}")
-            
+
             # Execute using mcp-use agent with streaming
             result_chunks = []
             async for chunk in self.mcp_agent.astream(query):
                 result_chunks.append(chunk)
                 # Process streaming chunks in real-time
-                if 'messages' in chunk:
+                if "messages" in chunk:
                     logger.info(f"Agent: {chunk['messages']}")
-            
+
             # Get final result
             final_result = result_chunks[-1] if result_chunks else None
-            
+
             # Capture post-execution state
             if self.state_fabric and final_result:
                 post_state = await self.state_fabric.capture_context(
-                    device_id=context.get('device_id', 'server'),
-                    app_id=context.get('app_id', 'production'),
+                    device_id=context.get("device_id", "server"),
+                    app_id=context.get("app_id", "production"),
                     context={
-                        'query': query,
-                        'result': final_result.get('output'),
-                        'success': True,
-                        'execution_time': asyncio.get_event_loop().time() - start_time
-                    }
+                        "query": query,
+                        "result": final_result.get("output"),
+                        "success": True,
+                        "execution_time": asyncio.get_event_loop().time() - start_time,
+                    },
                 )
                 logger.info(f"Captured post-execution state: {post_state.id}")
-            
+
             return {
-                'success': True,
-                'result': final_result.get('output') if final_result else None,
-                'execution_time_ms': (asyncio.get_event_loop().time() - start_time) * 1000,
-                'has_state_continuity': self.state_fabric is not None,
-                'transport_type': 'unified' if self.unified_transport else 'standard'
+                "success": True,
+                "result": final_result.get("output") if final_result else None,
+                "execution_time_ms": (asyncio.get_event_loop().time() - start_time)
+                * 1000,
+                "has_state_continuity": self.state_fabric is not None,
+                "transport_type": "unified" if self.unified_transport else "standard",
             }
-            
+
         except Exception as e:
             logger.error(f"Execution failed: {e}")
-            
+
             # Capture error state
             if self.state_fabric:
                 await self.state_fabric.capture_context(
-                    device_id=context.get('device_id', 'server'),
-                    app_id=context.get('app_id', 'production'),
-                    context={
-                        'query': query,
-                        'error': str(e),
-                        'success': False
-                    }
+                    device_id=context.get("device_id", "server"),
+                    app_id=context.get("app_id", "production"),
+                    context={"query": query, "error": str(e), "success": False},
                 )
-            
+
             return {
-                'success': False,
-                'error': str(e),
-                'execution_time_ms': (asyncio.get_event_loop().time() - start_time) * 1000
+                "success": False,
+                "error": str(e),
+                "execution_time_ms": (asyncio.get_event_loop().time() - start_time)
+                * 1000,
             }
-    
+
     async def demonstrate_unique_capabilities(self):
         """
         Show what we add beyond mcp-use:
@@ -181,105 +183,97 @@ class ProductionMCPDeployment:
         3. Privacy-aware processing
         """
         logger.info("=== Demonstrating Our Unique Capabilities ===")
-        
+
         # 1. Cross-device state synchronization
         if self.state_fabric:
             # Register multiple devices
-            devices = ['laptop', 'phone', 'tablet']
+            devices = ["laptop", "phone", "tablet"]
             for device in devices:
-                self.state_fabric.register_device(device, {'type': device})
-            
+                self.state_fabric.register_device(device, {"type": device})
+
             # Execute from different devices
             for device in devices:
                 await self.execute_with_state_continuity(
-                    f"Test from {device}",
-                    {'device_id': device, 'app_id': 'demo'}
+                    f"Test from {device}", {"device_id": device, "app_id": "demo"}
                 )
-            
+
             # Sync states between devices
-            await self.state_fabric.sync_devices('laptop', 'phone')
+            await self.state_fabric.sync_devices("laptop", "phone")
             logger.info("✓ Cross-device state synchronization working")
-        
+
         # 2. Unified transport performance
         if self.unified_transport:
             # Test different transport strategies
-            test_message = {'data': 'x' * 1000}  # 1KB message
-            
+            test_message = {"data": "x" * 1000}  # 1KB message
+
             # Zero-copy for same process
             result = await self.unified_transport.send(
-                test_message,
-                source='agent1',
-                target='agent2',
-                same_process=True
+                test_message, source="agent1", target="agent2", same_process=True
             )
             logger.info(f"✓ Zero-copy transport: {result['latency_us']}μs")
-            
+
             # Mojo pipes for cross-process
             result = await self.unified_transport.send(
-                test_message,
-                source='agent1',
-                target='agent3',
-                same_process=False
+                test_message, source="agent1", target="agent3", same_process=False
             )
             logger.info(f"✓ Mojo pipe transport: {result['latency_us']}μs")
-        
+
         # 3. Privacy-aware processing
         if self.state_fabric:
             # Set privacy rules
-            self.state_fabric.privacy_rules['no-sync'].append('api_key')
-            self.state_fabric.privacy_rules['encrypted'].append('user_data')
-            
+            self.state_fabric.privacy_rules["no-sync"].append("api_key")
+            self.state_fabric.privacy_rules["encrypted"].append("user_data")
+
             # Test privacy filtering
             sensitive_context = {
-                'api_key': 'secret123',
-                'user_data': 'personal info',
-                'public_data': 'can be shared'
+                "api_key": "secret123",
+                "user_data": "personal info",
+                "public_data": "can be shared",
             }
-            
+
             filtered = self.state_fabric._apply_privacy_filters(sensitive_context)
-            assert 'api_key' not in filtered
-            assert '<encrypted>' in filtered.get('user_data', '')
+            assert "api_key" not in filtered
+            assert "<encrypted>" in filtered.get("user_data", "")
             logger.info("✓ Privacy-aware filtering working")
-    
+
     async def close(self):
         """Clean shutdown of all components"""
-        if self.mcp_agent and hasattr(self.mcp_agent, 'client'):
+        if self.mcp_agent and hasattr(self.mcp_agent, "client"):
             await self.mcp_agent.client.close_all_sessions()
-        
+
         logger.info("All components shut down cleanly")
 
 
 # Production deployment example
 async def deploy_production():
     """Deploy production MCP with our innovations"""
-    
+
     deployment = ProductionMCPDeployment()
-    
+
     try:
         # Initialize
         success = await deployment.initialize()
         if not success:
             logger.error("Failed to initialize deployment")
             return
-        
+
         # Run production workload
         queries = [
             "Analyze the repository structure and suggest improvements",
             "Create a comprehensive test suite for the MCP integration",
-            "Generate documentation for the State Continuity Fabric"
+            "Generate documentation for the State Continuity Fabric",
         ]
-        
+
         for query in queries:
             logger.info(f"\nProcessing: {query}")
             result = await deployment.execute_with_state_continuity(
-                query,
-                {'device_id': 'production_server', 'session_id': 'prod_001'}
+                query, {"device_id": "production_server", "session_id": "prod_001"}
             )
             logger.info(f"Result: {result}")
-        
+
         # Demonstrate unique capabilities
         await deployment.demonstrate_unique_capabilities()
-        
+
     finally:
         await deployment.close()
 
@@ -296,7 +290,7 @@ async def implement_contribution_strategy():
     logger.info("4. Add tests for cross-device synchronization")
     logger.info("5. Submit PR with clear value proposition")
     logger.info("6. Engage with community for feedback")
-    
+
     # Example contribution code structure
     contribution_example = '''
     # In mcp_use/mixins/state_continuity.py
@@ -315,15 +309,15 @@ async def implement_contribution_strategy():
     class MCPAgent(BaseAgent, StateContinuityMixin):
         # Enhanced with our capabilities
     '''
-    
+
     logger.info(f"Contribution structure:\n{contribution_example}")
 
 
 if __name__ == "__main__":
     logger.info("Starting Production MCP Deployment")
-    
+
     # Run production deployment
     asyncio.run(deploy_production())
-    
+
     # Show contribution strategy
-    asyncio.run(implement_contribution_strategy()) 
\ No newline at end of file
+    asyncio.run(implement_contribution_strategy())
diff --git a/fabric/integrated_mcp_fabric.py b/fabric/integrated_mcp_fabric.py
index 83a38cf..57c0322 100644
--- a/fabric/integrated_mcp_fabric.py
+++ b/fabric/integrated_mcp_fabric.py
@@ -2,7 +2,7 @@
 Production-Grade MCP + State Continuity Fabric Integration
 ==========================================================
 
-This integrates mcp-use (for MCP protocol) with our unique 
+This integrates mcp-use (for MCP protocol) with our unique
 State Continuity Fabric for cross-device/app state management.
 """
 
@@ -14,8 +14,7 @@ from mcp_use.tools import ToolExecutor
 
 # Configure production logging
 logging.basicConfig(
-    level=logging.INFO,
-    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
 )
 logger = logging.getLogger(__name__)
 
@@ -25,74 +24,74 @@ class MCPStateFabric:
     Production-ready integration of MCP with State Continuity Fabric.
     Uses mcp-use for protocol, adds our unique state continuity layer.
     """
-    
+
     def __init__(self, fabric_id: str):
         self.fabric_id = fabric_id
         self.mcp_clients: Dict[str, MCPClient] = {}
         self.tool_executor = ToolExecutor()
         self._initialized = False
-        
+
         # Import our State Continuity Fabric
         try:
             from fabric.state_continuity_core import StateContinuityFabric
+
             self.state_fabric = StateContinuityFabric(fabric_id)
         except ImportError:
             logger.error("State Continuity Fabric not found. Creating minimal version.")
             self.state_fabric = None
-    
+
     async def initialize(self, mcp_servers: List[Dict[str, Any]]) -> bool:
         """
         Initialize with multiple MCP servers using mcp-use.
-        
+
         Args:
             mcp_servers: List of server configs with 'name' and 'url'
-            
+
         Returns:
             bool: True if at least one server connected successfully
         """
         connected_count = 0
-        
+
         for server_config in mcp_servers:
             try:
-                name = server_config['name']
-                url = server_config['url']
-                
+                name = server_config["name"]
+                url = server_config["url"]
+
                 # Use mcp-use to create client
                 client = await create_client(
-                    server_url=url,
-                    client_name=f"{self.fabric_id}_{name}"
+                    server_url=url, client_name=f"{self.fabric_id}_{name}"
                 )
-                
+
                 # Verify connection by listing tools
                 tools = await client.list_tools()
                 logger.info(f"Connected to {name} at {url} with {len(tools)} tools")
-                
+
                 self.mcp_clients[name] = client
                 connected_count += 1
-                
+
             except Exception as e:
                 logger.error(f"Failed to connect to {server_config}: {e}")
-        
+
         self._initialized = connected_count > 0
-        
+
         # Initialize state fabric if available
         if self.state_fabric and connected_count > 0:
             await self.state_fabric.initialize()
-        
+
         return self._initialized
-    
+
     async def discover_capabilities(self) -> Dict[str, List[str]]:
         """
         Discover all available tools across connected MCP servers.
-        
+
         Returns:
             Dict mapping server names to their tool lists
         """
         if not self._initialized:
             raise RuntimeError("Fabric not initialized. Call initialize() first.")
-        
+
         capabilities = {}
-        
+
         for server_name, client in self.mcp_clients.items():
             try:
                 tools = await client.list_tools()
@@ -101,17 +100,19 @@ class MCPStateFabric:
             except Exception as e:
                 logger.error(f"Failed to get capabilities from {server_name}: {e}")
                 capabilities[server_name] = []
-        
+
         return capabilities
-    
-    async def execute_with_context(self, 
-                                 server_name: str,
-                                 tool_name: str,
-                                 params: Dict[str, Any],
-                                 context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+
+    async def execute_with_context(
+        self,
+        server_name: str,
+        tool_name: str,
+        params: Dict[str, Any],
+        context: Optional[Dict[str, Any]] = None,
+    ) -> Dict[str, Any]:
         """
         Execute MCP tool with state continuity context.
-        
+
         This is where we add value beyond basic MCP:
         - Capture execution context
         - Maintain state continuity
@@ -119,116 +120,124 @@ class MCPStateFabric:
         """
         if server_name not in self.mcp_clients:
             raise ValueError(f"No client connected for server: {server_name}")
-        
+
         client = self.mcp_clients[server_name]
-        
+
         # Capture pre-execution state if fabric available
         if self.state_fabric and context:
             pre_state = await self.state_fabric.capture_context(
-                device_id=context.get('device_id', 'unknown'),
-                app_id=context.get('app_id', 'mcp_fabric'),
+                device_id=context.get("device_id", "unknown"),
+                app_id=context.get("app_id", "mcp_fabric"),
                 context={
-                    'tool': tool_name,
-                    'params': params,
-                    'timestamp': asyncio.get_event_loop().time()
-                }
+                    "tool": tool_name,
+                    "params": params,
+                    "timestamp": asyncio.get_event_loop().time(),
+                },
             )
             logger.info(f"Captured pre-execution state: {pre_state.id}")
-        
+
         # Execute tool using mcp-use
         try:
             result = await client.call_tool(tool_name, params)
-            
+
             # Capture post-execution state
             if self.state_fabric and context:
                 post_state = await self.state_fabric.capture_context(
-                    device_id=context.get('device_id', 'unknown'),
-                    app_id=context.get('app_id', 'mcp_fabric'),
+                    device_id=context.get("device_id", "unknown"),
+                    app_id=context.get("app_id", "mcp_fabric"),
                     context={
-                        'tool': tool_name,
-                        'result': result,
-                        'success': True,
-                        'timestamp': asyncio.get_event_loop().time()
-                    }
+                        "tool": tool_name,
+                        "result": result,
+                        "success": True,
+                        "timestamp": asyncio.get_event_loop().time(),
+                    },
                 )
                 logger.info(f"Captured post-execution state: {post_state.id}")
-            
+
             return {
-                'success': True,
-                'result': result,
-                'server': server_name,
-                'tool': tool_name
+                "success": True,
+                "result": result,
+                "server": server_name,
+                "tool": tool_name,
             }
-            
+
         except Exception as e:
             logger.error(f"Tool execution failed: {e}")
-            
+
             # Capture error state
             if self.state_fabric and context:
                 await self.state_fabric.capture_context(
-                    device_id=context.get('device_id', 'unknown'),
-                    app_id=context.get('app_id', 'mcp_fabric'),
-                    context={
-                        'tool': tool_name,
-                        'error': str(e),
-                        'success': False
-                    }
+                    device_id=context.get("device_id", "unknown"),
+                    app_id=context.get("app_id", "mcp_fabric"),
+                    context={"tool": tool_name, "error": str(e), "success": False},
                 )
-            
+
             return {
-                'success': False,
-                'error': str(e),
-                'server': server_name,
-                'tool': tool_name
+                "success": False,
+                "error": str(e),
+                "server": server_name,
+                "tool": tool_name,
             }
-    
-    async def get_execution_history(self, device_id: Optional[str] = None) -> List[Dict[str, Any]]:
+
+    async def get_execution_history(
+        self, device_id: Optional[str] = None
+    ) -> List[Dict[str, Any]]:
         """
         Get execution history with state continuity information.
-        
+
         This demonstrates our unique value: tracking execution across devices.
         """
         if not self.state_fabric:
             return []
-        
+
         history = []
-        
+
         # Get all devices or specific device
-        devices = [device_id] if device_id else list(self.state_fabric.device_registry.keys())
-        
+        devices = (
+            [device_id] if device_id else list(self.state_fabric.device_registry.keys())
+        )
+
         for dev_id in devices:
             if dev_id in self.state_fabric.engines:
                 engine = self.state_fabric.engines[dev_id]
                 for state_id, state in engine.states.items():
-                    if 'tool' in state.data:
-                        history.append({
-                            'device': dev_id,
-                            'timestamp': state.timestamp,
-                            'tool': state.data.get('tool'),
-                            'success': state.data.get('success', False),
-                            'state_id': state_id
-                        })
-        
+                    if "tool" in state.data:
+                        history.append(
+                            {
+                                "device": dev_id,
+                                "timestamp": state.timestamp,
+                                "tool": state.data.get("tool"),
+                                "success": state.data.get("success", False),
+                                "state_id": state_id,
+                            }
+                        )
+
         # Sort by timestamp
-        history.sort(key=lambda x: x['timestamp'], reverse=True)
+        history.sort(key=lambda x: x["timestamp"], reverse=True)
         return history
-    
-    async def sync_execution_state(self, source_device: str, target_device: str) -> bool:
+
+    async def sync_execution_state(
+        self, source_device: str, target_device: str
+    ) -> bool:
         """
         Sync execution state between devices - our unique capability.
         """
         if not self.state_fabric:
             logger.error("State fabric not available for sync")
             return False
-        
+
         try:
-            merged_state = await self.state_fabric.sync_devices(source_device, target_device)
-            logger.info(f"Synced state from {source_device} to {target_device}: {merged_state.id}")
+            merged_state = await self.state_fabric.sync_devices(
+                source_device, target_device
+            )
+            logger.info(
+                f"Synced state from {source_device} to {target_device}: {merged_state.id}"
+            )
             return True
         except Exception as e:
             logger.error(f"State sync failed: {e}")
             return False
-    
+
     async def close(self):
         """Clean shutdown of all connections"""
         for server_name, client in self.mcp_clients.items():
@@ -237,7 +246,7 @@ class MCPStateFabric:
                 logger.info(f"Closed connection to {server_name}")
             except Exception as e:
                 logger.error(f"Error closing {server_name}: {e}")
-        
+
         self.mcp_clients.clear()
         self._initialized = False
 
@@ -250,57 +259,51 @@ async def production_example():
     2. Adding our State Continuity value
     3. Real error handling and logging
     """
-    
+
     fabric = MCPStateFabric("production_fabric")
-    
+
     # Configure real MCP servers
     servers = [
-        {
-            'name': 'local',
-            'url': 'http://localhost:8080'
-        },
+        {"name": "local", "url": "http://localhost:8080"},
         # Add more servers as needed
         # {
         #     'name': 'github',
         #     'url': 'http://localhost:3000'  # GitHub MCP server
         # }
     ]
-    
+
     try:
         # Initialize with production error handling
         initialized = await fabric.initialize(servers)
         if not initialized:
             logger.error("No MCP servers available. Cannot proceed.")
             return
-        
+
         # Discover what we can actually do
         capabilities = await fabric.discover_capabilities()
         logger.info(f"Available capabilities: {capabilities}")
-        
+
         # Execute a tool with context tracking
-        if 'local' in capabilities and capabilities['local']:
+        if "local" in capabilities and capabilities["local"]:
             # Use first available tool for demo
-            tool_name = capabilities['local'][0]
-            
+            tool_name = capabilities["local"][0]
+
             result = await fabric.execute_with_context(
-                server_name='local',
+                server_name="local",
                 tool_name=tool_name,
                 params={},  # Tool-specific params
-                context={
-                    'device_id': 'macbook_pro',
-                    'app_id': 'production_demo'
-                }
+                context={"device_id": "macbook_pro", "app_id": "production_demo"},
             )
-            
+
             logger.info(f"Execution result: {result}")
-            
+
             # Show execution history - our unique value
             history = await fabric.get_execution_history()
             logger.info(f"Execution history: {history}")
-        
+
     except Exception as e:
         logger.error(f"Production example failed: {e}")
-        
+
     finally:
         # Always clean up
         await fabric.close()
@@ -308,4 +311,4 @@ async def production_example():
 
 if __name__ == "__main__":
     # Run production example
-    asyncio.run(production_example()) 
\ No newline at end of file
+    asyncio.run(production_example())
diff --git a/fabric/state_continuity_core.py b/fabric/state_continuity_core.py
index d66e2b4..c62f5ca 100644
--- a/fabric/state_continuity_core.py
+++ b/fabric/state_continuity_core.py
@@ -22,25 +22,26 @@ from mcp_use import MCPClient, create_client
 @dataclass
 class VectorClock:
     """Vector clock for distributed state synchronization"""
+
     clocks: Dict[str, int] = field(default_factory=dict)
-    
+
     def increment(self, node_id: str):
         """Increment clock for a node"""
         self.clocks[node_id] = self.clocks.get(node_id, 0) + 1
-    
-    def update(self, other: 'VectorClock'):
+
+    def update(self, other: "VectorClock"):
         """Update with another vector clock"""
         for node_id, clock in other.clocks.items():
             self.clocks[node_id] = max(self.clocks.get(node_id, 0), clock)
-    
-    def happens_before(self, other: 'VectorClock') -> bool:
+
+    def happens_before(self, other: "VectorClock") -> bool:
         """Check if this clock happens before another"""
         for node_id, clock in self.clocks.items():
             if clock > other.clocks.get(node_id, 0):
                 return False
         return True
-    
-    def concurrent_with(self, other: 'VectorClock') -> bool:
+
+    def concurrent_with(self, other: "VectorClock") -> bool:
         """Check if two clocks are concurrent"""
         return not self.happens_before(other) and not other.happens_before(self)
 
@@ -48,6 +49,7 @@ class VectorClock:
 @dataclass
 class StateNode:
     """A node in the state graph"""
+
     id: str
     data: Dict[str, Any]
     vector_clock: VectorClock
@@ -55,7 +57,7 @@ class StateNode:
     application_id: str
     timestamp: float
     parent_id: Optional[str] = None
-    
+
     def hash(self) -> str:
         """Generate hash of the state"""
         content = json.dumps(self.data, sort_keys=True)
@@ -67,33 +69,35 @@ class DifferentialStateEngine:
     The core innovation: tracks state changes differentially across
     devices and applications, enabling seamless continuity.
     """
-    
+
     def __init__(self, node_id: str):
         self.node_id = node_id
         self.states: Dict[str, StateNode] = {}
         self.current_state_id: Optional[str] = None
         self.vector_clock = VectorClock()
-        
+
         # Differential storage - only store changes
         self.deltas: Dict[str, Dict[str, Any]] = {}
-        
+
         # Conflict resolution strategies
         self.conflict_handlers = {
-            'last-write-wins': self._resolve_lww,
-            'merge': self._resolve_merge,
-            'user-defined': self._resolve_user_defined
+            "last-write-wins": self._resolve_lww,
+            "merge": self._resolve_merge,
+            "user-defined": self._resolve_user_defined,
         }
-    
-    def capture_state(self, data: Dict[str, Any], device_id: str, app_id: str) -> StateNode:
+
+    def capture_state(
+        self, data: Dict[str, Any], device_id: str, app_id: str
+    ) -> StateNode:
         """Capture current state with differential tracking"""
         self.vector_clock.increment(self.node_id)
-        
+
         # Calculate delta from previous state
         delta = {}
         if self.current_state_id:
             current = self.states[self.current_state_id]
             delta = self._calculate_delta(current.data, data)
-        
+
         # Create new state node
         state = StateNode(
             id=f"{self.node_id}_{time.time()}",
@@ -102,87 +106,77 @@ class DifferentialStateEngine:
             device_id=device_id,
             application_id=app_id,
             timestamp=time.time(),
-            parent_id=self.current_state_id
+            parent_id=self.current_state_id,
         )
-        
+
         # Store state and delta
         self.states[state.id] = state
         if delta:
             self.deltas[state.id] = delta
-        
+
         self.current_state_id = state.id
         return state
-    
+
     def _calculate_delta(self, old_data: Dict, new_data: Dict) -> Dict:
         """Calculate differential changes between states"""
-        delta = {
-            'added': {},
-            'modified': {},
-            'removed': []
-        }
-        
+        delta = {"added": {}, "modified": {}, "removed": []}
+
         # Find added and modified keys
         for key, value in new_data.items():
             if key not in old_data:
-                delta['added'][key] = value
+                delta["added"][key] = value
             elif old_data[key] != value:
-                delta['modified'][key] = {
-                    'old': old_data[key],
-                    'new': value
-                }
-        
+                delta["modified"][key] = {"old": old_data[key], "new": value}
+
         # Find removed keys
         for key in old_data:
             if key not in new_data:
-                delta['removed'].append(key)
-        
+                delta["removed"].append(key)
+
         return delta
-    
-    def merge_states(self, remote_states: List[StateNode], strategy: str = 'merge') -> StateNode:
+
+    def merge_states(
+        self, remote_states: List[StateNode], strategy: str = "merge"
+    ) -> StateNode:
         """Merge remote states with local state using vector clocks"""
         if not remote_states:
             return self.states[self.current_state_id]
-        
+
         # Group states by vector clock relationships
         concurrent_states = []
         for remote in remote_states:
             if self.vector_clock.concurrent_with(remote.vector_clock):
                 concurrent_states.append(remote)
-        
+
         # Resolve conflicts if any
         if concurrent_states:
             resolver = self.conflict_handlers.get(strategy, self._resolve_merge)
             merged_data = resolver(
-                self.states[self.current_state_id],
-                concurrent_states
+                self.states[self.current_state_id], concurrent_states
             )
         else:
             # No conflicts, take the most recent
             all_states = [self.states[self.current_state_id]] + remote_states
             latest = max(all_states, key=lambda s: s.timestamp)
             merged_data = latest.data
-        
+
         # Update vector clocks
         for remote in remote_states:
             self.vector_clock.update(remote.vector_clock)
-        
+
         # Create merged state
-        return self.capture_state(
-            merged_data,
-            self.node_id,
-            "merged"
-        )
-    
+        return self.capture_state(merged_data, self.node_id, "merged")
+
     def _resolve_lww(self, local: StateNode, remotes: List[StateNode]) -> Dict:
         """Last-write-wins conflict resolution"""
         all_states = [local] + remotes
         latest = max(all_states, key=lambda s: s.timestamp)
         return latest.data
-    
+
     def _resolve_merge(self, local: StateNode, remotes: List[StateNode]) -> Dict:
         """Merge all concurrent states"""
         merged = local.data.copy()
-        
+
         for remote in remotes:
             for key, value in remote.data.items():
                 if key not in merged:
@@ -197,9 +191,9 @@ class DifferentialStateEngine:
                     # Conflict: keep both values
                     merged[f"{key}_local"] = merged[key]
                     merged[f"{key}_remote_{remote.device_id}"] = value
-        
+
         return merged
-    
+
     def _resolve_user_defined(self, local: StateNode, remotes: List[StateNode]) -> Dict:
         """Placeholder for user-defined conflict resolution"""
         # This would call a user-provided function
@@ -211,168 +205,162 @@ class StateContinuityFabric:
     The main fabric that orchestrates state continuity across
     devices and applications, built on top of MCP for service integration.
     """
-    
+
     def __init__(self, fabric_id: str):
         self.fabric_id = fabric_id
         self.engines: Dict[str, DifferentialStateEngine] = {}
         self.mcp_client: Optional[MCPClient] = None
-        
+
         # Cross-device identity management
         self.device_registry: Dict[str, Dict[str, Any]] = {}
-        
+
         # Privacy boundaries
         self.privacy_rules: Dict[str, List[str]] = {
-            'no-sync': [],  # Keys that should never sync
-            'device-only': [],  # Keys that stay on device
-            'encrypted': []  # Keys that need encryption
+            "no-sync": [],  # Keys that should never sync
+            "device-only": [],  # Keys that stay on device
+            "encrypted": [],  # Keys that need encryption
         }
-    
+
     async def initialize(self, mcp_server_url: Optional[str] = None):
         """Initialize the fabric with MCP connection"""
         if mcp_server_url:
             # Use mcp-use to connect to MCP server
             self.mcp_client = await create_client(
-                server_url=mcp_server_url,
-                client_name=f"fabric_{self.fabric_id}"
+                server_url=mcp_server_url, client_name=f"fabric_{self.fabric_id}"
             )
-            
+
             # Discover available tools
             tools = await self.mcp_client.list_tools()
             print(f"Connected to MCP server with {len(tools)} available tools")
-    
+
     def register_device(self, device_id: str, device_info: Dict[str, Any]):
         """Register a device with the fabric"""
         self.device_registry[device_id] = {
-            'info': device_info,
-            'registered_at': time.time(),
-            'last_sync': None
+            "info": device_info,
+            "registered_at": time.time(),
+            "last_sync": None,
         }
-        
+
         # Create state engine for device
         self.engines[device_id] = DifferentialStateEngine(device_id)
-    
-    async def capture_context(self, device_id: str, app_id: str, context: Dict[str, Any]) -> StateNode:
+
+    async def capture_context(
+        self, device_id: str, app_id: str, context: Dict[str, Any]
+    ) -> StateNode:
         """Capture context from a device/application"""
         if device_id not in self.engines:
             raise ValueError(f"Device {device_id} not registered")
-        
+
         # Apply privacy filters
         filtered_context = self._apply_privacy_filters(context)
-        
+
         # Capture state
         state = self.engines[device_id].capture_state(
-            filtered_context,
-            device_id,
-            app_id
+            filtered_context, device_id, app_id
         )
-        
+
         # If MCP is connected, enrich context
         if self.mcp_client:
             enriched = await self._enrich_via_mcp(filtered_context)
             if enriched:
                 state.data.update(enriched)
-        
+
         return state
-    
+
     async def _enrich_via_mcp(self, context: Dict[str, Any]) -> Dict[str, Any]:
         """Use MCP tools to enrich context"""
         enriched = {}
-        
+
         try:
             # Example: Use MCP tools to analyze context
-            if 'text' in context and self.mcp_client:
+            if "text" in context and self.mcp_client:
                 # This would call actual MCP tools
                 result = await self.mcp_client.call_tool(
-                    'analyze_text',
-                    {'text': context['text']}
+                    "analyze_text", {"text": context["text"]}
                 )
-                enriched['mcp_analysis'] = result
+                enriched["mcp_analysis"] = result
         except Exception as e:
             print(f"MCP enrichment failed: {e}")
-        
+
         return enriched
-    
+
     def _apply_privacy_filters(self, context: Dict[str, Any]) -> Dict[str, Any]:
         """Apply privacy rules to context"""
         filtered = {}
-        
+
         for key, value in context.items():
-            if key in self.privacy_rules['no-sync']:
+            if key in self.privacy_rules["no-sync"]:
                 continue
-            elif key in self.privacy_rules['encrypted']:
+            elif key in self.privacy_rules["encrypted"]:
                 # In production, actually encrypt
                 filtered[key] = f"<encrypted>{value}</encrypted>"
             else:
                 filtered[key] = value
-        
+
         return filtered
-    
+
     async def sync_devices(self, source_device: str, target_device: str) -> StateNode:
         """Synchronize state between devices"""
         if source_device not in self.engines or target_device not in self.engines:
             raise ValueError("Both devices must be registered")
-        
+
         source_engine = self.engines[source_device]
         target_engine = self.engines[target_device]
-        
+
         # Get current states
         source_states = [source_engine.states[source_engine.current_state_id]]
-        
+
         # Merge states
         merged = target_engine.merge_states(source_states)
-        
+
         # Update sync timestamps
-        self.device_registry[source_device]['last_sync'] = time.time()
-        self.device_registry[target_device]['last_sync'] = time.time()
-        
+        self.device_registry[source_device]["last_sync"] = time.time()
+        self.device_registry[target_device]["last_sync"] = time.time()
+
         return merged
-    
+
     def get_continuity_graph(self) -> Dict[str, Any]:
         """Get the full continuity graph for visualization"""
-        graph = {
-            'nodes': [],
-            'edges': [],
-            'devices': list(self.device_registry.keys())
-        }
-        
+        graph = {"nodes": [], "edges": [], "devices": list(self.device_registry.keys())}
+
         # Collect all states
         for device_id, engine in self.engines.items():
             for state_id, state in engine.states.items():
-                graph['nodes'].append({
-                    'id': state_id,
-                    'device': device_id,
-                    'app': state.application_id,
-                    'timestamp': state.timestamp,
-                    'data_keys': list(state.data.keys())
-                })
-                
+                graph["nodes"].append(
+                    {
+                        "id": state_id,
+                        "device": device_id,
+                        "app": state.application_id,
+                        "timestamp": state.timestamp,
+                        "data_keys": list(state.data.keys()),
+                    }
+                )
+
                 if state.parent_id:
-                    graph['edges'].append({
-                        'from': state.parent_id,
-                        'to': state_id,
-                        'type': 'evolution'
-                    })
-        
+                    graph["edges"].append(
+                        {"from": state.parent_id, "to": state_id, "type": "evolution"}
+                    )
+
         return graph
 
 
 # Example usage showing integration with mcp-use
 async def demonstrate_fabric():
     """Demonstrate the State Continuity Fabric with MCP integration"""
-    
+
     # Create fabric
     fabric = StateContinuityFabric("user_123")
-    
+
     # Initialize with MCP server (if available)
     try:
         await fabric.initialize("http://localhost:8080")
     except:
         print("Running without MCP server")
-    
+
     # Register devices
     fabric.register_device("macbook", {"type": "laptop", "os": "macOS"})
     fabric.register_device("iphone", {"type": "phone", "os": "iOS"})
-    
+
     # Capture context on MacBook
     macbook_context = await fabric.capture_context(
         "macbook",
@@ -381,12 +369,12 @@ async def demonstrate_fabric():
             "url": "https://example.com",
             "search_query": "quantum computing",
             "timestamp": time.time(),
-            "private_key": "should_not_sync"  # This won't sync
-        }
+            "private_key": "should_not_sync",  # This won't sync
+        },
     )
-    
+
     print(f"Captured MacBook state: {macbook_context.id}")
-    
+
     # Capture context on iPhone
     iphone_context = await fabric.capture_context(
         "iphone",
@@ -394,20 +382,20 @@ async def demonstrate_fabric():
         {
             "location": "work",
             "last_action": "reading_article",
-            "timestamp": time.time()
-        }
+            "timestamp": time.time(),
+        },
     )
-    
+
     print(f"Captured iPhone state: {iphone_context.id}")
-    
+
     # Sync states
     merged = await fabric.sync_devices("macbook", "iphone")
     print(f"Merged state: {merged.data}")
-    
+
     # Get continuity graph
     graph = fabric.get_continuity_graph()
     print(f"Continuity graph: {len(graph['nodes'])} nodes, {len(graph['edges'])} edges")
 
 
 if __name__ == "__main__":
-    asyncio.run(demonstrate_fabric()) 
\ No newline at end of file
+    asyncio.run(demonstrate_fabric())
diff --git a/guardian_linter_watchdog.py b/guardian_linter_watchdog.py
index c34b887..6f044ea 100644
--- a/guardian_linter_watchdog.py
+++ b/guardian_linter_watchdog.py
@@ -21,43 +21,49 @@ LINT_COMMAND = ["pylint"]
 EXCLUDED_DIRS = {"__pycache__", ".git", "venv", "node_modules", ".cursor"}
 # ---------------------
 
-logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
+)
 logger = logging.getLogger(__name__)
 
+
 async def run_linter(file_path: Path):
     """Run the linter on a specific file."""
     if not any(part in EXCLUDED_DIRS for part in file_path.parts):
         command = LINT_COMMAND + [str(file_path)]
         logger.info(f"Guardian: Analyzing {file_path.relative_to(PROJECT_ROOT)}...")
-        
+
         process = await asyncio.create_subprocess_exec(
-            *command,
-            stdout=subprocess.PIPE,
-            stderr=subprocess.PIPE
+            *command, stdout=subprocess.PIPE, stderr=subprocess.PIPE
         )
         stdout, stderr = await process.communicate()
 
         if process.returncode != 0:
-            logger.warning(f"Guardian: Found issues in {file_path.relative_to(PROJECT_ROOT)}")
+            logger.warning(
+                f"Guardian: Found issues in {file_path.relative_to(PROJECT_ROOT)}"
+            )
             if stdout:
                 print("\n--- LINT REPORT ---")
                 print(stdout.decode().strip())
                 print("--- END REPORT ---\n")
             if stderr:
-                logger.error(f"Linter error on {file_path.relative_to(PROJECT_ROOT)}:\n{stderr.decode().strip()}")
+                logger.error(
+                    f"Linter error on {file_path.relative_to(PROJECT_ROOT)}:\n{stderr.decode().strip()}"
+                )
         else:
             logger.info(f"Guardian: {file_path.relative_to(PROJECT_ROOT)} looks clean!")
 
+
 async def watch_directory():
     """Watch the project directory for file changes."""
     logger.info("Guardian Agent (Linter Watchdog) is now active.")
     logger.info(f"Watching for changes in: {PROJECT_ROOT}")
-    
+
     # Simple polling-based watcher
     last_mtimes = {}
-    
+
     while True:
-        for file_path in PROJECT_ROOT.rglob('*'):
+        for file_path in PROJECT_ROOT.rglob("*"):
             if file_path.is_file() and file_path.suffix in WATCHED_EXTENSIONS:
                 try:
                     mtime = file_path.stat().st_mtime
@@ -73,10 +79,11 @@ async def watch_directory():
                     if file_path in last_mtimes:
                         del last_mtimes[file_path]
 
-        await asyncio.sleep(2) # Check for changes every 2 seconds
+        await asyncio.sleep(2)  # Check for changes every 2 seconds
+
 
 if __name__ == "__main__":
     try:
         asyncio.run(watch_directory())
     except KeyboardInterrupt:
-        logger.info("Guardian Agent deactivated.") 
\ No newline at end of file
+        logger.info("Guardian Agent deactivated.")
diff --git a/llm/continuous_learning_system.py b/llm/continuous_learning_system.py
index d49c3a7..59c6bab 100644
--- a/llm/continuous_learning_system.py
+++ b/llm/continuous_learning_system.py
@@ -35,9 +35,11 @@ from protocols.multimodal_llm_analyzer import task as analyze_massive_data
 
 logger = logging.getLogger(__name__)
 
+
 @dataclass
 class TrainingData:
     """Training data structure"""
+
     text: str
     metadata: Dict[str, Any]
     source: str
@@ -45,9 +47,11 @@ class TrainingData:
     quality_score: float
     embedding: Optional[List[float]] = None
 
+
 @dataclass
 class ModelVersion:
     """Model version information"""
+
     version_id: str
     timestamp: datetime
     performance_metrics: Dict[str, float]
@@ -56,681 +60,731 @@ class ModelVersion:
     file_path: str
     checksum: str
 
+
 class ContinuousLearningLLM:
     """
     Continuous Learning LLM System
-    
+
     Learns from massive datasets in real-time without cutoff periods,
     using both classical and quantum computing resources.
     """
-    
+
     def __init__(self, model_name: str = "continuous_learner"):
         self.model_name = model_name
         self.llm_connector = LLMConnector()
         self.quantum_connector = DWaveQuantumConnector()
-        
+
         # Training state
         self.current_model_version = None
         self.training_queue = asyncio.Queue()
         self.is_training = False
         self.training_stats = {
-            'total_samples_processed': 0,
-            'total_training_time': 0.0,
-            'quantum_optimizations': 0,
-            'model_versions': 0
+            "total_samples_processed": 0,
+            "total_training_time": 0.0,
+            "quantum_optimizations": 0,
+            "model_versions": 0,
         }
-        
+
         # Model storage
         self.model_dir = Path("models") / model_name
         self.model_dir.mkdir(parents=True, exist_ok=True)
-        
+
         # Data storage
         self.data_dir = Path("data") / model_name
         self.data_dir.mkdir(parents=True, exist_ok=True)
-        
+
         # Performance tracking
         self.performance_history = []
-        
+
     async def initialize(self, config: Dict[str, Any] = None) -> bool:
         """Initialize the continuous learning system"""
         try:
             logger.info("Initializing Continuous Learning LLM System...")
-            
+
             # Initialize LLM connector
-            llm_config = config.get('llm', {}) if config else {}
+            llm_config = config.get("llm", {}) if config else {}
             llm_connected = await self.llm_connector.connect(llm_config)
-            
+
             if not llm_connected:
                 logger.error("Failed to connect to LLM")
                 return False
-            
+
             logger.info("✅ LLM connected successfully")
-            
+
             # Initialize quantum connector
-            quantum_config = config.get('quantum', {}) if config else {}
+            quantum_config = config.get("quantum", {}) if config else {}
             quantum_connected = await self.quantum_connector.connect(quantum_config)
-            
+
             if quantum_connected:
                 logger.info("✅ Quantum computing resources available")
             else:
                 logger.info("⚠️  Quantum computing not available, using classical only")
-            
+
             # Load or create initial model
             await self._load_or_create_model()
-            
+
             # Start background training loop
             asyncio.create_task(self._training_loop())
-            
+
             logger.info("✅ Continuous Learning LLM System initialized")
             return True
-            
+
         except Exception as e:
             logger.error(f"Failed to initialize continuous learning system: {e}")
             return False
-    
-    async def ingest_data(self, data_source: str, data_type: str = "text") -> Dict[str, Any]:
+
+    async def ingest_data(
+        self, data_source: str, data_type: str = "text"
+    ) -> Dict[str, Any]:
         """
         Ingest new data for continuous learning
-        
+
         Args:
             data_source: Source of the data (file path, URL, etc.)
             data_type: Type of data (text, code, structured, etc.)
         """
         try:
             logger.info(f"Ingesting data from: {data_source}")
-            
+
             # Analyze massive dataset if it's a large collection
-            if data_source.endswith('/gptdata') or 'massive' in data_source.lower():
+            if data_source.endswith("/gptdata") or "massive" in data_source.lower():
                 analysis_result = await self._analyze_massive_dataset(data_source)
                 return await self._process_massive_data(analysis_result)
-            
+
             # Process regular data
             training_data = await self._preprocess_data(data_source, data_type)
-            
+
             # Add to training queue
             await self.training_queue.put(training_data)
-            
+
             return {
-                'success': True,
-                'data_ingested': len(training_data),
-                'queue_size': self.training_queue.qsize(),
-                'data_type': data_type
+                "success": True,
+                "data_ingested": len(training_data),
+                "queue_size": self.training_queue.qsize(),
+                "data_type": data_type,
             }
-            
+
         except Exception as e:
             logger.error(f"Data ingestion failed: {e}")
-            return {
-                'success': False,
-                'error': str(e)
-            }
-    
-    async def train_incrementally(self, training_data: List[TrainingData]) -> Dict[str, Any]:
+            return {"success": False, "error": str(e)}
+
+    async def train_incrementally(
+        self, training_data: List[TrainingData]
+    ) -> Dict[str, Any]:
         """
         Perform incremental training on new data
-        
+
         Args:
             training_data: List of training data samples
         """
         try:
             start_time = time.time()
-            logger.info(f"Starting incremental training on {len(training_data)} samples")
-            
+            logger.info(
+                f"Starting incremental training on {len(training_data)} samples"
+            )
+
             # Preprocess training data
             processed_data = await self._preprocess_training_data(training_data)
-            
+
             # Use quantum optimization for hyperparameter tuning
             if self.quantum_connector.connected:
-                optimized_params = await self._quantum_hyperparameter_optimization(processed_data)
+                optimized_params = await self._quantum_hyperparameter_optimization(
+                    processed_data
+                )
             else:
-                optimized_params = self._classical_hyperparameter_optimization(processed_data)
-            
+                optimized_params = self._classical_hyperparameter_optimization(
+                    processed_data
+                )
+
             # Perform incremental training
-            training_result = await self._perform_training(processed_data, optimized_params)
-            
+            training_result = await self._perform_training(
+                processed_data, optimized_params
+            )
+
             # Update model version
             new_version = await self._create_model_version(training_result)
-            
+
             # Update performance tracking
             training_time = time.time() - start_time
-            self.training_stats['total_samples_processed'] += len(training_data)
-            self.training_stats['total_training_time'] += training_time
-            self.training_stats['model_versions'] += 1
-            
+            self.training_stats["total_samples_processed"] += len(training_data)
+            self.training_stats["total_training_time"] += training_time
+            self.training_stats["model_versions"] += 1
+
             if self.quantum_connector.connected:
-                self.training_stats['quantum_optimizations'] += 1
-            
+                self.training_stats["quantum_optimizations"] += 1
+
             return {
-                'success': True,
-                'training_time': training_time,
-                'samples_processed': len(training_data),
-                'new_model_version': new_version.version_id,
-                'performance_improvement': training_result.get('improvement', 0.0),
-                'quantum_optimized': self.quantum_connector.connected
+                "success": True,
+                "training_time": training_time,
+                "samples_processed": len(training_data),
+                "new_model_version": new_version.version_id,
+                "performance_improvement": training_result.get("improvement", 0.0),
+                "quantum_optimized": self.quantum_connector.connected,
             }
-            
+
         except Exception as e:
             logger.error(f"Incremental training failed: {e}")
-            return {
-                'success': False,
-                'error': str(e)
-            }
-    
+            return {"success": False, "error": str(e)}
+
     async def get_model_info(self) -> Dict[str, Any]:
         """Get current model information"""
         if not self.current_model_version:
-            return {
-                'success': False,
-                'error': 'No model loaded'
-            }
-        
+            return {"success": False, "error": "No model loaded"}
+
         return {
-            'success': True,
-            'model_name': self.model_name,
-            'current_version': self.current_model_version.version_id,
-            'created_at': self.current_model_version.timestamp.isoformat(),
-            'training_stats': self.training_stats,
-            'performance_metrics': self.current_model_version.performance_metrics,
-            'quantum_optimized': self.current_model_version.quantum_optimized
+            "success": True,
+            "model_name": self.model_name,
+            "current_version": self.current_model_version.version_id,
+            "created_at": self.current_model_version.timestamp.isoformat(),
+            "training_stats": self.training_stats,
+            "performance_metrics": self.current_model_version.performance_metrics,
+            "quantum_optimized": self.current_model_version.quantum_optimized,
         }
-    
+
     async def rollback_model(self, version_id: str) -> Dict[str, Any]:
         """
         Rollback to a previous model version
-        
+
         Args:
             version_id: Version ID to rollback to
         """
         try:
             # Find version in history
             version_path = self.model_dir / f"{version_id}.pkl"
-            
+
             if not version_path.exists():
                 return {
-                    'success': False,
-                    'error': f'Model version {version_id} not found'
+                    "success": False,
+                    "error": f"Model version {version_id} not found",
                 }
-            
+
             # Load the version
-            with open(version_path, 'rb') as f:
+            with open(version_path, "rb") as f:
                 model_data = pickle.load(f)
-            
+
             # Set as current model
-            self.current_model_version = model_data['version_info']
-            
+            self.current_model_version = model_data["version_info"]
+
             logger.info(f"Rolled back to model version: {version_id}")
-            
+
             return {
-                'success': True,
-                'rolled_back_to': version_id,
-                'timestamp': self.current_model_version.timestamp.isoformat()
+                "success": True,
+                "rolled_back_to": version_id,
+                "timestamp": self.current_model_version.timestamp.isoformat(),
             }
-            
+
         except Exception as e:
             logger.error(f"Model rollback failed: {e}")
-            return {
-                'success': False,
-                'error': str(e)
-            }
-    
+            return {"success": False, "error": str(e)}
+
     async def _analyze_massive_dataset(self, data_source: str) -> Dict[str, Any]:
         """Analyze massive dataset using existing analyzer"""
         try:
             # Use the existing multimodal LLM analyzer
             analysis_result = analyze_massive_data()
-            
-            if analysis_result['success']:
-                logger.info(f"Analyzed {analysis_result.get('total_files_discovered', 0)} files")
+
+            if analysis_result["success"]:
+                logger.info(
+                    f"Analyzed {analysis_result.get('total_files_discovered', 0)} files"
+                )
                 return analysis_result
             else:
-                logger.error(f"Massive dataset analysis failed: {analysis_result.get('error')}")
-                return {'success': False, 'error': 'Analysis failed'}
-                
+                logger.error(
+                    f"Massive dataset analysis failed: {analysis_result.get('error')}"
+                )
+                return {"success": False, "error": "Analysis failed"}
+
         except Exception as e:
             logger.error(f"Massive dataset analysis failed: {e}")
-            return {'success': False, 'error': str(e)}
-    
-    async def _process_massive_data(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
+            return {"success": False, "error": str(e)}
+
+    async def _process_massive_data(
+        self, analysis_result: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Process massive dataset analysis results"""
         try:
-            if not analysis_result['success']:
+            if not analysis_result["success"]:
                 return analysis_result
-            
+
             # Extract insights and patterns
-            insights = analysis_result.get('insights', {})
-            generated_ideas = analysis_result.get('generated_ideas', [])
-            optimizations = analysis_result.get('optimizations', [])
-            
+            insights = analysis_result.get("insights", {})
+            generated_ideas = analysis_result.get("generated_ideas", [])
+            optimizations = analysis_result.get("optimizations", [])
+
             # Create training data from insights
             training_data = []
-            
+
             # Add insights as training data
             for insight_type, insight_list in insights.items():
                 for insight in insight_list:
-                    training_data.append(TrainingData(
-                        text=str(insight),
-                        metadata={'type': 'insight', 'category': insight_type},
-                        source='massive_analysis',
-                        timestamp=datetime.utcnow(),
-                        quality_score=0.9
-                    ))
-            
+                    training_data.append(
+                        TrainingData(
+                            text=str(insight),
+                            metadata={"type": "insight", "category": insight_type},
+                            source="massive_analysis",
+                            timestamp=datetime.utcnow(),
+                            quality_score=0.9,
+                        )
+                    )
+
             # Add generated ideas as training data
             for idea in generated_ideas:
-                training_data.append(TrainingData(
-                    text=f"{idea.get('name', '')}: {idea.get('description', '')}",
-                    metadata={'type': 'idea', 'rationale': idea.get('rationale', '')},
-                    source='massive_analysis',
-                    timestamp=datetime.utcnow(),
-                    quality_score=0.8
-                ))
-            
+                training_data.append(
+                    TrainingData(
+                        text=f"{idea.get('name', '')}: {idea.get('description', '')}",
+                        metadata={
+                            "type": "idea",
+                            "rationale": idea.get("rationale", ""),
+                        },
+                        source="massive_analysis",
+                        timestamp=datetime.utcnow(),
+                        quality_score=0.8,
+                    )
+                )
+
             # Add optimizations as training data
             for opt in optimizations:
-                training_data.append(TrainingData(
-                    text=f"Optimization: {opt.get('action', '')}",
-                    metadata={'type': 'optimization', 'priority': opt.get('priority', 'medium')},
-                    source='massive_analysis',
-                    timestamp=datetime.utcnow(),
-                    quality_score=0.85
-                ))
-            
+                training_data.append(
+                    TrainingData(
+                        text=f"Optimization: {opt.get('action', '')}",
+                        metadata={
+                            "type": "optimization",
+                            "priority": opt.get("priority", "medium"),
+                        },
+                        source="massive_analysis",
+                        timestamp=datetime.utcnow(),
+                        quality_score=0.85,
+                    )
+                )
+
             # Add to training queue
             for data in training_data:
                 await self.training_queue.put(data)
-            
+
             return {
-                'success': True,
-                'training_data_created': len(training_data),
-                'insights_processed': len(insights),
-                'ideas_generated': len(generated_ideas),
-                'optimizations_found': len(optimizations),
-                'queue_size': self.training_queue.qsize()
+                "success": True,
+                "training_data_created": len(training_data),
+                "insights_processed": len(insights),
+                "ideas_generated": len(generated_ideas),
+                "optimizations_found": len(optimizations),
+                "queue_size": self.training_queue.qsize(),
             }
-            
+
         except Exception as e:
             logger.error(f"Massive data processing failed: {e}")
-            return {
-                'success': False,
-                'error': str(e)
-            }
-    
-    async def _preprocess_data(self, data_source: str, data_type: str) -> List[TrainingData]:
+            return {"success": False, "error": str(e)}
+
+    async def _preprocess_data(
+        self, data_source: str, data_type: str
+    ) -> List[TrainingData]:
         """Preprocess data for training"""
         training_data = []
-        
+
         try:
             if data_type == "text":
                 # Read text file
-                with open(data_source, 'r', encoding='utf-8') as f:
+                with open(data_source, "r", encoding="utf-8") as f:
                     content = f.read()
-                
+
                 # Split into chunks
                 chunks = self._split_text_into_chunks(content, max_chunk_size=1000)
-                
+
                 for i, chunk in enumerate(chunks):
-                    training_data.append(TrainingData(
-                        text=chunk,
-                        metadata={'chunk_id': i, 'source_file': data_source},
-                        source=data_source,
-                        timestamp=datetime.utcnow(),
-                        quality_score=0.7
-                    ))
-            
+                    training_data.append(
+                        TrainingData(
+                            text=chunk,
+                            metadata={"chunk_id": i, "source_file": data_source},
+                            source=data_source,
+                            timestamp=datetime.utcnow(),
+                            quality_score=0.7,
+                        )
+                    )
+
             elif data_type == "code":
                 # Read code file
-                with open(data_source, 'r', encoding='utf-8') as f:
+                with open(data_source, "r", encoding="utf-8") as f:
                     content = f.read()
-                
-                training_data.append(TrainingData(
-                    text=content,
-                    metadata={'file_type': 'code', 'language': self._detect_language(data_source)},
-                    source=data_source,
-                    timestamp=datetime.utcnow(),
-                    quality_score=0.8
-                ))
-            
+
+                training_data.append(
+                    TrainingData(
+                        text=content,
+                        metadata={
+                            "file_type": "code",
+                            "language": self._detect_language(data_source),
+                        },
+                        source=data_source,
+                        timestamp=datetime.utcnow(),
+                        quality_score=0.8,
+                    )
+                )
+
             return training_data
-            
+
         except Exception as e:
             logger.error(f"Data preprocessing failed: {e}")
             return []
-    
-    async def _preprocess_training_data(self, training_data: List[TrainingData]) -> List[TrainingData]:
+
+    async def _preprocess_training_data(
+        self, training_data: List[TrainingData]
+    ) -> List[TrainingData]:
         """Preprocess training data for model training"""
         processed_data = []
-        
+
         for data in training_data:
             # Clean and normalize text
             cleaned_text = self._clean_text(data.text)
-            
+
             # Calculate embeddings (simplified)
             embedding = self._calculate_embedding(cleaned_text)
-            
+
             # Update data
             data.text = cleaned_text
             data.embedding = embedding
-            
+
             # Filter by quality
             if data.quality_score > 0.5:
                 processed_data.append(data)
-        
+
         return processed_data
-    
-    async def _quantum_hyperparameter_optimization(self, training_data: List[TrainingData]) -> Dict[str, Any]:
+
+    async def _quantum_hyperparameter_optimization(
+        self, training_data: List[TrainingData]
+    ) -> Dict[str, Any]:
         """Use quantum computing for hyperparameter optimization"""
         try:
             # Create optimization problem for hyperparameters
-            optimization_problem = self._create_hyperparameter_optimization_problem(training_data)
-            
+            optimization_problem = self._create_hyperparameter_optimization_problem(
+                training_data
+            )
+
             # Solve using quantum annealer
-            result = await self.quantum_connector.execute_action('solve_qubo', {
-                'qubo': optimization_problem,
-                'num_reads': 200,
-                'annealing_time': 50
-            })
-            
-            if result.get('success', False):
+            result = await self.quantum_connector.execute_action(
+                "solve_qubo",
+                {"qubo": optimization_problem, "num_reads": 200, "annealing_time": 50},
+            )
+
+            if result.get("success", False):
                 # Extract optimized parameters
-                solution = result.get('best_solution', {})
+                solution = result.get("best_solution", {})
                 return self._extract_hyperparameters_from_solution(solution)
             else:
                 logger.warning("Quantum optimization failed, using classical fallback")
                 return self._classical_hyperparameter_optimization(training_data)
-                
+
         except Exception as e:
             logger.error(f"Quantum hyperparameter optimization failed: {e}")
             return self._classical_hyperparameter_optimization(training_data)
-    
-    def _classical_hyperparameter_optimization(self, training_data: List[TrainingData]) -> Dict[str, Any]:
+
+    def _classical_hyperparameter_optimization(
+        self, training_data: List[TrainingData]
+    ) -> Dict[str, Any]:
         """Classical hyperparameter optimization"""
         # Simple grid search or random search
         return {
-            'learning_rate': 0.001,
-            'batch_size': 32,
-            'epochs': 10,
-            'optimization_method': 'classical'
+            "learning_rate": 0.001,
+            "batch_size": 32,
+            "epochs": 10,
+            "optimization_method": "classical",
         }
-    
-    async def _perform_training(self, training_data: List[TrainingData], params: Dict[str, Any]) -> Dict[str, Any]:
+
+    async def _perform_training(
+        self, training_data: List[TrainingData], params: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Perform actual model training"""
         try:
             # Simulate training process
             training_time = len(training_data) * 0.01  # Simulate training time
             await asyncio.sleep(training_time)
-            
+
             # Calculate performance improvement
             improvement = np.random.uniform(0.01, 0.05)  # Simulate improvement
-            
+
             return {
-                'success': True,
-                'training_time': training_time,
-                'improvement': improvement,
-                'params_used': params
+                "success": True,
+                "training_time": training_time,
+                "improvement": improvement,
+                "params_used": params,
             }
-            
+
         except Exception as e:
             logger.error(f"Training failed: {e}")
-            return {
-                'success': False,
-                'error': str(e)
-            }
-    
-    async def _create_model_version(self, training_result: Dict[str, Any]) -> ModelVersion:
+            return {"success": False, "error": str(e)}
+
+    async def _create_model_version(
+        self, training_result: Dict[str, Any]
+    ) -> ModelVersion:
         """Create a new model version"""
         version_id = f"v{self.training_stats['model_versions'] + 1}_{int(time.time())}"
-        
+
         # Create version info
         version = ModelVersion(
             version_id=version_id,
             timestamp=datetime.utcnow(),
             performance_metrics={
-                'accuracy': 0.85 + training_result.get('improvement', 0.0),
-                'loss': 0.15 - training_result.get('improvement', 0.0) * 0.5
+                "accuracy": 0.85 + training_result.get("improvement", 0.0),
+                "loss": 0.15 - training_result.get("improvement", 0.0) * 0.5,
             },
-            training_data_size=self.training_stats['total_samples_processed'],
+            training_data_size=self.training_stats["total_samples_processed"],
             quantum_optimized=self.quantum_connector.connected,
             file_path=str(self.model_dir / f"{version_id}.pkl"),
-            checksum=hashlib.md5(version_id.encode()).hexdigest()
+            checksum=hashlib.md5(version_id.encode()).hexdigest(),
         )
-        
+
         # Save model version
         model_data = {
-            'version_info': version,
-            'training_result': training_result,
-            'model_state': 'simulated_model_state'
+            "version_info": version,
+            "training_result": training_result,
+            "model_state": "simulated_model_state",
         }
-        
-        with open(version.file_path, 'wb') as f:
+
+        with open(version.file_path, "wb") as f:
             pickle.dump(model_data, f)
-        
+
         # Update current version
         self.current_model_version = version
-        
+
         logger.info(f"Created model version: {version_id}")
         return version
-    
+
     async def _training_loop(self):
         """Background training loop"""
         while True:
             try:
                 # Wait for training data
                 training_data = []
-                
+
                 # Collect data from queue
                 while not self.training_queue.empty() and len(training_data) < 100:
                     data = await self.training_queue.get()
                     training_data.append(data)
-                
+
                 if training_data:
                     # Perform incremental training
                     result = await self.train_incrementally(training_data)
-                    
-                    if result['success']:
-                        logger.info(f"Training completed: {result['samples_processed']} samples")
+
+                    if result["success"]:
+                        logger.info(
+                            f"Training completed: {result['samples_processed']} samples"
+                        )
                     else:
                         logger.error(f"Training failed: {result['error']}")
-                
+
                 # Wait before next iteration
                 await asyncio.sleep(10)
-                
+
             except Exception as e:
                 logger.error(f"Training loop error: {e}")
                 await asyncio.sleep(30)
-    
+
     async def _load_or_create_model(self):
         """Load existing model or create new one"""
         try:
             # Look for existing model versions
             model_files = list(self.model_dir.glob("*.pkl"))
-            
+
             if model_files:
                 # Load latest version
                 latest_file = max(model_files, key=lambda f: f.stat().st_mtime)
-                
-                with open(latest_file, 'rb') as f:
+
+                with open(latest_file, "rb") as f:
                     model_data = pickle.load(f)
-                
-                self.current_model_version = model_data['version_info']
-                logger.info(f"Loaded model version: {self.current_model_version.version_id}")
+
+                self.current_model_version = model_data["version_info"]
+                logger.info(
+                    f"Loaded model version: {self.current_model_version.version_id}"
+                )
             else:
                 # Create initial model
                 initial_version = ModelVersion(
                     version_id="v1_initial",
                     timestamp=datetime.utcnow(),
-                    performance_metrics={'accuracy': 0.8, 'loss': 0.2},
+                    performance_metrics={"accuracy": 0.8, "loss": 0.2},
                     training_data_size=0,
                     quantum_optimized=False,
                     file_path=str(self.model_dir / "v1_initial.pkl"),
-                    checksum="initial"
+                    checksum="initial",
                 )
-                
+
                 self.current_model_version = initial_version
                 logger.info("Created initial model version")
-                
+
         except Exception as e:
             logger.error(f"Failed to load/create model: {e}")
-    
-    def _split_text_into_chunks(self, text: str, max_chunk_size: int = 1000) -> List[str]:
+
+    def _split_text_into_chunks(
+        self, text: str, max_chunk_size: int = 1000
+    ) -> List[str]:
         """Split text into chunks for training"""
         words = text.split()
         chunks = []
         current_chunk = []
         current_size = 0
-        
+
         for word in words:
             if current_size + len(word) + 1 > max_chunk_size:
-                chunks.append(' '.join(current_chunk))
+                chunks.append(" ".join(current_chunk))
                 current_chunk = [word]
                 current_size = len(word)
             else:
                 current_chunk.append(word)
                 current_size += len(word) + 1
-        
+
         if current_chunk:
-            chunks.append(' '.join(current_chunk))
-        
+            chunks.append(" ".join(current_chunk))
+
         return chunks
-    
+
     def _detect_language(self, file_path: str) -> str:
         """Detect programming language from file extension"""
         ext = Path(file_path).suffix.lower()
         language_map = {
-            '.py': 'python',
-            '.js': 'javascript',
-            '.ts': 'typescript',
-            '.java': 'java',
-            '.cpp': 'cpp',
-            '.c': 'c',
-            '.go': 'go',
-            '.rs': 'rust'
+            ".py": "python",
+            ".js": "javascript",
+            ".ts": "typescript",
+            ".java": "java",
+            ".cpp": "cpp",
+            ".c": "c",
+            ".go": "go",
+            ".rs": "rust",
         }
-        return language_map.get(ext, 'unknown')
-    
+        return language_map.get(ext, "unknown")
+
     def _clean_text(self, text: str) -> str:
         """Clean and normalize text"""
         # Remove extra whitespace
-        text = ' '.join(text.split())
+        text = " ".join(text.split())
         # Basic cleaning
         text = text.strip()
         return text
-    
+
     def _calculate_embedding(self, text: str) -> List[float]:
         """Calculate text embedding (simplified)"""
         # In real implementation, would use a proper embedding model
         # For now, create a simple hash-based embedding
         hash_value = hash(text) % 1000
         return [float(hash_value % 100) / 100.0 for _ in range(10)]
-    
-    def _create_hyperparameter_optimization_problem(self, training_data: List[TrainingData]) -> Dict[str, float]:
+
+    def _create_hyperparameter_optimization_problem(
+        self, training_data: List[TrainingData]
+    ) -> Dict[str, float]:
         """Create QUBO problem for hyperparameter optimization"""
         # Simplified QUBO for learning rate and batch size optimization
         qubo = {}
-        
+
         # Learning rate options: 0.0001, 0.0005, 0.001, 0.005, 0.01
         lr_values = [0.0001, 0.0005, 0.001, 0.005, 0.01]
         for i, lr in enumerate(lr_values):
-            qubo[f'lr_{i}'] = abs(lr - 0.001) * 1000  # Penalty for deviation from default
-        
+            qubo[f"lr_{i}"] = (
+                abs(lr - 0.001) * 1000
+            )  # Penalty for deviation from default
+
         # Batch size options: 16, 32, 64, 128
         batch_values = [16, 32, 64, 128]
         for i, bs in enumerate(batch_values):
-            qubo[f'batch_{i}'] = abs(bs - 32) * 10  # Penalty for deviation from default
-        
+            qubo[f"batch_{i}"] = abs(bs - 32) * 10  # Penalty for deviation from default
+
         # Add constraints (only one value per parameter)
         for i in range(len(lr_values)):
             for j in range(i + 1, len(lr_values)):
-                qubo[f'lr_{i}*lr_{j}'] = 1000  # Large penalty for multiple selections
-        
+                qubo[f"lr_{i}*lr_{j}"] = 1000  # Large penalty for multiple selections
+
         for i in range(len(batch_values)):
             for j in range(i + 1, len(batch_values)):
-                qubo[f'batch_{i}*batch_{j}'] = 1000
-        
+                qubo[f"batch_{i}*batch_{j}"] = 1000
+
         return qubo
-    
-    def _extract_hyperparameters_from_solution(self, solution: Dict[str, int]) -> Dict[str, Any]:
+
+    def _extract_hyperparameters_from_solution(
+        self, solution: Dict[str, int]
+    ) -> Dict[str, Any]:
         """Extract hyperparameters from quantum solution"""
         lr_values = [0.0001, 0.0005, 0.001, 0.005, 0.01]
         batch_values = [16, 32, 64, 128]
-        
+
         selected_lr = 0.001  # Default
         selected_batch = 32  # Default
-        
+
         for i, lr in enumerate(lr_values):
-            if solution.get(f'lr_{i}', 0) == 1:
+            if solution.get(f"lr_{i}", 0) == 1:
                 selected_lr = lr
                 break
-        
+
         for i, bs in enumerate(batch_values):
-            if solution.get(f'batch_{i}', 0) == 1:
+            if solution.get(f"batch_{i}", 0) == 1:
                 selected_batch = bs
                 break
-        
+
         return {
-            'learning_rate': selected_lr,
-            'batch_size': selected_batch,
-            'epochs': 10,
-            'optimization_method': 'quantum'
+            "learning_rate": selected_lr,
+            "batch_size": selected_batch,
+            "epochs": 10,
+            "optimization_method": "quantum",
         }
 
+
 # Global continuous learning system instance
 continuous_learner = ContinuousLearningLLM()
 
+
 # Example usage
 async def demonstrate_continuous_learning():
     """Demonstrate continuous learning LLM system"""
-    
+
     print("=== Continuous Learning LLM System Demo ===\n")
-    
+
     # Initialize system
-    config = {
-        'quantum': {
-            'api_token': os.environ.get('DWAVE_API_TOKEN')
-        }
-    }
-    
+    config = {"quantum": {"api_token": os.environ.get("DWAVE_API_TOKEN")}}
+
     initialized = await continuous_learner.initialize(config)
     if not initialized:
         print("❌ Failed to initialize continuous learning system")
         return
-    
+
     print("✅ Continuous Learning LLM System initialized\n")
-    
+
     # Demo 1: Ingest massive dataset
     print("1. Ingesting massive dataset:")
-    ingest_result = await continuous_learner.ingest_data('/data/gptdata', 'massive')
-    
-    if ingest_result['success']:
+    ingest_result = await continuous_learner.ingest_data("/data/gptdata", "massive")
+
+    if ingest_result["success"]:
         print(f"   - Data ingested: {ingest_result['data_ingested']}")
         print(f"   - Queue size: {ingest_result['queue_size']}")
     else:
         print(f"   - Error: {ingest_result['error']}")
     print()
-    
+
     # Demo 2: Get model info
     print("2. Current model information:")
     model_info = await continuous_learner.get_model_info()
-    
-    if model_info['success']:
+
+    if model_info["success"]:
         print(f"   - Model: {model_info['model_name']}")
         print(f"   - Version: {model_info['current_version']}")
         print(f"   - Quantum optimized: {model_info['quantum_optimized']}")
-        print(f"   - Total samples: {model_info['training_stats']['total_samples_processed']}")
+        print(
+            f"   - Total samples: {model_info['training_stats']['total_samples_processed']}"
+        )
     else:
         print(f"   - Error: {model_info['error']}")
     print()
-    
+
     # Demo 3: Wait for training and check again
     print("3. Waiting for training to complete...")
     await asyncio.sleep(15)  # Wait for background training
-    
+
     updated_info = await continuous_learner.get_model_info()
-    if updated_info['success']:
+    if updated_info["success"]:
         print(f"   - Updated version: {updated_info['current_version']}")
-        print(f"   - New samples: {updated_info['training_stats']['total_samples_processed']}")
-        print(f"   - Quantum optimizations: {updated_info['training_stats']['quantum_optimizations']}")
+        print(
+            f"   - New samples: {updated_info['training_stats']['total_samples_processed']}"
+        )
+        print(
+            f"   - Quantum optimizations: {updated_info['training_stats']['quantum_optimizations']}"
+        )
     print()
-    
+
     print("✅ Continuous Learning LLM System Demo Complete!")
 
+
 if __name__ == "__main__":
-    asyncio.run(demonstrate_continuous_learning()) 
\ No newline at end of file
+    asyncio.run(demonstrate_continuous_learning())
diff --git a/main.py b/main.py
index 3006d4a..eb5787c 100644
--- a/main.py
+++ b/main.py
@@ -7,42 +7,46 @@ from utils.logger import log
 from utils.tracker import get_protocol_stats
 import sys
 
-def run_self_correcting_executor(protocol='default_protocol', iterations=1):
+
+def run_self_correcting_executor(protocol="default_protocol", iterations=1):
     """Run the self-correcting executor with automatic mutation"""
     log(f"🚀 Starting Self-Correcting MCP Executor")
     log(f"Protocol: {protocol}, Iterations: {iterations}")
-    
+
     for i in range(iterations):
         log(f"\n--- Iteration {i+1}/{iterations} ---")
-        
+
         # Execute the protocol
         outcome = execute_task(protocol)
-        
+
         # Immediate mutation check after each execution
         mutated = mutate_protocol(protocol)
-        
+
         # Log iteration summary
         stats = get_protocol_stats(protocol)
         if stats:
-            log(f"Current stats - Success rate: {stats['success_rate']:.2%}, "
-                f"Total executions: {stats['total_executions']}")
-        
+            log(
+                f"Current stats - Success rate: {stats['success_rate']:.2%}, "
+                f"Total executions: {stats['total_executions']}"
+            )
+
         if mutated:
             log(f"🔄 Protocol {protocol} was mutated due to poor performance")
-        
+
     log(f"✅ Self-correcting executor completed {iterations} iterations")
-    
+
     # Final analysis
     final_stats = get_protocol_stats(protocol)
     if final_stats:
         log(f"Final performance - Success rate: {final_stats['success_rate']:.2%}")
-    
+
     return final_stats
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     # Command line arguments
-    protocol = sys.argv[1] if len(sys.argv) > 1 else 'default_protocol'
+    protocol = sys.argv[1] if len(sys.argv) > 1 else "default_protocol"
     iterations = int(sys.argv[2]) if len(sys.argv) > 2 else 5
-    
+
     # Run the self-correcting executor
-    run_self_correcting_executor(protocol, iterations)
\ No newline at end of file
+    run_self_correcting_executor(protocol, iterations)
diff --git a/mcp_runtime_template_hg/api/app.py b/mcp_runtime_template_hg/api/app.py
index 4cb2668..93878d8 100644
--- a/mcp_runtime_template_hg/api/app.py
+++ b/mcp_runtime_template_hg/api/app.py
@@ -1,6 +1,8 @@
 from fastapi import FastAPI
+
 app = FastAPI()
 
-@app.post('/mcp/agent/run')
+
+@app.post("/mcp/agent/run")
 def run_agent(payload: dict):
     return {"status": "Agent run requested", "payload": payload}
diff --git a/mcp_runtime_template_hg/cli/mcp.py b/mcp_runtime_template_hg/cli/mcp.py
index eaa0772..fe61180 100644
--- a/mcp_runtime_template_hg/cli/mcp.py
+++ b/mcp_runtime_template_hg/cli/mcp.py
@@ -1,11 +1,13 @@
 import argparse
 
+
 def run():
     parser = argparse.ArgumentParser()
-    parser.add_argument('agent')
-    parser.add_argument('--context', help='JSON context string')
+    parser.add_argument("agent")
+    parser.add_argument("--context", help="JSON context string")
     args = parser.parse_args()
     print(f"[MCP] Running agent: {args.agent} with context {args.context}")
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     run()
diff --git a/mcp_server/__init__.py b/mcp_server/__init__.py
index c84d3ae..a9a2c5b 100644
--- a/mcp_server/__init__.py
+++ b/mcp_server/__init__.py
@@ -1 +1 @@
-__all__ = [] 
\ No newline at end of file
+__all__ = []
diff --git a/mcp_server/main.py b/mcp_server/main.py
index 202a3bc..c0a292a 100644
--- a/mcp_server/main.py
+++ b/mcp_server/main.py
@@ -83,7 +83,10 @@ class MCPServer:
                 input_schema={
                     "type": "object",
                     "properties": {
-                        "code": {"type": "string", "description": "The code to analyze"},
+                        "code": {
+                            "type": "string",
+                            "description": "The code to analyze",
+                        },
                         "language": {
                             "type": "string",
                             "description": "Programming language",
@@ -206,9 +209,7 @@ class MCPServer:
             raise Exception(f"Unknown method: {method}")
         return handlers[method]
 
-    async def _handle_initialize(
-        self, params: Dict[str, Any]
-    ) -> Dict[str, Any]:
+    async def _handle_initialize(self, params: Dict[str, Any]) -> Dict[str, Any]:
         """Handle the 'initialize' request."""
         client_info = params.get("clientInfo", {})
         LOGGER.info("Initializing session for client: %s", client_info.get("name"))
@@ -250,16 +251,12 @@ class MCPServer:
             "timestamp": datetime.utcnow().isoformat(),
         }
 
-    async def _handle_resources_list(
-        self, params: Dict[str, Any]
-    ) -> Dict[str, Any]:
+    async def _handle_resources_list(self, params: Dict[str, Any]) -> Dict[str, Any]:
         """Handle the 'resources/list' request."""
         _ = params  # Unused
         return {"resources": [resource.to_dict() for resource in self.resources]}
 
-    async def _handle_resources_read(
-        self, params: Dict[str, Any]
-    ) -> Dict[str, Any]:
+    async def _handle_resources_read(self, params: Dict[str, Any]) -> Dict[str, Any]:
         """Handle the 'resources/read' request."""
         uri = params.get("uri")
         content = ""
@@ -297,9 +294,7 @@ class MCPServer:
         _ = params  # Unused
         return {"status": "subscribed"}
 
-    async def _execute_code_analyzer(
-        self, arguments: Dict[str, Any]
-    ) -> Dict[str, Any]:
+    async def _execute_code_analyzer(self, arguments: Dict[str, Any]) -> Dict[str, Any]:
         """Execute the 'code_analyzer' tool."""
         code = arguments.get("code")
         if not code:
@@ -308,7 +303,9 @@ class MCPServer:
         try:
             tree = ast.parse(code)
             lines_of_code = len(code.splitlines())
-            num_functions = sum(1 for node in ast.walk(tree) if isinstance(node, ast.FunctionDef))
+            num_functions = sum(
+                1 for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)
+            )
             complexity = self._calculate_complexity(tree)
 
             result = {
@@ -369,7 +366,11 @@ class MCPServer:
 
             if message.get("jsonrpc") != "2.0":
                 issues.append("Invalid 'jsonrpc' version")
-            if "method" not in message and "result" not in message and "error" not in message:
+            if (
+                "method" not in message
+                and "result" not in message
+                and "error" not in message
+            ):
                 issues.append("Missing 'method', 'result', or 'error' field")
             if "id" not in message:
                 issues.append("Missing 'id' field for request/response")
@@ -406,17 +407,12 @@ class MCPServer:
                     "Found 'time.sleep'. Consider using 'asyncio.sleep' in async code."
                 )
             if re.search(r"except\s*:", code):
-                suggestions.append(
-                    "Found broad 'except:'. Specify the exception type."
-                )
+                suggestions.append("Found broad 'except:'. Specify the exception type.")
 
         except SyntaxError as e:
             suggestions.append(f"Syntax Error: {e}")
 
-        result = {
-            "issues_found": len(suggestions),
-            "suggestions": suggestions
-        }
+        result = {"issues_found": len(suggestions), "suggestions": suggestions}
         return {
             "content": [
                 {
@@ -452,7 +448,9 @@ async def handle_stdin_stdout():
         w_transport, w_protocol = await asyncio.get_event_loop().connect_write_pipe(
             asyncio.Protocol, sys.stdout
         )
-        writer = asyncio.StreamWriter(w_transport, w_protocol, None, asyncio.get_event_loop())
+        writer = asyncio.StreamWriter(
+            w_transport, w_protocol, None, asyncio.get_event_loop()
+        )
 
     while not reader.at_eof():
         line = await reader.readline()
@@ -467,7 +465,7 @@ async def handle_stdin_stdout():
             if writer:
                 writer.write(response_str.encode())
                 await writer.drain()
-            else: # Fallback for Windows
+            else:  # Fallback for Windows
                 print(response_str, flush=True)
 
         except json.JSONDecodeError:
@@ -483,4 +481,4 @@ if __name__ == "__main__":
     try:
         asyncio.run(handle_stdin_stdout())
     except KeyboardInterrupt:
-        LOGGER.info("MCP Server deactivated.") 
\ No newline at end of file
+        LOGGER.info("MCP Server deactivated.")
diff --git a/mcp_server/quantum_tools.py b/mcp_server/quantum_tools.py
index d734c62..fe04098 100644
--- a/mcp_server/quantum_tools.py
+++ b/mcp_server/quantum_tools.py
@@ -25,433 +25,442 @@ from connectors.dwave_quantum_connector import DWaveQuantumConnector, QuantumRes
 
 logger = logging.getLogger(__name__)
 
+
 class QuantumMCPTools:
     """Quantum computing tools for MCP integration"""
-    
+
     def __init__(self):
         self.quantum_connector = DWaveQuantumConnector()
         self.connected = False
         self.solver_info = {}
-        
+
     async def initialize(self, config: Dict[str, Any] = None) -> bool:
         """Initialize quantum connector"""
         try:
             if config is None:
                 config = {}
-            
+
             # Try to connect to D-Wave
             self.connected = await self.quantum_connector.connect(config)
-            
+
             if self.connected:
                 # Get solver information
-                solver_result = await self.quantum_connector.execute_action('get_solver_info')
-                self.solver_info = solver_result.get('solver_info', {})
-                logger.info(f"Connected to quantum solver: {self.solver_info.get('name', 'Unknown')}")
+                solver_result = await self.quantum_connector.execute_action(
+                    "get_solver_info"
+                )
+                self.solver_info = solver_result.get("solver_info", {})
+                logger.info(
+                    f"Connected to quantum solver: {self.solver_info.get('name', 'Unknown')}"
+                )
             else:
                 logger.warning("Quantum connector not available, using simulation mode")
-            
+
             return True
-            
+
         except Exception as e:
             logger.error(f"Failed to initialize quantum tools: {e}")
             return False
-    
-    async def solve_qubo(self, qubo_dict: Dict[str, float], num_reads: int = 100, annealing_time: int = 20) -> Dict[str, Any]:
+
+    async def solve_qubo(
+        self,
+        qubo_dict: Dict[str, float],
+        num_reads: int = 100,
+        annealing_time: int = 20,
+    ) -> Dict[str, Any]:
         """
         Solve QUBO problem using quantum annealer
-        
+
         Args:
             qubo_dict: QUBO coefficients as dictionary
             num_reads: Number of annealing runs
             annealing_time: Annealing time in microseconds
-            
+
         Returns:
             Dictionary with solution and metadata
         """
         try:
             params = {
-                'qubo': qubo_dict,
-                'num_reads': num_reads,
-                'annealing_time': annealing_time
+                "qubo": qubo_dict,
+                "num_reads": num_reads,
+                "annealing_time": annealing_time,
             }
-            
-            result = await self.quantum_connector.execute_action('solve_qubo', params)
-            
-            if 'error' in result:
+
+            result = await self.quantum_connector.execute_action("solve_qubo", params)
+
+            if "error" in result:
                 return {
-                    'success': False,
-                    'error': result['error'],
-                    'method': 'quantum_annealing'
+                    "success": False,
+                    "error": result["error"],
+                    "method": "quantum_annealing",
                 }
-            
+
             # Process quantum result
-            quantum_result = result.get('quantum_result', {})
-            samples = quantum_result.get('samples', [])
-            energies = quantum_result.get('energies', [])
-            
+            quantum_result = result.get("quantum_result", {})
+            samples = quantum_result.get("samples", [])
+            energies = quantum_result.get("energies", [])
+
             if not samples:
                 return {
-                    'success': False,
-                    'error': 'No valid solutions found',
-                    'method': 'quantum_annealing'
+                    "success": False,
+                    "error": "No valid solutions found",
+                    "method": "quantum_annealing",
                 }
-            
+
             # Find best solution
             best_idx = np.argmin(energies) if energies else 0
             best_sample = samples[best_idx] if samples else {}
-            best_energy = energies[best_idx] if energies else float('inf')
-            
+            best_energy = energies[best_idx] if energies else float("inf")
+
             return {
-                'success': True,
-                'method': 'quantum_annealing',
-                'best_solution': best_sample,
-                'best_energy': best_energy,
-                'num_solutions': len(samples),
-                'solver_info': self.solver_info,
-                'quantum_metadata': {
-                    'num_reads': num_reads,
-                    'annealing_time_us': annealing_time,
-                    'chain_break_fraction': quantum_result.get('chain_break_fraction', 0.0),
-                    'success_rate': quantum_result.get('success_rate', 0.0)
-                }
+                "success": True,
+                "method": "quantum_annealing",
+                "best_solution": best_sample,
+                "best_energy": best_energy,
+                "num_solutions": len(samples),
+                "solver_info": self.solver_info,
+                "quantum_metadata": {
+                    "num_reads": num_reads,
+                    "annealing_time_us": annealing_time,
+                    "chain_break_fraction": quantum_result.get(
+                        "chain_break_fraction", 0.0
+                    ),
+                    "success_rate": quantum_result.get("success_rate", 0.0),
+                },
             }
-            
+
         except Exception as e:
             logger.error(f"QUBO solving failed: {e}")
-            return {
-                'success': False,
-                'error': str(e),
-                'method': 'quantum_annealing'
-            }
-    
-    async def optimize_tsp(self, distances: List[List[float]], num_reads: int = 100) -> Dict[str, Any]:
+            return {"success": False, "error": str(e), "method": "quantum_annealing"}
+
+    async def optimize_tsp(
+        self, distances: List[List[float]], num_reads: int = 100
+    ) -> Dict[str, Any]:
         """
         Solve Traveling Salesman Problem using quantum optimization
-        
+
         Args:
             distances: Distance matrix between cities
             num_reads: Number of annealing runs
-            
+
         Returns:
             Dictionary with optimal route and cost
         """
         try:
-            params = {
-                'distances': distances,
-                'num_reads': num_reads
-            }
-            
-            result = await self.quantum_connector.execute_action('traveling_salesman', params)
-            
-            if 'error' in result:
+            params = {"distances": distances, "num_reads": num_reads}
+
+            result = await self.quantum_connector.execute_action(
+                "traveling_salesman", params
+            )
+
+            if "error" in result:
                 return {
-                    'success': False,
-                    'error': result['error'],
-                    'method': 'quantum_tsp'
+                    "success": False,
+                    "error": result["error"],
+                    "method": "quantum_tsp",
                 }
-            
+
             return {
-                'success': True,
-                'method': 'quantum_tsp',
-                'optimal_route': result.get('optimal_route', []),
-                'total_distance': result.get('total_distance', 0.0),
-                'solver_info': self.solver_info,
-                'quantum_metadata': result.get('quantum_metadata', {})
+                "success": True,
+                "method": "quantum_tsp",
+                "optimal_route": result.get("optimal_route", []),
+                "total_distance": result.get("total_distance", 0.0),
+                "solver_info": self.solver_info,
+                "quantum_metadata": result.get("quantum_metadata", {}),
             }
-            
+
         except Exception as e:
             logger.error(f"TSP optimization failed: {e}")
-            return {
-                'success': False,
-                'error': str(e),
-                'method': 'quantum_tsp'
-            }
-    
-    async def optimize_max_cut(self, graph: Dict[str, List[str]], weights: Dict[str, float] = None, num_reads: int = 100) -> Dict[str, Any]:
+            return {"success": False, "error": str(e), "method": "quantum_tsp"}
+
+    async def optimize_max_cut(
+        self,
+        graph: Dict[str, List[str]],
+        weights: Dict[str, float] = None,
+        num_reads: int = 100,
+    ) -> Dict[str, Any]:
         """
         Solve Maximum Cut problem using quantum optimization
-        
+
         Args:
             graph: Graph as adjacency list
             weights: Edge weights (optional)
             num_reads: Number of annealing runs
-            
+
         Returns:
             Dictionary with optimal cut and weight
         """
         try:
-            params = {
-                'graph': graph,
-                'weights': weights or {},
-                'num_reads': num_reads
-            }
-            
-            result = await self.quantum_connector.execute_action('max_cut', params)
-            
-            if 'error' in result:
+            params = {"graph": graph, "weights": weights or {}, "num_reads": num_reads}
+
+            result = await self.quantum_connector.execute_action("max_cut", params)
+
+            if "error" in result:
                 return {
-                    'success': False,
-                    'error': result['error'],
-                    'method': 'quantum_max_cut'
+                    "success": False,
+                    "error": result["error"],
+                    "method": "quantum_max_cut",
                 }
-            
+
             return {
-                'success': True,
-                'method': 'quantum_max_cut',
-                'partition_a': result.get('partition_a', []),
-                'partition_b': result.get('partition_b', []),
-                'cut_weight': result.get('cut_weight', 0.0),
-                'solver_info': self.solver_info,
-                'quantum_metadata': result.get('quantum_metadata', {})
+                "success": True,
+                "method": "quantum_max_cut",
+                "partition_a": result.get("partition_a", []),
+                "partition_b": result.get("partition_b", []),
+                "cut_weight": result.get("cut_weight", 0.0),
+                "solver_info": self.solver_info,
+                "quantum_metadata": result.get("quantum_metadata", {}),
             }
-            
+
         except Exception as e:
             logger.error(f"Max Cut optimization failed: {e}")
-            return {
-                'success': False,
-                'error': str(e),
-                'method': 'quantum_max_cut'
-            }
-    
-    async def accelerate_llm_training(self, training_data: Dict[str, Any], model_config: Dict[str, Any]) -> Dict[str, Any]:
+            return {"success": False, "error": str(e), "method": "quantum_max_cut"}
+
+    async def accelerate_llm_training(
+        self, training_data: Dict[str, Any], model_config: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """
         Quantum-accelerated LLM fine-tuning
-        
+
         Args:
             training_data: Training dataset and parameters
             model_config: Model configuration
-            
+
         Returns:
             Dictionary with optimized training parameters
         """
         try:
             # Convert LLM training to optimization problem
-            optimization_problem = self._llm_to_optimization_problem(training_data, model_config)
-            
+            optimization_problem = self._llm_to_optimization_problem(
+                training_data, model_config
+            )
+
             # Solve using quantum annealer
             qubo_result = await self.solve_qubo(
-                optimization_problem['qubo'],
+                optimization_problem["qubo"],
                 num_reads=200,  # More reads for training optimization
-                annealing_time=50  # Longer annealing for better results
+                annealing_time=50,  # Longer annealing for better results
             )
-            
-            if not qubo_result['success']:
+
+            if not qubo_result["success"]:
                 return {
-                    'success': False,
-                    'error': qubo_result['error'],
-                    'method': 'quantum_llm_acceleration'
+                    "success": False,
+                    "error": qubo_result["error"],
+                    "method": "quantum_llm_acceleration",
                 }
-            
+
             # Convert quantum solution back to training parameters
             optimized_params = self._quantum_solution_to_training_params(
-                qubo_result['best_solution'],
-                training_data,
-                model_config
+                qubo_result["best_solution"], training_data, model_config
             )
-            
+
             return {
-                'success': True,
-                'method': 'quantum_llm_acceleration',
-                'optimized_parameters': optimized_params,
-                'expected_improvement': self._estimate_training_improvement(qubo_result),
-                'quantum_metadata': qubo_result['quantum_metadata'],
-                'solver_info': self.solver_info
+                "success": True,
+                "method": "quantum_llm_acceleration",
+                "optimized_parameters": optimized_params,
+                "expected_improvement": self._estimate_training_improvement(
+                    qubo_result
+                ),
+                "quantum_metadata": qubo_result["quantum_metadata"],
+                "solver_info": self.solver_info,
             }
-            
+
         except Exception as e:
             logger.error(f"LLM acceleration failed: {e}")
             return {
-                'success': False,
-                'error': str(e),
-                'method': 'quantum_llm_acceleration'
+                "success": False,
+                "error": str(e),
+                "method": "quantum_llm_acceleration",
             }
-    
-    async def manage_quantum_resources(self, action: str, params: Dict[str, Any] = None) -> Dict[str, Any]:
+
+    async def manage_quantum_resources(
+        self, action: str, params: Dict[str, Any] = None
+    ) -> Dict[str, Any]:
         """
         Manage quantum computing resources
-        
+
         Args:
             action: Resource management action
             params: Action parameters
-            
+
         Returns:
             Dictionary with resource status
         """
         try:
-            if action == 'get_status':
+            if action == "get_status":
                 return {
-                    'success': True,
-                    'connected': self.connected,
-                    'solver_info': self.solver_info,
-                    'available_qubits': self.solver_info.get('num_qubits', 0),
-                    'solver_type': self.solver_info.get('type', 'unknown')
+                    "success": True,
+                    "connected": self.connected,
+                    "solver_info": self.solver_info,
+                    "available_qubits": self.solver_info.get("num_qubits", 0),
+                    "solver_type": self.solver_info.get("type", "unknown"),
                 }
-            
-            elif action == 'reserve_qubits':
-                num_qubits = params.get('num_qubits', 1)
-                available = self.solver_info.get('num_qubits', 0)
-                
+
+            elif action == "reserve_qubits":
+                num_qubits = params.get("num_qubits", 1)
+                available = self.solver_info.get("num_qubits", 0)
+
                 if num_qubits <= available:
                     return {
-                        'success': True,
-                        'reserved_qubits': num_qubits,
-                        'remaining_qubits': available - num_qubits
+                        "success": True,
+                        "reserved_qubits": num_qubits,
+                        "remaining_qubits": available - num_qubits,
                     }
                 else:
                     return {
-                        'success': False,
-                        'error': f'Not enough qubits available. Requested: {num_qubits}, Available: {available}'
+                        "success": False,
+                        "error": f"Not enough qubits available. Requested: {num_qubits}, Available: {available}",
                     }
-            
-            elif action == 'get_solver_properties':
+
+            elif action == "get_solver_properties":
                 return {
-                    'success': True,
-                    'solver_properties': self.solver_info,
-                    'annealing_time_range': self.solver_info.get('annealing_time_range', []),
-                    'programming_thermalization': self.solver_info.get('programming_thermalization', [])
+                    "success": True,
+                    "solver_properties": self.solver_info,
+                    "annealing_time_range": self.solver_info.get(
+                        "annealing_time_range", []
+                    ),
+                    "programming_thermalization": self.solver_info.get(
+                        "programming_thermalization", []
+                    ),
                 }
-            
+
             else:
-                return {
-                    'success': False,
-                    'error': f'Unknown action: {action}'
-                }
-                
+                return {"success": False, "error": f"Unknown action: {action}"}
+
         except Exception as e:
             logger.error(f"Resource management failed: {e}")
-            return {
-                'success': False,
-                'error': str(e)
-            }
-    
-    def _llm_to_optimization_problem(self, training_data: Dict[str, Any], model_config: Dict[str, Any]) -> Dict[str, Any]:
+            return {"success": False, "error": str(e)}
+
+    def _llm_to_optimization_problem(
+        self, training_data: Dict[str, Any], model_config: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Convert LLM training problem to QUBO optimization"""
         # This is a simplified conversion - real implementation would be more sophisticated
-        
+
         # Extract training parameters
-        learning_rate = model_config.get('learning_rate', 0.001)
-        batch_size = model_config.get('batch_size', 32)
-        epochs = model_config.get('epochs', 10)
-        
+        learning_rate = model_config.get("learning_rate", 0.001)
+        batch_size = model_config.get("batch_size", 32)
+        epochs = model_config.get("epochs", 10)
+
         # Create QUBO for hyperparameter optimization
         qubo = {}
-        
+
         # Learning rate optimization (discrete values)
         lr_values = [0.0001, 0.0005, 0.001, 0.005, 0.01]
         for i, lr in enumerate(lr_values):
-            qubo[f'x{i}'] = abs(lr - learning_rate) * 1000  # Penalty for deviation
-        
+            qubo[f"x{i}"] = abs(lr - learning_rate) * 1000  # Penalty for deviation
+
         # Batch size optimization
         batch_values = [16, 32, 64, 128]
         for i, bs in enumerate(batch_values):
-            qubo[f'y{i}'] = abs(bs - batch_size) * 10
-        
+            qubo[f"y{i}"] = abs(bs - batch_size) * 10
+
         # Add constraints (only one value per parameter)
         for i in range(len(lr_values)):
             for j in range(i + 1, len(lr_values)):
-                qubo[f'x{i}*x{j}'] = 1000  # Large penalty for multiple selections
-        
+                qubo[f"x{i}*x{j}"] = 1000  # Large penalty for multiple selections
+
         for i in range(len(batch_values)):
             for j in range(i + 1, len(batch_values)):
-                qubo[f'y{i}*y{j}'] = 1000
-        
-        return {'qubo': qubo}
-    
-    def _quantum_solution_to_training_params(self, solution: Dict[str, int], training_data: Dict[str, Any], model_config: Dict[str, Any]) -> Dict[str, Any]:
+                qubo[f"y{i}*y{j}"] = 1000
+
+        return {"qubo": qubo}
+
+    def _quantum_solution_to_training_params(
+        self,
+        solution: Dict[str, int],
+        training_data: Dict[str, Any],
+        model_config: Dict[str, Any],
+    ) -> Dict[str, Any]:
         """Convert quantum solution back to training parameters"""
         # Extract selected values from quantum solution
         lr_values = [0.0001, 0.0005, 0.001, 0.005, 0.01]
         batch_values = [16, 32, 64, 128]
-        
+
         selected_lr = None
         selected_batch = None
-        
+
         for i, lr in enumerate(lr_values):
-            if solution.get(f'x{i}', 0) == 1:
+            if solution.get(f"x{i}", 0) == 1:
                 selected_lr = lr
                 break
-        
+
         for i, bs in enumerate(batch_values):
-            if solution.get(f'y{i}', 0) == 1:
+            if solution.get(f"y{i}", 0) == 1:
                 selected_batch = bs
                 break
-        
+
         return {
-            'learning_rate': selected_lr or model_config.get('learning_rate', 0.001),
-            'batch_size': selected_batch or model_config.get('batch_size', 32),
-            'epochs': model_config.get('epochs', 10),
-            'optimization_method': 'quantum_annealing'
+            "learning_rate": selected_lr or model_config.get("learning_rate", 0.001),
+            "batch_size": selected_batch or model_config.get("batch_size", 32),
+            "epochs": model_config.get("epochs", 10),
+            "optimization_method": "quantum_annealing",
         }
-    
-    def _estimate_training_improvement(self, qubo_result: Dict[str, Any]) -> Dict[str, Any]:
+
+    def _estimate_training_improvement(
+        self, qubo_result: Dict[str, Any]
+    ) -> Dict[str, Any]:
         """Estimate expected improvement from quantum optimization"""
         # This would use historical data and quantum result quality
         return {
-            'convergence_speedup': 1.5,  # 50% faster convergence
-            'final_accuracy_improvement': 0.02,  # 2% accuracy improvement
-            'confidence': 0.85
+            "convergence_speedup": 1.5,  # 50% faster convergence
+            "final_accuracy_improvement": 0.02,  # 2% accuracy improvement
+            "confidence": 0.85,
         }
 
+
 # Global quantum tools instance
 quantum_tools = QuantumMCPTools()
 
+
 # Example usage
 async def demonstrate_quantum_tools():
     """Demonstrate quantum MCP tools"""
-    
+
     print("=== Quantum MCP Tools Demo ===\n")
-    
+
     # Initialize quantum tools
     await quantum_tools.initialize()
-    
+
     # Demo 1: QUBO solving
     print("1. QUBO Problem Solving:")
-    qubo = {
-        'x0': -1.0,
-        'x1': -1.0,
-        'x0*x1': 2.0
-    }
-    
+    qubo = {"x0": -1.0, "x1": -1.0, "x0*x1": 2.0}
+
     result = await quantum_tools.solve_qubo(qubo, num_reads=100)
     print(f"   - Success: {result['success']}")
-    if result['success']:
+    if result["success"]:
         print(f"   - Best solution: {result['best_solution']}")
         print(f"   - Energy: {result['best_energy']}")
         print(f"   - Method: {result['method']}")
     else:
         print(f"   - Error: {result['error']}")
     print()
-    
+
     # Demo 2: Resource management
     print("2. Quantum Resource Management:")
-    status = await quantum_tools.manage_quantum_resources('get_status')
+    status = await quantum_tools.manage_quantum_resources("get_status")
     print(f"   - Connected: {status['connected']}")
     print(f"   - Available qubits: {status['available_qubits']}")
     print(f"   - Solver type: {status['solver_type']}")
     print()
-    
+
     # Demo 3: LLM acceleration
     print("3. LLM Training Acceleration:")
-    training_data = {
-        'dataset_size': 10000,
-        'vocabulary_size': 50000
-    }
-    model_config = {
-        'learning_rate': 0.001,
-        'batch_size': 32,
-        'epochs': 10
-    }
-    
-    llm_result = await quantum_tools.accelerate_llm_training(training_data, model_config)
+    training_data = {"dataset_size": 10000, "vocabulary_size": 50000}
+    model_config = {"learning_rate": 0.001, "batch_size": 32, "epochs": 10}
+
+    llm_result = await quantum_tools.accelerate_llm_training(
+        training_data, model_config
+    )
     print(f"   - Success: {llm_result['success']}")
-    if llm_result['success']:
+    if llm_result["success"]:
         print(f"   - Optimized parameters: {llm_result['optimized_parameters']}")
         print(f"   - Expected improvement: {llm_result['expected_improvement']}")
     else:
         print(f"   - Error: {llm_result['error']}")
     print()
-    
+
     print("✅ Quantum MCP Tools Demo Complete!")
 
+
 if __name__ == "__main__":
-    asyncio.run(demonstrate_quantum_tools()) 
\ No newline at end of file
+    asyncio.run(demonstrate_quantum_tools())
diff --git a/orchestrator.py b/orchestrator.py
index c874916..b924b02 100644
--- a/orchestrator.py
+++ b/orchestrator.py
@@ -12,6 +12,7 @@ from utils.db_tracker import track_outcome, get_protocol_stats
 try:
     from agents.specialized.code_generator import code_generator_agent
     from agents.specialized.filesystem_agent import file_system_agent
+
     specialized_agents_available = True
 except ImportError as e:
     print(f"Warning: Could not import specialized agents: {e}")
@@ -22,11 +23,13 @@ except ImportError as e:
 # Import LLM connector
 try:
     from connectors.llm_connector import llm_connector
+
     llm_available = True
 except ImportError:
     llm_available = False
     llm_connector = None
 
+
 class OrchestrationEngine:
     """
     Multi-layer orchestration matching the architecture:
@@ -37,270 +40,301 @@ class OrchestrationEngine:
     - Protocol Implementation: MCP core
     - Persistence: Knowledge graph
     """
-    
+
     def __init__(self):
         self.agents = {}
         self.connectors = {}
         self.active_workflows = {}
         self.message_bus = MessageBus()
         self.knowledge_graph = KnowledgeGraph()
-        
+
         # Register specialized agents
         if specialized_agents_available:
             if code_generator_agent:
-                self.agents['code_generator'] = code_generator_agent
+                self.agents["code_generator"] = code_generator_agent
             if file_system_agent:
-                self.agents['file_system_agent'] = file_system_agent
-        
-    async def execute_intent(self, intent: str, sources: Optional[List[str]] = None, options: Optional[Dict] = None):
+                self.agents["file_system_agent"] = file_system_agent
+
+    async def execute_intent(
+        self,
+        intent: str,
+        sources: Optional[List[str]] = None,
+        options: Optional[Dict] = None,
+    ):
         """
         Main entry point - processes user intent through the full stack
         """
         log(f"🎯 Processing intent: {intent}")
-        
+
         # 1. Intent Analysis
         analyzed_intent = await self.analyze_intent(intent)
-        
+
         # 2. Component Discovery
-        required_components = await self.discover_components(analyzed_intent, sources or [])
-        
+        required_components = await self.discover_components(
+            analyzed_intent, sources or []
+        )
+
         # 3. Workflow Generation
         workflow = await self.generate_workflow(analyzed_intent, required_components)
-        
+
         # 4. Execute Workflow
         result = await self.execute_workflow(workflow)
-        
+
         # 5. Learn from Execution
         await self.learn_from_execution(workflow, result)
-        
+
         return result
-    
+
     async def analyze_intent(self, intent: str) -> Dict:
         """Analyze user intent to determine required actions"""
         intent_lower = intent.lower()
-        
+
         # Determine action type based on keywords
-        if any(keyword in intent_lower for keyword in ['generate', 'create', 'code', 'api', 'endpoint']):
-            action = 'generate_code'
-            target = 'api'
-        elif any(keyword in intent_lower for keyword in ['list', 'show files', 'directory', 'ls']):
-            action = 'list_directory'
-            target = 'filesystem'
-        elif any(keyword in intent_lower for keyword in ['read', 'open', 'cat', 'show file']):
-            action = 'read_file'
-            target = 'filesystem'
-        elif any(keyword in intent_lower for keyword in ['multi-modal', 'llm', 'ideate', 'learn']):
-            action = 'multimodal_analysis'
-            target = 'llm'
-        elif any(keyword in intent_lower for keyword in ['analyze', 'pattern', 'insight']):
-            action = 'analyze'
-            target = 'data'
-        elif any(keyword in intent_lower for keyword in ['check', 'health', 'status']):
-            action = 'monitor'
-            target = 'system'
+        if any(
+            keyword in intent_lower
+            for keyword in ["generate", "create", "code", "api", "endpoint"]
+        ):
+            action = "generate_code"
+            target = "api"
+        elif any(
+            keyword in intent_lower
+            for keyword in ["list", "show files", "directory", "ls"]
+        ):
+            action = "list_directory"
+            target = "filesystem"
+        elif any(
+            keyword in intent_lower for keyword in ["read", "open", "cat", "show file"]
+        ):
+            action = "read_file"
+            target = "filesystem"
+        elif any(
+            keyword in intent_lower
+            for keyword in ["multi-modal", "llm", "ideate", "learn"]
+        ):
+            action = "multimodal_analysis"
+            target = "llm"
+        elif any(
+            keyword in intent_lower for keyword in ["analyze", "pattern", "insight"]
+        ):
+            action = "analyze"
+            target = "data"
+        elif any(keyword in intent_lower for keyword in ["check", "health", "status"]):
+            action = "monitor"
+            target = "system"
         else:
-            action = 'process'
-            target = 'general'
-        
+            action = "process"
+            target = "general"
+
         return {
-            'original_intent': intent,
-            'parsed_intent': {
-                'action': action,
-                'target': target,
-                'details': self._extract_details(intent, action),
-                'constraints': [],
-                'requirements': []
+            "original_intent": intent,
+            "parsed_intent": {
+                "action": action,
+                "target": target,
+                "details": self._extract_details(intent, action),
+                "constraints": [],
+                "requirements": [],
             },
-            'confidence': 0.95,
-            'timestamp': datetime.utcnow().isoformat()
+            "confidence": 0.95,
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     def _extract_details(self, intent: str, action: str) -> Dict:
         """Extract specific details like paths from the intent string."""
-        if action in ['list_directory', 'read_file']:
+        if action in ["list_directory", "read_file"]:
             # Simple path extraction: assumes path is the last word
             parts = intent.split()
-            path = parts[-1] if len(parts) > 1 and '/' in parts[-1] else '.'
+            path = parts[-1] if len(parts) > 1 and "/" in parts[-1] else "."
             # A more robust NLP/regex solution would be better here in a real system
-            return {'path': path}
+            return {"path": path}
         return {}
-    
+
     async def discover_components(self, intent: Dict, sources: List[str]) -> Dict:
         """Discover which components are needed"""
         components = {
-            'agents': [],
-            'protocols': [],
-            'connectors': [],
-            'analyzers': [],
-            'services': []
+            "agents": [],
+            "protocols": [],
+            "connectors": [],
+            "analyzers": [],
+            "services": [],
         }
-        
+
         # Based on intent, determine needed components
-        action = intent['parsed_intent']['action']
-        
-        if action == 'generate_code':
-            components['agents'].append('code_generator')
-        elif action in ['list_directory', 'read_file']:
-            components['agents'].append('file_system_agent')
-        elif action == 'analyze':
-            components['analyzers'].append('pattern_detector')
-            components['protocols'].append('data_processor')
-        elif action == 'monitor':
-            components['protocols'].append('system_monitor')
+        action = intent["parsed_intent"]["action"]
+
+        if action == "generate_code":
+            components["agents"].append("code_generator")
+        elif action in ["list_directory", "read_file"]:
+            components["agents"].append("file_system_agent")
+        elif action == "analyze":
+            components["analyzers"].append("pattern_detector")
+            components["protocols"].append("data_processor")
+        elif action == "monitor":
+            components["protocols"].append("system_monitor")
         else:
             # Default workflow
-            components['protocols'].append('data_processor')
-            components['analyzers'].append('pattern_detector')
-            
+            components["protocols"].append("data_processor")
+            components["analyzers"].append("pattern_detector")
+
         if sources:
             for source in sources:
-                if source == 'github':
-                    components['connectors'].append('github_mcp')
-                elif source == 'claude':
-                    components['connectors'].append('claude_mcp')
-                    
+                if source == "github":
+                    components["connectors"].append("github_mcp")
+                elif source == "claude":
+                    components["connectors"].append("claude_mcp")
+
         return components
-    
+
     async def generate_workflow(self, intent: Dict, components: Dict) -> Dict:
         """Generate optimized workflow from components"""
         workflow = {
-            'id': f"wf_{datetime.utcnow().timestamp()}",
-            'intent': intent,
-            'steps': [],
-            'parallel_groups': [],
-            'decision_points': []
+            "id": f"wf_{datetime.utcnow().timestamp()}",
+            "intent": intent,
+            "steps": [],
+            "parallel_groups": [],
+            "decision_points": [],
         }
-        
+
         # Build workflow steps based on action
-        action = intent['parsed_intent']['action']
-        
-        if action == 'generate_code' and 'code_generator' in components['agents']:
+        action = intent["parsed_intent"]["action"]
+
+        if action == "generate_code" and "code_generator" in components["agents"]:
             # Code generation workflow
-            workflow['steps'].append({
-                'type': 'agent',
-                'name': 'code_generator',
-                'inputs': {
-                    'intent': intent['original_intent'],
-                    'context': {
-                        'endpoint_name': 'generated-endpoint',
-                        'description': 'Auto-generated API endpoint'
-                    }
-                },
-                'outputs': ['generated_code', 'instructions']
-            })
-        elif action in ['list_directory', 'read_file'] and 'file_system_agent' in components['agents']:
+            workflow["steps"].append(
+                {
+                    "type": "agent",
+                    "name": "code_generator",
+                    "inputs": {
+                        "intent": intent["original_intent"],
+                        "context": {
+                            "endpoint_name": "generated-endpoint",
+                            "description": "Auto-generated API endpoint",
+                        },
+                    },
+                    "outputs": ["generated_code", "instructions"],
+                }
+            )
+        elif (
+            action in ["list_directory", "read_file"]
+            and "file_system_agent" in components["agents"]
+        ):
             # Filesystem workflow
-            workflow['steps'].append({
-                'type': 'agent',
-                'name': 'file_system_agent',
-                'inputs': {
-                    'action': action,
-                    'path': intent['parsed_intent'].get('details', {}).get('path', '.')
-                },
-                'outputs': ['file_data']
-            })
+            workflow["steps"].append(
+                {
+                    "type": "agent",
+                    "name": "file_system_agent",
+                    "inputs": {
+                        "action": action,
+                        "path": intent["parsed_intent"]
+                        .get("details", {})
+                        .get("path", "."),
+                    },
+                    "outputs": ["file_data"],
+                }
+            )
         else:
             # Default data processing workflow
-            if 'data_processor' in components['protocols']:
-                workflow['steps'].append({
-                    'type': 'protocol',
-                    'name': 'data_processor',
-                    'inputs': {'source': 'user_data'},
-                    'outputs': ['processed_data']
-                })
-                
-            if 'pattern_detector' in components['analyzers']:
-                workflow['steps'].append({
-                    'type': 'analyzer',
-                    'name': 'pattern_detector',
-                    'inputs': {'data': 'processed_data'},
-                    'outputs': ['patterns', 'insights']
-                })
-            
+            if "data_processor" in components["protocols"]:
+                workflow["steps"].append(
+                    {
+                        "type": "protocol",
+                        "name": "data_processor",
+                        "inputs": {"source": "user_data"},
+                        "outputs": ["processed_data"],
+                    }
+                )
+
+            if "pattern_detector" in components["analyzers"]:
+                workflow["steps"].append(
+                    {
+                        "type": "analyzer",
+                        "name": "pattern_detector",
+                        "inputs": {"data": "processed_data"},
+                        "outputs": ["patterns", "insights"],
+                    }
+                )
+
         return workflow
-    
+
     async def execute_workflow(self, workflow: Dict) -> Dict:
         """Execute the generated workflow"""
         results = {
-            'workflow_id': workflow['id'],
-            'status': 'running',
-            'steps_completed': [],
-            'outputs': {}
+            "workflow_id": workflow["id"],
+            "status": "running",
+            "steps_completed": [],
+            "outputs": {},
         }
-        
+
         # Execute each step
-        for step in workflow['steps']:
+        for step in workflow["steps"]:
             try:
-                if step['type'] == 'protocol':
-                    result = await self.execute_protocol(step['name'], step['inputs'])
-                elif step['type'] == 'analyzer':
-                    result = await self.execute_analyzer(step['name'], step['inputs'])
-                elif step['type'] == 'agent':
-                    result = await self.execute_agent(step['name'], step['inputs'])
-                    
-                results['steps_completed'].append({
-                    'step': step['name'],
-                    'status': 'success',
-                    'output': result
-                })
-                
+                if step["type"] == "protocol":
+                    result = await self.execute_protocol(step["name"], step["inputs"])
+                elif step["type"] == "analyzer":
+                    result = await self.execute_analyzer(step["name"], step["inputs"])
+                elif step["type"] == "agent":
+                    result = await self.execute_agent(step["name"], step["inputs"])
+
+                results["steps_completed"].append(
+                    {"step": step["name"], "status": "success", "output": result}
+                )
+
                 # Store outputs for next steps
-                for output_key in step.get('outputs', []):
-                    results['outputs'][output_key] = result
-                    
+                for output_key in step.get("outputs", []):
+                    results["outputs"][output_key] = result
+
             except Exception as e:
-                results['steps_completed'].append({
-                    'step': step['name'],
-                    'status': 'failed',
-                    'error': str(e)
-                })
-                results['status'] = 'failed'
+                results["steps_completed"].append(
+                    {"step": step["name"], "status": "failed", "error": str(e)}
+                )
+                results["status"] = "failed"
                 break
-                
-        if results['status'] == 'running':
-            results['status'] = 'completed'
-            
+
+        if results["status"] == "running":
+            results["status"] = "completed"
+
         return results
-    
+
     async def execute_protocol(self, name: str, inputs: Dict) -> Any:
         """Execute a protocol"""
         from protocols.loader import load_protocol
+
         protocol = load_protocol(name)
         if protocol:
-            return protocol['task']()
+            return protocol["task"]()
         raise Exception(f"Protocol {name} not found")
-    
+
     async def execute_analyzer(self, name: str, inputs: Dict) -> Any:
         """Execute an analyzer"""
         # Load and execute analyzer
         # This would be implemented based on analyzer type
-        return {'analysis': 'complete', 'insights': []}
-    
+        return {"analysis": "complete", "insights": []}
+
     async def execute_agent(self, name: str, inputs: Dict) -> Any:
         """Execute an agent task"""
         if name in self.agents:
             return await self.agents[name].execute(inputs)
         raise Exception(f"Agent {name} not found")
-    
+
     async def learn_from_execution(self, workflow: Dict, result: Dict):
         """Learn from execution to improve future workflows"""
         # Track execution metrics
         execution_data = {
-            'workflow_id': workflow['id'],
-            'intent': workflow['intent']['original_intent'],
-            'success': result['status'] == 'completed',
-            'duration': datetime.utcnow().timestamp() - float(workflow['id'].split('_')[1]),
-            'steps_count': len(result['steps_completed']),
-            'timestamp': datetime.utcnow().isoformat()
+            "workflow_id": workflow["id"],
+            "intent": workflow["intent"]["original_intent"],
+            "success": result["status"] == "completed",
+            "duration": datetime.utcnow().timestamp()
+            - float(workflow["id"].split("_")[1]),
+            "steps_count": len(result["steps_completed"]),
+            "timestamp": datetime.utcnow().isoformat(),
         }
-        
+
         # Store in knowledge graph for future optimization
         await self.knowledge_graph.store_execution(execution_data)
-        
+
         # Trigger mutation if needed
-        if not execution_data['success']:
+        if not execution_data["success"]:
             await self.trigger_improvement(workflow, result)
-    
+
     async def trigger_improvement(self, workflow: Dict, result: Dict):
         """Trigger system improvement based on failure"""
         log(f"🔧 Triggering improvement for workflow {workflow['id']}")
@@ -313,58 +347,56 @@ class OrchestrationEngine:
 
 class MessageBus:
     """Handles A2A (Agent to Agent) communication"""
-    
+
     def __init__(self):
         self.subscribers = {}
         self.message_queue = asyncio.Queue()
-        
+
     async def publish(self, topic: str, message: Dict):
         """Publish message to topic"""
-        await self.message_queue.put({
-            'topic': topic,
-            'message': message,
-            'timestamp': datetime.utcnow().isoformat()
-        })
-        
+        await self.message_queue.put(
+            {
+                "topic": topic,
+                "message": message,
+                "timestamp": datetime.utcnow().isoformat(),
+            }
+        )
+
     async def subscribe(self, topic: str, callback):
         """Subscribe to topic"""
         if topic not in self.subscribers:
             self.subscribers[topic] = []
         self.subscribers[topic].append(callback)
-        
+
     async def process_messages(self):
         """Process message queue"""
         while True:
             msg = await self.message_queue.get()
-            topic = msg['topic']
+            topic = msg["topic"]
             if topic in self.subscribers:
                 for callback in self.subscribers[topic]:
-                    await callback(msg['message'])
+                    await callback(msg["message"])
 
 
 class KnowledgeGraph:
     """Manages relationships and accelerates learning"""
-    
+
     def __init__(self):
         self.graph = {}  # Simple in-memory for now
-        
+
     async def store_execution(self, data: Dict):
         """Store execution data in graph"""
-        node_id = data['workflow_id']
-        self.graph[node_id] = {
-            'data': data,
-            'relationships': [],
-            'insights': []
-        }
-        
+        node_id = data["workflow_id"]
+        self.graph[node_id] = {"data": data, "relationships": [], "insights": []}
+
     async def find_similar_executions(self, intent: str) -> List[Dict]:
         """Find similar past executions"""
         similar = []
         for node_id, node in self.graph.items():
-            if intent.lower() in node['data']['intent'].lower():
-                similar.append(node['data'])
+            if intent.lower() in node["data"]["intent"].lower():
+                similar.append(node["data"])
         return similar
-    
+
     async def get_optimization_hints(self, workflow: Dict) -> List[str]:
         """Get hints for optimizing workflow"""
         hints = []
@@ -377,11 +409,8 @@ class KnowledgeGraph:
 async def run_mcp(intent: str, sources: List[str], quantum: bool = False) -> Dict:
     """Run MCP orchestration from CLI"""
     engine = OrchestrationEngine()
-    
-    options = {
-        'quantum_optimization': quantum,
-        'sources': sources
-    }
-    
+
+    options = {"quantum_optimization": quantum, "sources": sources}
+
     result = await engine.execute_intent(intent, sources, options)
-    return result 
\ No newline at end of file
+    return result
diff --git a/protocols/--development.py b/protocols/--development.py
index f68679a..15d8232 100644
--- a/protocols/--development.py
+++ b/protocols/--development.py
@@ -2,14 +2,15 @@
 # Previous failure rate: 50.00%
 # Mutation timestamp: {"timestamp": "2025-06-20T01:16:14.421448"}
 
+
 def task():
     """Improved task implementation after mutation"""
     from random import choice, random
-    
+
     # Enhanced logic with better success probability
     if random() < 0.8:  # 80% success rate instead of 50%
-        result = {'success': True, 'action': 'optimized_execution', 'mutation_gen': 1}
+        result = {"success": True, "action": "optimized_execution", "mutation_gen": 1}
     else:
-        result = {'success': False, 'action': 'controlled_failure', 'mutation_gen': 1}
-    
+        result = {"success": False, "action": "controlled_failure", "mutation_gen": 1}
+
     return result
diff --git a/protocols/__init__.py b/protocols/__init__.py
index b9b8fab..60e8b91 100644
--- a/protocols/__init__.py
+++ b/protocols/__init__.py
@@ -3,4 +3,4 @@ Self-Correcting Protocol Framework
 Dynamic protocol loading and execution
 """
 
-__all__ = ["loader"] 
\ No newline at end of file
+__all__ = ["loader"]
diff --git a/protocols/always_fails.py b/protocols/always_fails.py
index 14eb70e..8e2526d 100644
--- a/protocols/always_fails.py
+++ b/protocols/always_fails.py
@@ -2,14 +2,15 @@
 # Previous failure rate: 100.00%
 # Mutation timestamp: {"timestamp": "2025-06-17T23:48:10.484687"}
 
+
 def task():
     """Improved task implementation after mutation"""
     from random import choice, random
-    
+
     # Enhanced logic with better success probability
     if random() < 0.8:  # 80% success rate instead of 50%
-        result = {'success': True, 'action': 'optimized_execution', 'mutation_gen': 1}
+        result = {"success": True, "action": "optimized_execution", "mutation_gen": 1}
     else:
-        result = {'success': False, 'action': 'controlled_failure', 'mutation_gen': 1}
-    
+        result = {"success": False, "action": "controlled_failure", "mutation_gen": 1}
+
     return result
diff --git a/protocols/api_health_checker.py b/protocols/api_health_checker.py
index 647c1b1..5b2a100 100644
--- a/protocols/api_health_checker.py
+++ b/protocols/api_health_checker.py
@@ -2,46 +2,50 @@
 import requests
 import time
 
+
 def task():
     """Check health of various API endpoints"""
     endpoints = [
-        {'name': 'Local API', 'url': 'http://localhost:8080/health'},
-        {'name': 'JSONPlaceholder', 'url': 'https://jsonplaceholder.typicode.com/posts/1'},
-        {'name': 'GitHub API', 'url': 'https://api.github.com/rate_limit'}
+        {"name": "Local API", "url": "http://localhost:8080/health"},
+        {
+            "name": "JSONPlaceholder",
+            "url": "https://jsonplaceholder.typicode.com/posts/1",
+        },
+        {"name": "GitHub API", "url": "https://api.github.com/rate_limit"},
     ]
-    
+
     results = []
     failures = 0
-    
+
     for endpoint in endpoints:
         try:
             start_time = time.time()
-            response = requests.get(endpoint['url'], timeout=5)
+            response = requests.get(endpoint["url"], timeout=5)
             response_time = (time.time() - start_time) * 1000  # ms
-            
-            results.append({
-                'name': endpoint['name'],
-                'status': response.status_code,
-                'response_time_ms': round(response_time, 2),
-                'healthy': response.status_code == 200
-            })
-            
+
+            results.append(
+                {
+                    "name": endpoint["name"],
+                    "status": response.status_code,
+                    "response_time_ms": round(response_time, 2),
+                    "healthy": response.status_code == 200,
+                }
+            )
+
             if response.status_code != 200:
                 failures += 1
-                
+
         except Exception as e:
             failures += 1
-            results.append({
-                'name': endpoint['name'],
-                'error': str(e),
-                'healthy': False
-            })
-    
+            results.append(
+                {"name": endpoint["name"], "error": str(e), "healthy": False}
+            )
+
     return {
-        'success': failures < len(endpoints) / 2,  # Success if less than half failed
-        'action': 'api_health_check',
-        'total_endpoints': len(endpoints),
-        'healthy_count': len(endpoints) - failures,
-        'failure_count': failures,
-        'results': results
-    } 
\ No newline at end of file
+        "success": failures < len(endpoints) / 2,  # Success if less than half failed
+        "action": "api_health_check",
+        "total_endpoints": len(endpoints),
+        "healthy_count": len(endpoints) - failures,
+        "failure_count": failures,
+        "results": results,
+    }
diff --git a/protocols/data_processor.py b/protocols/data_processor.py
index 8cee79e..b57d364 100644
--- a/protocols/data_processor.py
+++ b/protocols/data_processor.py
@@ -4,79 +4,82 @@ import csv
 import os
 from datetime import datetime
 
+
 def task():
     """Process data files and extract insights"""
     # Try multiple possible data directories
     possible_dirs = [
-        os.environ.get('DATA_DIR', '/data'),
-        '/data',
-        '/app/data',
-        '/tmp',
-        os.getcwd()
+        os.environ.get("DATA_DIR", "/data"),
+        "/data",
+        "/app/data",
+        "/tmp",
+        os.getcwd(),
     ]
-    
+
     data_dir = None
     for dir_path in possible_dirs:
         if os.path.exists(dir_path) and os.path.isdir(dir_path):
             data_dir = dir_path
             break
-    
+
     if not data_dir:
         # Create a mock result when no data directory exists
         return {
-            'success': True,
-            'action': 'data_processing',
-            'mode': 'simulation',
-            'message': 'No data directory found, returning simulated results',
-            'files_processed': 3,
-            'total_records': 150,
-            'insights': [
-                'Simulated: Found 3 data files',
-                'Simulated: Processed 150 records total',
-                'Simulated: Average processing time 0.5s per file'
+            "success": True,
+            "action": "data_processing",
+            "mode": "simulation",
+            "message": "No data directory found, returning simulated results",
+            "files_processed": 3,
+            "total_records": 150,
+            "insights": [
+                "Simulated: Found 3 data files",
+                "Simulated: Processed 150 records total",
+                "Simulated: Average processing time 0.5s per file",
             ],
-            'timestamp': datetime.utcnow().isoformat()
+            "timestamp": datetime.utcnow().isoformat(),
         }
-    
+
     try:
         processed_count = 0
         total_records = 0
         insights = []
-        
+
         # Look for JSON and CSV files
         files = os.listdir(data_dir)[:10]  # Limit to 10 files
-        
+
         if not files:
             # No files found, return success with empty results
             return {
-                'success': True,
-                'action': 'data_processing',
-                'message': f'No data files found in {data_dir}',
-                'files_processed': 0,
-                'total_records': 0,
-                'insights': [],
-                'timestamp': datetime.utcnow().isoformat()
+                "success": True,
+                "action": "data_processing",
+                "message": f"No data files found in {data_dir}",
+                "files_processed": 0,
+                "total_records": 0,
+                "insights": [],
+                "timestamp": datetime.utcnow().isoformat(),
             }
-        
+
         for filename in files:
             file_path = os.path.join(data_dir, filename)
-            
-            if filename.endswith('.json'):
+
+            if filename.endswith(".json"):
                 try:
-                    with open(file_path, 'r') as f:
+                    with open(file_path, "r") as f:
                         data = json.load(f)
                         if isinstance(data, list):
                             total_records += len(data)
                         elif isinstance(data, dict):
                             total_records += 1
                         processed_count += 1
-                        insights.append(f"{filename}: {type(data).__name__} with {len(data) if isinstance(data, (list, dict)) else 1} items")
+                        insights.append(
+                            f"{filename}: {type(data).__name__} with {len(data) if isinstance(data, (list, dict)) else 1} items"
+                        )
                 except:
                     pass
-                    
-            elif filename.endswith('.csv'):
+
+            elif filename.endswith(".csv"):
                 try:
-                    with open(file_path, 'r') as f:
+                    with open(file_path, "r") as f:
                         reader = csv.reader(f)
                         row_count = sum(1 for row in reader)
                         total_records += row_count
@@ -84,21 +87,19 @@ def task():
                         insights.append(f"{filename}: CSV with {row_count} rows")
                 except:
                     pass
-        
+
         # Always return success if we got this far
         return {
-            'success': True,
-            'action': 'data_processing',
-            'directory': data_dir,
-            'files_processed': processed_count,
-            'total_records': total_records,
-            'insights': insights[:5] if insights else ['No data files found to process'],
-            'timestamp': datetime.utcnow().isoformat()
+            "success": True,
+            "action": "data_processing",
+            "directory": data_dir,
+            "files_processed": processed_count,
+            "total_records": total_records,
+            "insights": (
+                insights[:5] if insights else ["No data files found to process"]
+            ),
+            "timestamp": datetime.utcnow().isoformat(),
         }
-        
+
     except Exception as e:
-        return {
-            'success': False,
-            'action': 'data_processing',
-            'error': str(e)
-        } 
\ No newline at end of file
+        return {"success": False, "action": "data_processing", "error": str(e)}
diff --git a/protocols/database_health_check.py b/protocols/database_health_check.py
index 1a7e5d2..2a5694d 100644
--- a/protocols/database_health_check.py
+++ b/protocols/database_health_check.py
@@ -3,44 +3,48 @@ import psycopg2
 import os
 from datetime import datetime
 
+
 def task():
     """Check PostgreSQL database connectivity and health"""
     try:
         # Get database credentials from environment
         db_config = {
-            'host': os.environ.get('POSTGRES_HOST', 'mcp_db'),
-            'port': os.environ.get('POSTGRES_PORT', '5432'),
-            'user': os.environ.get('POSTGRES_USER', 'mcp'),
-            'password': os.environ.get('POSTGRES_PASSWORD', 'mcp'),
-            'database': os.environ.get('POSTGRES_DB', 'mcp')
+            "host": os.environ.get("POSTGRES_HOST", "mcp_db"),
+            "port": os.environ.get("POSTGRES_PORT", "5432"),
+            "user": os.environ.get("POSTGRES_USER", "mcp"),
+            "password": os.environ.get("POSTGRES_PASSWORD", "mcp"),
+            "database": os.environ.get("POSTGRES_DB", "mcp"),
         }
-        
+
         # Connect to database
         conn = psycopg2.connect(**db_config)
         cursor = conn.cursor()
-        
+
         # Check database version
         cursor.execute("SELECT version();")
         db_version = cursor.fetchone()[0]
-        
+
         # Check database size
         cursor.execute("SELECT pg_database_size(current_database());")
         db_size = cursor.fetchone()[0]
-        
+
         # Check connection count
         cursor.execute("SELECT count(*) FROM pg_stat_activity;")
         connection_count = cursor.fetchone()[0]
-        
+
         # Check table count
-        cursor.execute("""
+        cursor.execute(
+            """
             SELECT count(*) 
             FROM information_schema.tables 
             WHERE table_schema = 'public';
-        """)
+        """
+        )
         table_count = cursor.fetchone()[0]
-        
+
         # Create a test table if it doesn't exist
-        cursor.execute("""
+        cursor.execute(
+            """
             CREATE TABLE IF NOT EXISTS protocol_executions (
                 id SERIAL PRIMARY KEY,
                 protocol_name VARCHAR(100),
@@ -48,36 +52,45 @@ def task():
                 success BOOLEAN,
                 details JSONB
             );
-        """)
-        
+        """
+        )
+
         # Insert a test record
-        cursor.execute("""
+        cursor.execute(
+            """
             INSERT INTO protocol_executions (protocol_name, execution_time, success, details)
             VALUES (%s, %s, %s, %s);
-        """, ('database_health_check', datetime.utcnow(), True, '{"action": "health_check"}'))
-        
+        """,
+            (
+                "database_health_check",
+                datetime.utcnow(),
+                True,
+                '{"action": "health_check"}',
+            ),
+        )
+
         conn.commit()
         cursor.close()
         conn.close()
-        
+
         return {
-            'success': True,
-            'action': 'database_health_check',
-            'database_info': {
-                'version': db_version.split(',')[0],  # Truncate long version string
-                'size_bytes': db_size,
-                'size_mb': round(db_size / (1024 * 1024), 2),
-                'connection_count': connection_count,
-                'table_count': table_count
+            "success": True,
+            "action": "database_health_check",
+            "database_info": {
+                "version": db_version.split(",")[0],  # Truncate long version string
+                "size_bytes": db_size,
+                "size_mb": round(db_size / (1024 * 1024), 2),
+                "connection_count": connection_count,
+                "table_count": table_count,
             },
-            'test_write': 'successful',
-            'timestamp': datetime.utcnow().isoformat()
+            "test_write": "successful",
+            "timestamp": datetime.utcnow().isoformat(),
         }
-        
+
     except Exception as e:
         return {
-            'success': False,
-            'action': 'database_health_check',
-            'error': str(e),
-            'timestamp': datetime.utcnow().isoformat()
-        } 
\ No newline at end of file
+            "success": False,
+            "action": "database_health_check",
+            "error": str(e),
+            "timestamp": datetime.utcnow().isoformat(),
+        }
diff --git a/protocols/default_protocol.py b/protocols/default_protocol.py
index 6574c87..1fb0bbd 100644
--- a/protocols/default_protocol.py
+++ b/protocols/default_protocol.py
@@ -2,14 +2,15 @@
 # Previous failure rate: 80.00%
 # Mutation timestamp: {"timestamp": "2025-06-17T22:38:35.707168"}
 
+
 def task():
     """Improved task implementation after mutation"""
     from random import choice, random
-    
+
     # Enhanced logic with better success probability
     if random() < 0.8:  # 80% success rate instead of 50%
-        result = {'success': True, 'action': 'optimized_execution', 'mutation_gen': 1}
+        result = {"success": True, "action": "optimized_execution", "mutation_gen": 1}
     else:
-        result = {'success': False, 'action': 'controlled_failure', 'mutation_gen': 1}
-    
+        result = {"success": False, "action": "controlled_failure", "mutation_gen": 1}
+
     return result
diff --git a/protocols/execution_analytics.py b/protocols/execution_analytics.py
index 6c5861e..03f5c16 100644
--- a/protocols/execution_analytics.py
+++ b/protocols/execution_analytics.py
@@ -4,21 +4,23 @@ import os
 from datetime import datetime, timedelta
 import json
 
+
 def task():
     """Analyze execution patterns and provide insights from database"""
     try:
         # Connect to database
         conn = psycopg2.connect(
-            host=os.environ.get('POSTGRES_HOST', 'mcp_db'),
-            port=os.environ.get('POSTGRES_PORT', '5432'),
-            user=os.environ.get('POSTGRES_USER', 'mcp'),
-            password=os.environ.get('POSTGRES_PASSWORD', 'mcp'),
-            database=os.environ.get('POSTGRES_DB', 'mcp')
+            host=os.environ.get("POSTGRES_HOST", "mcp_db"),
+            port=os.environ.get("POSTGRES_PORT", "5432"),
+            user=os.environ.get("POSTGRES_USER", "mcp"),
+            password=os.environ.get("POSTGRES_PASSWORD", "mcp"),
+            database=os.environ.get("POSTGRES_DB", "mcp"),
         )
         cursor = conn.cursor()
-        
+
         # Get overall statistics
-        cursor.execute("""
+        cursor.execute(
+            """
             SELECT 
                 COUNT(*) as total_executions,
                 SUM(CASE WHEN success THEN 1 ELSE 0 END) as successful_runs,
@@ -26,13 +28,15 @@ def task():
                 MIN(execution_time) as first_execution,
                 MAX(execution_time) as last_execution
             FROM protocol_executions
-        """)
-        
+        """
+        )
+
         overall_stats = cursor.fetchone()
         total, successes, unique_protocols, first_exec, last_exec = overall_stats
-        
+
         # Get per-protocol performance
-        cursor.execute("""
+        cursor.execute(
+            """
             SELECT 
                 protocol_name,
                 COUNT(*) as runs,
@@ -41,20 +45,24 @@ def task():
             FROM protocol_executions
             GROUP BY protocol_name
             ORDER BY success_rate DESC
-        """)
-        
+        """
+        )
+
         protocol_performance = []
         for row in cursor.fetchall():
             protocol_name, runs, successes, success_rate = row
-            protocol_performance.append({
-                'protocol': protocol_name,
-                'runs': runs,
-                'successes': successes,
-                'success_rate': round(success_rate, 2)
-            })
-        
+            protocol_performance.append(
+                {
+                    "protocol": protocol_name,
+                    "runs": runs,
+                    "successes": successes,
+                    "success_rate": round(success_rate, 2),
+                }
+            )
+
         # Get recent failure patterns
-        cursor.execute("""
+        cursor.execute(
+            """
             SELECT 
                 protocol_name,
                 (details->>'error')::text as error_message,
@@ -66,19 +74,19 @@ def task():
             GROUP BY protocol_name, error_message
             ORDER BY occurrences DESC
             LIMIT 5
-        """)
-        
+        """
+        )
+
         failure_patterns = []
         for row in cursor.fetchall():
             protocol, error, count = row
-            failure_patterns.append({
-                'protocol': protocol,
-                'error': error,
-                'occurrences': count
-            })
-        
+            failure_patterns.append(
+                {"protocol": protocol, "error": error, "occurrences": count}
+            )
+
         # Get mutation effectiveness
-        cursor.execute("""
+        cursor.execute(
+            """
             SELECT 
                 pm.protocol_name,
                 pm.previous_failure_rate * 100 as before_mutation,
@@ -98,57 +106,60 @@ def task():
             ) current_stats ON pm.protocol_name = current_stats.protocol_name
             ORDER BY pm.mutation_time DESC
             LIMIT 5
-        """)
-        
+        """
+        )
+
         mutation_effectiveness = []
         for row in cursor.fetchall():
             protocol, before, after = row
-            mutation_effectiveness.append({
-                'protocol': protocol,
-                'failure_rate_before': round(before, 2),
-                'success_rate_after': round(after or 0, 2),
-                'improvement': round((after or 0) - (100 - before), 2)
-            })
-        
+            mutation_effectiveness.append(
+                {
+                    "protocol": protocol,
+                    "failure_rate_before": round(before, 2),
+                    "success_rate_after": round(after or 0, 2),
+                    "improvement": round((after or 0) - (100 - before), 2),
+                }
+            )
+
         cursor.close()
         conn.close()
-        
+
         # Generate insights
         insights = []
         if total > 0:
             overall_success_rate = (successes / total) * 100
             insights.append(f"Overall success rate: {overall_success_rate:.1f}%")
-            
+
             if overall_success_rate < 50:
-                insights.append("⚠️ System performance below 50% - review failing protocols")
+                insights.append(
+                    "⚠️ System performance below 50% - review failing protocols"
+                )
             elif overall_success_rate > 80:
                 insights.append("✅ System performing well with >80% success rate")
-                
+
             if len(failure_patterns) > 0:
-                insights.append(f"Most common error: '{failure_patterns[0]['error']}' ({failure_patterns[0]['occurrences']} times)")
-        
+                insights.append(
+                    f"Most common error: '{failure_patterns[0]['error']}' ({failure_patterns[0]['occurrences']} times)"
+                )
+
         return {
-            'success': True,
-            'action': 'execution_analytics',
-            'overall_stats': {
-                'total_executions': total,
-                'successful_runs': successes,
-                'unique_protocols': unique_protocols,
-                'time_range': {
-                    'first': first_exec.isoformat() if first_exec else None,
-                    'last': last_exec.isoformat() if last_exec else None
-                }
+            "success": True,
+            "action": "execution_analytics",
+            "overall_stats": {
+                "total_executions": total,
+                "successful_runs": successes,
+                "unique_protocols": unique_protocols,
+                "time_range": {
+                    "first": first_exec.isoformat() if first_exec else None,
+                    "last": last_exec.isoformat() if last_exec else None,
+                },
             },
-            'protocol_performance': protocol_performance,
-            'recent_failures': failure_patterns,
-            'mutation_effectiveness': mutation_effectiveness,
-            'insights': insights,
-            'timestamp': datetime.utcnow().isoformat()
+            "protocol_performance": protocol_performance,
+            "recent_failures": failure_patterns,
+            "mutation_effectiveness": mutation_effectiveness,
+            "insights": insights,
+            "timestamp": datetime.utcnow().isoformat(),
         }
-        
+
     except Exception as e:
-        return {
-            'success': False,
-            'action': 'execution_analytics',
-            'error': str(e)
-        }
+        return {"success": False, "action": "execution_analytics", "error": str(e)}
diff --git a/protocols/file_validator.py b/protocols/file_validator.py
index 1d468c2..7a8ea2f 100644
--- a/protocols/file_validator.py
+++ b/protocols/file_validator.py
@@ -2,14 +2,15 @@
 # Previous failure rate: 100.00%
 # Mutation timestamp: {"timestamp": "2025-06-17T23:27:06.905318"}
 
+
 def task():
     """Improved task implementation after mutation"""
     from random import choice, random
-    
+
     # Enhanced logic with better success probability
     if random() < 0.8:  # 80% success rate instead of 50%
-        result = {'success': True, 'action': 'optimized_execution', 'mutation_gen': 1}
+        result = {"success": True, "action": "optimized_execution", "mutation_gen": 1}
     else:
-        result = {'success': False, 'action': 'controlled_failure', 'mutation_gen': 1}
-    
+        result = {"success": False, "action": "controlled_failure", "mutation_gen": 1}
+
     return result
diff --git a/protocols/file_validator_docker.py b/protocols/file_validator_docker.py
index 2e374b8..7456329 100644
--- a/protocols/file_validator_docker.py
+++ b/protocols/file_validator_docker.py
@@ -2,14 +2,15 @@
 # Previous failure rate: 100.00%
 # Mutation timestamp: {"timestamp": "2025-06-17T23:43:47.338354"}
 
+
 def task():
     """Improved task implementation after mutation"""
     from random import choice, random
-    
+
     # Enhanced logic with better success probability
     if random() < 0.8:  # 80% success rate instead of 50%
-        result = {'success': True, 'action': 'optimized_execution', 'mutation_gen': 1}
+        result = {"success": True, "action": "optimized_execution", "mutation_gen": 1}
     else:
-        result = {'success': False, 'action': 'controlled_failure', 'mutation_gen': 1}
-    
+        result = {"success": False, "action": "controlled_failure", "mutation_gen": 1}
+
     return result
diff --git a/protocols/loader.py b/protocols/loader.py
index 3d6218f..813d4de 100644
--- a/protocols/loader.py
+++ b/protocols/loader.py
@@ -3,28 +3,24 @@ import importlib
 import os
 from utils.logger import log
 
+
 def load_protocol(name):
     """Dynamically load a protocol module by name"""
     try:
         module = importlib.import_module(f"protocols.{name}")
-        return {
-            'name': name,
-            'task': module.task
-        }
+        return {"name": name, "task": module.task}
     except ImportError as e:
         log(f"Failed to load protocol {name}: {e}")
         # Create default protocol if it doesn't exist
         create_default_protocol(name)
         # Try loading again
         module = importlib.import_module(f"protocols.{name}")
-        return {
-            'name': name,
-            'task': module.task
-        }
+        return {"name": name, "task": module.task}
+
 
 def create_default_protocol(name):
     """Create a default protocol file if it doesn't exist"""
-    protocol_file = f'protocols/{name}.py'
+    protocol_file = f"protocols/{name}.py"
     if not os.path.exists(protocol_file):
         log(f"Creating default protocol: {name}")
         default_content = f'''# Default protocol: {name}
@@ -36,17 +32,18 @@ def task():
     result = {{'success': bool(randint(0, 1)), 'action': 'default_execution'}}
     return result
 '''
-        with open(protocol_file, 'w') as f:
+        with open(protocol_file, "w") as f:
             f.write(default_content)
         log(f"Default protocol {name} created successfully")
 
+
 def list_protocols():
     """List all available protocols"""
     protocols = []
-    protocols_dir = 'protocols'
+    protocols_dir = "protocols"
     if os.path.exists(protocols_dir):
         for filename in os.listdir(protocols_dir):
-            if filename.endswith('.py') and not filename.startswith('__'):
+            if filename.endswith(".py") and not filename.startswith("__"):
                 protocol_name = filename[:-3]  # Remove .py extension
                 protocols.append(protocol_name)
-    return protocols
\ No newline at end of file
+    return protocols
diff --git a/protocols/log_analyzer.py b/protocols/log_analyzer.py
index eb40349..13990ca 100644
--- a/protocols/log_analyzer.py
+++ b/protocols/log_analyzer.py
@@ -4,66 +4,65 @@ import re
 from datetime import datetime
 from collections import Counter
 
+
 def task():
     """Analyze log files for errors, warnings, and patterns"""
-    log_dir = os.environ.get('LOG_DIR', '/app/logs')
-    
+    log_dir = os.environ.get("LOG_DIR", "/app/logs")
+
     try:
         total_lines = 0
         error_count = 0
         warning_count = 0
         error_patterns = []
         activity_timeline = Counter()
-        
+
         # Analyze all log files in directory
         for filename in os.listdir(log_dir):
-            if filename.endswith('.log'):
+            if filename.endswith(".log"):
                 file_path = os.path.join(log_dir, filename)
-                
-                with open(file_path, 'r') as f:
+
+                with open(file_path, "r") as f:
                     for line in f:
                         total_lines += 1
-                        
+
                         # Check for errors
-                        if 'error' in line.lower() or 'exception' in line.lower():
+                        if "error" in line.lower() or "exception" in line.lower():
                             error_count += 1
                             error_patterns.append(line.strip()[:100])  # First 100 chars
-                            
+
                         # Check for warnings
-                        if 'warning' in line.lower() or 'warn' in line.lower():
+                        if "warning" in line.lower() or "warn" in line.lower():
                             warning_count += 1
-                            
+
                         # Extract timestamps for activity timeline
-                        timestamp_match = re.search(r'\[(\d{4}-\d{2}-\d{2} \d{2}):', line)
+                        timestamp_match = re.search(
+                            r"\[(\d{4}-\d{2}-\d{2} \d{2}):", line
+                        )
                         if timestamp_match:
                             hour = timestamp_match.group(1)
                             activity_timeline[hour] += 1
-        
+
         # Calculate health score
         if total_lines > 0:
             error_rate = error_count / total_lines
             success = error_rate < 0.05  # Less than 5% errors
         else:
             success = True  # No logs is not a failure
-            
+
         # Get top activity hours
         top_hours = activity_timeline.most_common(5)
-        
+
         return {
-            'success': success,
-            'action': 'log_analysis',
-            'total_lines_analyzed': total_lines,
-            'error_count': error_count,
-            'warning_count': warning_count,
-            'error_rate': round(error_rate * 100, 2) if total_lines > 0 else 0,
-            'recent_errors': error_patterns[-5:],  # Last 5 errors
-            'peak_activity_hours': dict(top_hours),
-            'log_directory': log_dir
+            "success": success,
+            "action": "log_analysis",
+            "total_lines_analyzed": total_lines,
+            "error_count": error_count,
+            "warning_count": warning_count,
+            "error_rate": round(error_rate * 100, 2) if total_lines > 0 else 0,
+            "recent_errors": error_patterns[-5:],  # Last 5 errors
+            "peak_activity_hours": dict(top_hours),
+            "log_directory": log_dir,
         }
-        
+
     except Exception as e:
-        return {
-            'success': False,
-            'action': 'log_analysis',
-            'error': str(e)
-        } 
\ No newline at end of file
+        return {"success": False, "action": "log_analysis", "error": str(e)}
diff --git a/protocols/multimodal_llm_analyzer.py b/protocols/multimodal_llm_analyzer.py
index 4eb43fb..830cc9b 100644
--- a/protocols/multimodal_llm_analyzer.py
+++ b/protocols/multimodal_llm_analyzer.py
@@ -8,6 +8,7 @@ from typing import Dict, List, Any
 import numpy as np
 import random
 
+
 def task():
     """
     Real multi-modal LLM analyzer for MASSIVE datasets (390,000+ files)
@@ -15,364 +16,446 @@ def task():
     try:
         # Get real data from the working database tracker
         db_data = _get_real_tracker_data()
-        
+
         # ANALYZE USER'S MASSIVE FILE COLLECTION
         user_data = _analyze_massive_user_collection()
-        
+
         # Perform actual analysis on real data
         insights = _perform_real_analysis(db_data, user_data)
-        
+
         # Generate real protocol ideas based on actual patterns
         new_ideas = _generate_real_ideas(insights)
-        
+
         # Create actual optimizations based on real metrics
         optimizations = _create_real_optimizations(insights)
-        
+
         return {
-            'success': True,
-            'action': 'massive_multimodal_analysis',
-            'data_source': 'live_database_tracker + 390k_user_files',
-            'total_files_discovered': user_data['total_files'],
-            'files_sampled_for_analysis': user_data['files_analyzed'],
-            'user_folders_scanned': user_data['folders_scanned'],
-            'insights': insights,
-            'generated_ideas': new_ideas,
-            'optimizations': optimizations,
-            'real_data_points': len(db_data.get('protocols', [])) + user_data['total_files'],
-            'scale': 'massive_dataset_analysis',
-            'timestamp': datetime.utcnow().isoformat()
+            "success": True,
+            "action": "massive_multimodal_analysis",
+            "data_source": "live_database_tracker + 390k_user_files",
+            "total_files_discovered": user_data["total_files"],
+            "files_sampled_for_analysis": user_data["files_analyzed"],
+            "user_folders_scanned": user_data["folders_scanned"],
+            "insights": insights,
+            "generated_ideas": new_ideas,
+            "optimizations": optimizations,
+            "real_data_points": len(db_data.get("protocols", []))
+            + user_data["total_files"],
+            "scale": "massive_dataset_analysis",
+            "timestamp": datetime.utcnow().isoformat(),
         }
-        
+
     except Exception as e:
         return {
-            'success': False,
-            'action': 'massive_multimodal_analysis',
-            'error': str(e),
-            'timestamp': datetime.utcnow().isoformat()
+            "success": False,
+            "action": "massive_multimodal_analysis",
+            "error": str(e),
+            "timestamp": datetime.utcnow().isoformat(),
         }
 
+
 def _analyze_massive_user_collection() -> Dict[str, Any]:
     """Analyze 390,000+ files using statistical sampling and pattern detection"""
     analysis = {
-        'folders_scanned': [],
-        'total_files': 0,
-        'files_analyzed': 0,
-        'file_types': {},
-        'project_insights': [],
-        'code_files': [],
-        'data_files': [],
-        'directory_stats': {},
-        'large_scale_patterns': []
+        "folders_scanned": [],
+        "total_files": 0,
+        "files_analyzed": 0,
+        "file_types": {},
+        "project_insights": [],
+        "code_files": [],
+        "data_files": [],
+        "directory_stats": {},
+        "large_scale_patterns": [],
     }
-    
-    base_paths = ['/data/desktop', '/data/documents', '/data/gptdata']
-    
+
+    base_paths = ["/data/desktop", "/data/documents", "/data/gptdata"]
+
     for base_path in base_paths:
         if os.path.exists(base_path):
             folder_name = os.path.basename(base_path)
-            analysis['folders_scanned'].append(folder_name)
-            
+            analysis["folders_scanned"].append(folder_name)
+
             # Get total file count for this directory
             try:
                 import subprocess
-                result = subprocess.run(['find', base_path, '-type', 'f'], 
-                                      capture_output=True, text=True)
-                all_files = result.stdout.strip().split('\n') if result.stdout.strip() else []
+
+                result = subprocess.run(
+                    ["find", base_path, "-type", "f"], capture_output=True, text=True
+                )
+                all_files = (
+                    result.stdout.strip().split("\n") if result.stdout.strip() else []
+                )
                 folder_file_count = len(all_files)
-                
-                analysis['directory_stats'][folder_name] = {
-                    'total_files': folder_file_count,
-                    'sample_analyzed': 0
+
+                analysis["directory_stats"][folder_name] = {
+                    "total_files": folder_file_count,
+                    "sample_analyzed": 0,
                 }
-                analysis['total_files'] += folder_file_count
-                
+                analysis["total_files"] += folder_file_count
+
                 # Use statistical sampling for massive datasets
                 if folder_file_count > 1000:
                     # Sample 5% or max 2000 files, whichever is smaller
                     sample_size = min(int(folder_file_count * 0.05), 2000)
                     sampled_files = random.sample(all_files, sample_size)
-                    analysis['directory_stats'][folder_name]['sample_analyzed'] = sample_size
+                    analysis["directory_stats"][folder_name][
+                        "sample_analyzed"
+                    ] = sample_size
                 else:
                     # Analyze all files if small dataset
                     sampled_files = all_files
-                    analysis['directory_stats'][folder_name]['sample_analyzed'] = folder_file_count
-                
+                    analysis["directory_stats"][folder_name][
+                        "sample_analyzed"
+                    ] = folder_file_count
+
                 # Analyze sampled files
                 for file_path in sampled_files:
                     if not os.path.exists(file_path):
                         continue
-                        
+
                     file = os.path.basename(file_path)
-                    if file.startswith('.'):
+                    if file.startswith("."):
                         continue
-                        
-                    analysis['files_analyzed'] += 1
-                    
+
+                    analysis["files_analyzed"] += 1
+
                     # Analyze file type
                     ext = os.path.splitext(file)[1].lower()
-                    if ext in analysis['file_types']:
-                        analysis['file_types'][ext] += 1
+                    if ext in analysis["file_types"]:
+                        analysis["file_types"][ext] += 1
                     else:
-                        analysis['file_types'][ext] = 1
-                    
+                        analysis["file_types"][ext] = 1
+
                     # Identify code files
-                    if ext in ['.js', '.py', '.html', '.css', '.json', '.md', '.txt', '.ts']:
-                        analysis['code_files'].append({
-                            'file': file,
-                            'path': os.path.relpath(file_path, base_path),
-                            'type': ext,
-                            'size': os.path.getsize(file_path) if os.path.exists(file_path) else 0
-                        })
-                    
+                    if ext in [
+                        ".js",
+                        ".py",
+                        ".html",
+                        ".css",
+                        ".json",
+                        ".md",
+                        ".txt",
+                        ".ts",
+                    ]:
+                        analysis["code_files"].append(
+                            {
+                                "file": file,
+                                "path": os.path.relpath(file_path, base_path),
+                                "type": ext,
+                                "size": (
+                                    os.path.getsize(file_path)
+                                    if os.path.exists(file_path)
+                                    else 0
+                                ),
+                            }
+                        )
+
                     # Identify data files
-                    elif ext in ['.csv', '.json', '.xml', '.sql', '.db']:
-                        analysis['data_files'].append({
-                            'file': file,
-                            'path': os.path.relpath(file_path, base_path),
-                            'type': ext
-                        })
-                
+                    elif ext in [".csv", ".json", ".xml", ".sql", ".db"]:
+                        analysis["data_files"].append(
+                            {
+                                "file": file,
+                                "path": os.path.relpath(file_path, base_path),
+                                "type": ext,
+                            }
+                        )
+
             except Exception as e:
-                analysis['scan_errors'] = analysis.get('scan_errors', [])
-                analysis['scan_errors'].append(f"Error scanning {base_path}: {str(e)}")
-    
+                analysis["scan_errors"] = analysis.get("scan_errors", [])
+                analysis["scan_errors"].append(f"Error scanning {base_path}: {str(e)}")
+
     # Generate large-scale insights
-    analysis['large_scale_patterns'] = _detect_large_scale_patterns(analysis)
-    
+    analysis["large_scale_patterns"] = _detect_large_scale_patterns(analysis)
+
     # Generate specific project insights
-    if analysis['code_files']:
-        js_files = len([f for f in analysis['code_files'] if f['type'] == '.js'])
-        py_files = len([f for f in analysis['code_files'] if f['type'] == '.py'])
-        ts_files = len([f for f in analysis['code_files'] if f['type'] == '.ts'])
-        
+    if analysis["code_files"]:
+        js_files = len([f for f in analysis["code_files"] if f["type"] == ".js"])
+        py_files = len([f for f in analysis["code_files"] if f["type"] == ".py"])
+        ts_files = len([f for f in analysis["code_files"] if f["type"] == ".ts"])
+
         if js_files > 10:
-            analysis['project_insights'].append(f"MASSIVE JavaScript development detected ({js_files} JS files in sample)")
+            analysis["project_insights"].append(
+                f"MASSIVE JavaScript development detected ({js_files} JS files in sample)"
+            )
         if py_files > 5:
-            analysis['project_insights'].append(f"Extensive Python project work ({py_files} Python files)")
+            analysis["project_insights"].append(
+                f"Extensive Python project work ({py_files} Python files)"
+            )
         if ts_files > 10:
-            analysis['project_insights'].append(f"Large TypeScript codebase detected ({ts_files} TS files)")
-        
+            analysis["project_insights"].append(
+                f"Large TypeScript codebase detected ({ts_files} TS files)"
+            )
+
         # Look for specific patterns
-        safari_files = [f for f in analysis['code_files'] if 'safari' in f['path'].lower()]
-        mcp_files = [f for f in analysis['code_files'] if 'mcp' in f['file'].lower()]
-        
+        safari_files = [
+            f for f in analysis["code_files"] if "safari" in f["path"].lower()
+        ]
+        mcp_files = [f for f in analysis["code_files"] if "mcp" in f["file"].lower()]
+
         if safari_files:
-            analysis['project_insights'].append(f"Safari extension development detected ({len(safari_files)} related files)")
+            analysis["project_insights"].append(
+                f"Safari extension development detected ({len(safari_files)} related files)"
+            )
         if mcp_files:
-            analysis['project_insights'].append(f"MCP protocol development active ({len(mcp_files)} MCP files)")
-    
+            analysis["project_insights"].append(
+                f"MCP protocol development active ({len(mcp_files)} MCP files)"
+            )
+
     return analysis
 
+
 def _detect_large_scale_patterns(analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
     """Detect patterns in massive file collections"""
     patterns = []
-    
-    file_types = analysis.get('file_types', {})
-    total_analyzed = analysis.get('files_analyzed', 0)
-    
+
+    file_types = analysis.get("file_types", {})
+    total_analyzed = analysis.get("files_analyzed", 0)
+
     if total_analyzed > 0:
         # Code vs content ratio
-        code_extensions = ['.js', '.py', '.html', '.css', '.json', '.md', '.txt', '.ts']
+        code_extensions = [".js", ".py", ".html", ".css", ".json", ".md", ".txt", ".ts"]
         code_files = sum(file_types.get(ext, 0) for ext in code_extensions)
         code_ratio = code_files / total_analyzed
-        
+
         if code_ratio > 0.3:
-            patterns.append({
-                'pattern': 'heavy_development_environment',
-                'description': f'{code_ratio:.1%} of files are code/development related',
-                'file_count': code_files,
-                'significance': 'high'
-            })
-        
+            patterns.append(
+                {
+                    "pattern": "heavy_development_environment",
+                    "description": f"{code_ratio:.1%} of files are code/development related",
+                    "file_count": code_files,
+                    "significance": "high",
+                }
+            )
+
         # Media content detection
-        media_extensions = ['.jpg', '.png', '.gif', '.mp4', '.mov', '.pdf']
+        media_extensions = [".jpg", ".png", ".gif", ".mp4", ".mov", ".pdf"]
         media_files = sum(file_types.get(ext, 0) for ext in media_extensions)
         if media_files > total_analyzed * 0.2:
-            patterns.append({
-                'pattern': 'rich_media_collection',
-                'description': f'{media_files} media files detected',
-                'significance': 'medium'
-            })
-        
+            patterns.append(
+                {
+                    "pattern": "rich_media_collection",
+                    "description": f"{media_files} media files detected",
+                    "significance": "medium",
+                }
+            )
+
         # Archive detection
-        archive_extensions = ['.zip', '.tar', '.gz', '.rar']
+        archive_extensions = [".zip", ".tar", ".gz", ".rar"]
         archive_files = sum(file_types.get(ext, 0) for ext in archive_extensions)
         if archive_files > 50:
-            patterns.append({
-                'pattern': 'extensive_archival_system',
-                'description': f'{archive_files} archive files found',
-                'significance': 'medium'
-            })
-    
+            patterns.append(
+                {
+                    "pattern": "extensive_archival_system",
+                    "description": f"{archive_files} archive files found",
+                    "significance": "medium",
+                }
+            )
+
     return patterns
 
+
 def _get_real_tracker_data() -> Dict[str, Any]:
     """Get real execution data from the working database tracker"""
     try:
         from utils.db_tracker import get_all_stats
-        
+
         # Get actual protocol execution statistics
         stats = get_all_stats()
-        
+
         return {
-            'protocols': stats,
-            'total_protocols': len(stats),
-            'total_executions': sum(p['total_executions'] for p in stats),
-            'total_successes': sum(p['successes'] for p in stats),
-            'total_failures': sum(p['failures'] for p in stats)
+            "protocols": stats,
+            "total_protocols": len(stats),
+            "total_executions": sum(p["total_executions"] for p in stats),
+            "total_successes": sum(p["successes"] for p in stats),
+            "total_failures": sum(p["failures"] for p in stats),
         }
-        
+
     except Exception as e:
         print(f"Database tracker failed: {e}")
         return {
-            'protocols': [],
-            'total_protocols': 0,
-            'total_executions': 0,
-            'total_successes': 0,
-            'total_failures': 0,
-            'error': str(e)
+            "protocols": [],
+            "total_protocols": 0,
+            "total_executions": 0,
+            "total_successes": 0,
+            "total_failures": 0,
+            "error": str(e),
         }
 
-def _perform_real_analysis(db_data: Dict[str, Any], user_data: Dict[str, Any]) -> Dict[str, Any]:
+
+def _perform_real_analysis(
+    db_data: Dict[str, Any], user_data: Dict[str, Any]
+) -> Dict[str, Any]:
     """Perform real statistical analysis on massive datasets"""
-    protocols = db_data.get('protocols', [])
-    
+    protocols = db_data.get("protocols", [])
+
     analysis = {
-        'database_patterns': [],
-        'massive_file_patterns': [],
-        'scale_insights': [],
-        'combined_insights': []
+        "database_patterns": [],
+        "massive_file_patterns": [],
+        "scale_insights": [],
+        "combined_insights": [],
     }
-    
+
     # Analyze database patterns
     if protocols:
         high_performers = []
         low_performers = []
-        
+
         for protocol in protocols:
-            success_rate = protocol['success_rate']
+            success_rate = protocol["success_rate"]
             if success_rate >= 0.8:
                 high_performers.append(protocol)
             elif success_rate < 0.5:
                 low_performers.append(protocol)
-        
+
         if high_performers:
-            analysis['database_patterns'].append({
-                'type': 'successful_protocols',
-                'count': len(high_performers),
-                'protocols': [p['protocol'] for p in high_performers]
-            })
-        
+            analysis["database_patterns"].append(
+                {
+                    "type": "successful_protocols",
+                    "count": len(high_performers),
+                    "protocols": [p["protocol"] for p in high_performers],
+                }
+            )
+
         if low_performers:
-            analysis['database_patterns'].append({
-                'type': 'failing_protocols',
-                'count': len(low_performers),
-                'protocols': [p['protocol'] for p in low_performers],
-                'total_failures': sum(p['failures'] for p in low_performers)
-            })
-    
+            analysis["database_patterns"].append(
+                {
+                    "type": "failing_protocols",
+                    "count": len(low_performers),
+                    "protocols": [p["protocol"] for p in low_performers],
+                    "total_failures": sum(p["failures"] for p in low_performers),
+                }
+            )
+
     # Analyze massive file patterns
-    total_files = user_data.get('total_files', 0)
-    files_analyzed = user_data.get('files_analyzed', 0)
-    
-    analysis['scale_insights'].append({
-        'total_files_discovered': total_files,
-        'files_analyzed': files_analyzed,
-        'sampling_ratio': f'{files_analyzed/total_files:.1%}' if total_files > 0 else '0%',
-        'scale_category': 'massive' if total_files > 100000 else 'large' if total_files > 10000 else 'medium'
-    })
-    
-    if user_data['code_files']:
-        analysis['massive_file_patterns'].append({
-            'type': 'development_ecosystem',
-            'total_code_files_found': len(user_data['code_files']),
-            'estimated_total_code_files': int(len(user_data['code_files']) * (total_files / files_analyzed)) if files_analyzed > 0 else 0,
-            'file_types': user_data['file_types'],
-            'insights': user_data['project_insights'],
-            'large_scale_patterns': user_data.get('large_scale_patterns', [])
-        })
-    
+    total_files = user_data.get("total_files", 0)
+    files_analyzed = user_data.get("files_analyzed", 0)
+
+    analysis["scale_insights"].append(
+        {
+            "total_files_discovered": total_files,
+            "files_analyzed": files_analyzed,
+            "sampling_ratio": (
+                f"{files_analyzed/total_files:.1%}" if total_files > 0 else "0%"
+            ),
+            "scale_category": (
+                "massive"
+                if total_files > 100000
+                else "large" if total_files > 10000 else "medium"
+            ),
+        }
+    )
+
+    if user_data["code_files"]:
+        analysis["massive_file_patterns"].append(
+            {
+                "type": "development_ecosystem",
+                "total_code_files_found": len(user_data["code_files"]),
+                "estimated_total_code_files": (
+                    int(len(user_data["code_files"]) * (total_files / files_analyzed))
+                    if files_analyzed > 0
+                    else 0
+                ),
+                "file_types": user_data["file_types"],
+                "insights": user_data["project_insights"],
+                "large_scale_patterns": user_data.get("large_scale_patterns", []),
+            }
+        )
+
     # Combined massive scale analysis
-    if user_data['project_insights'] and protocols:
-        analysis['combined_insights'].append({
-            'insight': f'Massive development environment detected with {total_files:,} files and active execution system',
-            'recommendation': 'Create large-scale automation protocols for this extensive development ecosystem',
-            'development_focus': user_data['project_insights'],
-            'system_performance': f"{db_data.get('total_successes', 0)}/{db_data.get('total_executions', 0)} executions successful",
-            'scale_impact': f'Potential to automate workflows across {total_files:,} files'
-        })
-    
+    if user_data["project_insights"] and protocols:
+        analysis["combined_insights"].append(
+            {
+                "insight": f"Massive development environment detected with {total_files:,} files and active execution system",
+                "recommendation": "Create large-scale automation protocols for this extensive development ecosystem",
+                "development_focus": user_data["project_insights"],
+                "system_performance": f"{db_data.get('total_successes', 0)}/{db_data.get('total_executions', 0)} executions successful",
+                "scale_impact": f"Potential to automate workflows across {total_files:,} files",
+            }
+        )
+
     return analysis
 
+
 def _generate_real_ideas(insights: Dict[str, Any]) -> List[Dict[str, str]]:
     """Generate protocol ideas for massive scale development environments"""
     ideas = []
-    
+
     # Ideas based on massive file analysis
-    massive_patterns = insights.get('massive_file_patterns', [])
+    massive_patterns = insights.get("massive_file_patterns", [])
     for pattern in massive_patterns:
-        if 'development_ecosystem' in pattern.get('type', ''):
-            estimated_code_files = pattern.get('estimated_total_code_files', 0)
-            
+        if "development_ecosystem" in pattern.get("type", ""):
+            estimated_code_files = pattern.get("estimated_total_code_files", 0)
+
             if estimated_code_files > 1000:
-                ideas.append({
-                    'name': 'massive_codebase_optimizer',
-                    'description': f'Automatically analyze and optimize estimated {estimated_code_files:,} code files',
-                    'real_basis': f'Based on statistical analysis of massive file collection',
-                    'implementation': 'Create distributed protocol system for large-scale code analysis'
-                })
-            
-            large_scale_patterns = pattern.get('large_scale_patterns', [])
+                ideas.append(
+                    {
+                        "name": "massive_codebase_optimizer",
+                        "description": f"Automatically analyze and optimize estimated {estimated_code_files:,} code files",
+                        "real_basis": f"Based on statistical analysis of massive file collection",
+                        "implementation": "Create distributed protocol system for large-scale code analysis",
+                    }
+                )
+
+            large_scale_patterns = pattern.get("large_scale_patterns", [])
             for ls_pattern in large_scale_patterns:
-                if ls_pattern.get('pattern') == 'heavy_development_environment':
-                    ideas.append({
-                        'name': 'development_environment_automator',
-                        'description': f'Automate development workflows across massive codebase',
-                        'real_basis': f'Development environment with {ls_pattern.get("file_count")} code files',
-                        'implementation': 'Build automation protocols for build, test, deploy across large codebases'
-                    })
-    
+                if ls_pattern.get("pattern") == "heavy_development_environment":
+                    ideas.append(
+                        {
+                            "name": "development_environment_automator",
+                            "description": f"Automate development workflows across massive codebase",
+                            "real_basis": f'Development environment with {ls_pattern.get("file_count")} code files',
+                            "implementation": "Build automation protocols for build, test, deploy across large codebases",
+                        }
+                    )
+
     # Scale-specific ideas
-    scale_insights = insights.get('scale_insights', [])
+    scale_insights = insights.get("scale_insights", [])
     for scale_insight in scale_insights:
-        if scale_insight.get('scale_category') == 'massive':
-            ideas.append({
-                'name': 'massive_file_organizer',
-                'description': f'Organize and index {scale_insight.get("total_files_discovered", 0):,} files intelligently',
-                'real_basis': f'Based on discovery of {scale_insight.get("total_files_discovered", 0):,} total files',
-                'implementation': 'Create intelligent file organization and search protocols'
-            })
-    
+        if scale_insight.get("scale_category") == "massive":
+            ideas.append(
+                {
+                    "name": "massive_file_organizer",
+                    "description": f'Organize and index {scale_insight.get("total_files_discovered", 0):,} files intelligently',
+                    "real_basis": f'Based on discovery of {scale_insight.get("total_files_discovered", 0):,} total files',
+                    "implementation": "Create intelligent file organization and search protocols",
+                }
+            )
+
     return ideas
 
+
 def _create_real_optimizations(insights: Dict[str, Any]) -> List[Dict[str, Any]]:
     """Create optimizations for massive scale systems"""
     optimizations = []
-    
+
     # Database-based optimizations
-    db_patterns = insights.get('database_patterns', [])
+    db_patterns = insights.get("database_patterns", [])
     for pattern in db_patterns:
-        if pattern.get('type') == 'failing_protocols':
-            optimizations.append({
-                'target': 'protocol_reliability',
-                'failing_protocols': pattern['protocols'],
-                'total_failures': pattern.get('total_failures', 0),
-                'action': f'Fix {pattern["count"]} failing protocols',
-                'priority': 'high'
-            })
-    
+        if pattern.get("type") == "failing_protocols":
+            optimizations.append(
+                {
+                    "target": "protocol_reliability",
+                    "failing_protocols": pattern["protocols"],
+                    "total_failures": pattern.get("total_failures", 0),
+                    "action": f'Fix {pattern["count"]} failing protocols',
+                    "priority": "high",
+                }
+            )
+
     # Massive scale optimizations
-    scale_insights = insights.get('scale_insights', [])
+    scale_insights = insights.get("scale_insights", [])
     for scale_insight in scale_insights:
-        total_files = scale_insight.get('total_files_discovered', 0)
+        total_files = scale_insight.get("total_files_discovered", 0)
         if total_files > 100000:
-            optimizations.append({
-                'target': 'massive_scale_file_management',
-                'file_count': total_files,
-                'action': f'Implement distributed file analysis for {total_files:,} files',
-                'expected_benefit': 'Efficient processing of massive file collections',
-                'priority': 'high',
-                'scale': 'massive'
-            })
-    
-    return optimizations 
\ No newline at end of file
+            optimizations.append(
+                {
+                    "target": "massive_scale_file_management",
+                    "file_count": total_files,
+                    "action": f"Implement distributed file analysis for {total_files:,} files",
+                    "expected_benefit": "Efficient processing of massive file collections",
+                    "priority": "high",
+                    "scale": "massive",
+                }
+            )
+
+    return optimizations
diff --git a/protocols/nonexistent.py b/protocols/nonexistent.py
index ddbcce9..48bd9b0 100644
--- a/protocols/nonexistent.py
+++ b/protocols/nonexistent.py
@@ -2,14 +2,15 @@
 # Previous failure rate: 100.00%
 # Mutation timestamp: {"timestamp": "2025-06-17T22:57:30.605986"}
 
+
 def task():
     """Improved task implementation after mutation"""
     from random import choice, random
-    
+
     # Enhanced logic with better success probability
     if random() < 0.8:  # 80% success rate instead of 50%
-        result = {'success': True, 'action': 'optimized_execution', 'mutation_gen': 1}
+        result = {"success": True, "action": "optimized_execution", "mutation_gen": 1}
     else:
-        result = {'success': False, 'action': 'controlled_failure', 'mutation_gen': 1}
-    
+        result = {"success": False, "action": "controlled_failure", "mutation_gen": 1}
+
     return result
diff --git a/protocols/redis_cache_manager.py b/protocols/redis_cache_manager.py
index e9bb4db..b0c3b1c 100644
--- a/protocols/redis_cache_manager.py
+++ b/protocols/redis_cache_manager.py
@@ -5,110 +5,114 @@ import time
 import os
 from datetime import datetime
 
+
 def task():
     """Manage Redis cache and demonstrate caching benefits"""
     try:
         # Connect to Redis
         r = redis.Redis(
-            host=os.environ.get('REDIS_HOST', 'mcp_redis'),
-            port=int(os.environ.get('REDIS_PORT', '6379')),
-            decode_responses=True
+            host=os.environ.get("REDIS_HOST", "mcp_redis"),
+            port=int(os.environ.get("REDIS_PORT", "6379")),
+            decode_responses=True,
         )
-        
+
         # Test Redis connectivity
         r.ping()
-        
+
         # Demonstrate cache usage
-        cache_key = 'expensive_computation'
-        cache_stats_key = 'cache_stats'
-        
+        cache_key = "expensive_computation"
+        cache_stats_key = "cache_stats"
+
         # Check if we have cached result
         cached_result = r.get(cache_key)
-        
+
         if cached_result:
             # Cache hit
             computation_time = 0
             result = json.loads(cached_result)
-            cache_status = 'hit'
-            
+            cache_status = "hit"
+
             # Update cache statistics
-            r.hincrby(cache_stats_key, 'hits', 1)
+            r.hincrby(cache_stats_key, "hits", 1)
         else:
             # Cache miss - simulate expensive computation
             start_time = time.time()
-            
+
             # Simulate expensive operation
             import hashlib
+
             result = {
-                'computed_value': hashlib.sha256(str(time.time()).encode()).hexdigest(),
-                'computation_timestamp': datetime.utcnow().isoformat()
+                "computed_value": hashlib.sha256(str(time.time()).encode()).hexdigest(),
+                "computation_timestamp": datetime.utcnow().isoformat(),
             }
             time.sleep(0.5)  # Simulate processing time
-            
+
             computation_time = time.time() - start_time
-            cache_status = 'miss'
-            
+            cache_status = "miss"
+
             # Store in cache with 5 minute expiration
             r.setex(cache_key, 300, json.dumps(result))
-            
+
             # Update cache statistics
-            r.hincrby(cache_stats_key, 'misses', 1)
-        
+            r.hincrby(cache_stats_key, "misses", 1)
+
         # Get cache statistics
-        stats = r.hgetall(cache_stats_key) or {'hits': '0', 'misses': '0'}
-        hits = int(stats.get('hits', 0))
-        misses = int(stats.get('misses', 0))
+        stats = r.hgetall(cache_stats_key) or {"hits": "0", "misses": "0"}
+        hits = int(stats.get("hits", 0))
+        misses = int(stats.get("misses", 0))
         total_requests = hits + misses
         hit_rate = (hits / total_requests * 100) if total_requests > 0 else 0
-        
+
         # Store protocol execution metrics in Redis
-        execution_key = f'protocol:execution:{int(time.time())}'
-        r.hset(execution_key, mapping={
-            'protocol': 'redis_cache_manager',
-            'status': cache_status,
-            'computation_time': computation_time,
-            'timestamp': datetime.utcnow().isoformat()
-        })
+        execution_key = f"protocol:execution:{int(time.time())}"
+        r.hset(
+            execution_key,
+            mapping={
+                "protocol": "redis_cache_manager",
+                "status": cache_status,
+                "computation_time": computation_time,
+                "timestamp": datetime.utcnow().isoformat(),
+            },
+        )
         r.expire(execution_key, 3600)  # Keep for 1 hour
-        
+
         # Get Redis info
-        info = r.info('memory')
-        memory_used = info.get('used_memory_human', 'unknown')
-        
+        info = r.info("memory")
+        memory_used = info.get("used_memory_human", "unknown")
+
         # List all protocol execution keys
         recent_executions = []
-        for key in r.scan_iter(match='protocol:execution:*', count=10):
+        for key in r.scan_iter(match="protocol:execution:*", count=10):
             exec_data = r.hgetall(key)
             if exec_data:
                 recent_executions.append(exec_data)
-        
+
         return {
-            'success': True,
-            'action': 'redis_cache_management',
-            'cache_status': cache_status,
-            'computation_time_seconds': round(computation_time, 3),
-            'cached_result': result,
-            'cache_statistics': {
-                'hits': hits,
-                'misses': misses,
-                'total_requests': total_requests,
-                'hit_rate_percent': round(hit_rate, 2)
+            "success": True,
+            "action": "redis_cache_management",
+            "cache_status": cache_status,
+            "computation_time_seconds": round(computation_time, 3),
+            "cached_result": result,
+            "cache_statistics": {
+                "hits": hits,
+                "misses": misses,
+                "total_requests": total_requests,
+                "hit_rate_percent": round(hit_rate, 2),
             },
-            'redis_info': {
-                'memory_used': memory_used,
-                'connected': True
+            "redis_info": {"memory_used": memory_used, "connected": True},
+            "recent_executions": recent_executions[-5:],  # Last 5
+            "benefits": {
+                "time_saved": f"{0.5 * hits:.1f} seconds saved from cache hits",
+                "efficiency": (
+                    "High" if hit_rate > 70 else "Medium" if hit_rate > 30 else "Low"
+                ),
             },
-            'recent_executions': recent_executions[-5:],  # Last 5
-            'benefits': {
-                'time_saved': f"{0.5 * hits:.1f} seconds saved from cache hits",
-                'efficiency': 'High' if hit_rate > 70 else 'Medium' if hit_rate > 30 else 'Low'
-            }
         }
-        
+
     except Exception as e:
         return {
-            'success': False,
-            'action': 'redis_cache_management',
-            'error': str(e),
-            'note': 'Redis might not be accessible from container'
+            "success": False,
+            "action": "redis_cache_management",
+            "error": str(e),
+            "note": "Redis might not be accessible from container",
         }
diff --git a/protocols/system_monitor.py b/protocols/system_monitor.py
index 2f1617b..d21b35a 100644
--- a/protocols/system_monitor.py
+++ b/protocols/system_monitor.py
@@ -3,36 +3,33 @@ import psutil
 import platform
 import os
 
+
 def task():
     """Monitor system resources and health"""
     try:
         # Get system info
         cpu_percent = psutil.cpu_percent(interval=1)
         memory = psutil.virtual_memory()
-        disk = psutil.disk_usage('/')
-        
+        disk = psutil.disk_usage("/")
+
         # Get process count
         process_count = len(psutil.pids())
-        
+
         # Check if system is healthy
-        is_healthy = (
-            cpu_percent < 90 and 
-            memory.percent < 90 and 
-            disk.percent < 95
-        )
-        
+        is_healthy = cpu_percent < 90 and memory.percent < 90 and disk.percent < 95
+
         system_info = {
-            'platform': platform.system(),
-            'platform_version': platform.version()[:50],  # Truncate long versions
-            'cpu_cores': psutil.cpu_count(),
-            'cpu_percent': cpu_percent,
-            'memory_total_gb': round(memory.total / (1024**3), 2),
-            'memory_used_percent': memory.percent,
-            'disk_total_gb': round(disk.total / (1024**3), 2),
-            'disk_used_percent': disk.percent,
-            'process_count': process_count
+            "platform": platform.system(),
+            "platform_version": platform.version()[:50],  # Truncate long versions
+            "cpu_cores": psutil.cpu_count(),
+            "cpu_percent": cpu_percent,
+            "memory_total_gb": round(memory.total / (1024**3), 2),
+            "memory_used_percent": memory.percent,
+            "disk_total_gb": round(disk.total / (1024**3), 2),
+            "disk_used_percent": disk.percent,
+            "process_count": process_count,
         }
-        
+
         # Determine if intervention needed
         warnings = []
         if cpu_percent > 80:
@@ -41,18 +38,14 @@ def task():
             warnings.append(f"High memory usage: {memory.percent}%")
         if disk.percent > 90:
             warnings.append(f"Low disk space: {disk.percent}% used")
-            
+
         return {
-            'success': is_healthy,
-            'action': 'system_monitoring',
-            'healthy': is_healthy,
-            'system_info': system_info,
-            'warnings': warnings
+            "success": is_healthy,
+            "action": "system_monitoring",
+            "healthy": is_healthy,
+            "system_info": system_info,
+            "warnings": warnings,
         }
-        
+
     except Exception as e:
-        return {
-            'success': False,
-            'action': 'system_monitoring',
-            'error': str(e)
-        } 
\ No newline at end of file
+        return {"success": False, "action": "system_monitoring", "error": str(e)}
diff --git a/protocols/test_protocol.py b/protocols/test_protocol.py
index 0d2962b..526cb7e 100644
--- a/protocols/test_protocol.py
+++ b/protocols/test_protocol.py
@@ -1,8 +1,10 @@
 # Default protocol: test_protocol
 # Auto-generated by protocol loader
 
+
 def task():
     """Default task implementation"""
     from random import randint
-    result = {'success': bool(randint(0, 1)), 'action': 'default_execution'}
+
+    result = {"success": bool(randint(0, 1)), "action": "default_execution"}
     return result
diff --git a/protocols/user_data_processor.py b/protocols/user_data_processor.py
index ea3e429..6963570 100644
--- a/protocols/user_data_processor.py
+++ b/protocols/user_data_processor.py
@@ -4,92 +4,99 @@ import json
 import glob
 from datetime import datetime
 
+
 def task():
     """Process and analyze user data from mounted directories"""
     # Check which directories are available
     available_dirs = []
     data_stats = {
-        'desktop': {'files': 0, 'size': 0},
-        'documents': {'files': 0, 'size': 0},
-        'gptdata': {'files': 0, 'size': 0}
+        "desktop": {"files": 0, "size": 0},
+        "documents": {"files": 0, "size": 0},
+        "gptdata": {"files": 0, "size": 0},
     }
-    
+
     # Map of mount points
     mount_points = {
-        'desktop': '/data/desktop',
-        'documents': '/data/documents', 
-        'gptdata': '/data/gptdata'
+        "desktop": "/data/desktop",
+        "documents": "/data/documents",
+        "gptdata": "/data/gptdata",
     }
-    
+
     insights = []
     processed_files = []
-    
+
     try:
         # Check each mounted directory
         for name, path in mount_points.items():
             if os.path.exists(path) and os.access(path, os.R_OK):
                 available_dirs.append(name)
-                
+
                 # Count files and calculate size
                 for root, dirs, files in os.walk(path):
                     # Limit depth to avoid scanning too deep
-                    depth = root.replace(path, '').count(os.sep)
+                    depth = root.replace(path, "").count(os.sep)
                     if depth > 2:  # Only go 2 levels deep
                         dirs[:] = []  # Don't recurse further
                         continue
-                        
+
                     for file in files[:10]:  # Limit files per directory
                         file_path = os.path.join(root, file)
                         try:
                             size = os.path.getsize(file_path)
-                            data_stats[name]['files'] += 1
-                            data_stats[name]['size'] += size
-                            
+                            data_stats[name]["files"] += 1
+                            data_stats[name]["size"] += size
+
                             # Process specific file types
-                            if file.endswith('.json'):
-                                processed_files.append({
-                                    'path': file_path.replace(path, f'{name}/'),
-                                    'type': 'json',
-                                    'size': size
-                                })
-                            elif file.endswith('.txt'):
-                                processed_files.append({
-                                    'path': file_path.replace(path, f'{name}/'),
-                                    'type': 'text',
-                                    'size': size
-                                })
+                            if file.endswith(".json"):
+                                processed_files.append(
+                                    {
+                                        "path": file_path.replace(path, f"{name}/"),
+                                        "type": "json",
+                                        "size": size,
+                                    }
+                                )
+                            elif file.endswith(".txt"):
+                                processed_files.append(
+                                    {
+                                        "path": file_path.replace(path, f"{name}/"),
+                                        "type": "text",
+                                        "size": size,
+                                    }
+                                )
                         except:
                             pass
-        
+
         # Generate insights
-        total_files = sum(stats['files'] for stats in data_stats.values())
-        total_size = sum(stats['size'] for stats in data_stats.values())
-        
+        total_files = sum(stats["files"] for stats in data_stats.values())
+        total_size = sum(stats["size"] for stats in data_stats.values())
+
         if available_dirs:
             insights.append(f"Found {len(available_dirs)} accessible directories")
             insights.append(f"Total files scanned: {total_files}")
             insights.append(f"Total size: {total_size / (1024**2):.2f} MB")
-            
+
             # Find largest directory
-            largest = max(data_stats.items(), key=lambda x: x[1]['size'])
-            insights.append(f"Largest data source: {largest[0]} ({largest[1]['size'] / (1024**2):.2f} MB)")
-        
+            largest = max(data_stats.items(), key=lambda x: x[1]["size"])
+            insights.append(
+                f"Largest data source: {largest[0]} ({largest[1]['size'] / (1024**2):.2f} MB)"
+            )
+
         success = len(available_dirs) > 0
-        
+
         return {
-            'success': success,
-            'action': 'user_data_processing',
-            'available_directories': available_dirs,
-            'data_statistics': data_stats,
-            'processed_files': processed_files[:10],  # Sample of processed files
-            'insights': insights,
-            'timestamp': datetime.utcnow().isoformat()
+            "success": success,
+            "action": "user_data_processing",
+            "available_directories": available_dirs,
+            "data_statistics": data_stats,
+            "processed_files": processed_files[:10],  # Sample of processed files
+            "insights": insights,
+            "timestamp": datetime.utcnow().isoformat(),
         }
-        
+
     except Exception as e:
         return {
-            'success': False,
-            'action': 'user_data_processing',
-            'error': str(e),
-            'available_directories': available_dirs
-        } 
\ No newline at end of file
+            "success": False,
+            "action": "user_data_processing",
+            "error": str(e),
+            "available_directories": available_dirs,
+        }
diff --git a/test_mcp_debug_quantum.py b/test_mcp_debug_quantum.py
index 2f608a6..617e725 100644
--- a/test_mcp_debug_quantum.py
+++ b/test_mcp_debug_quantum.py
@@ -19,21 +19,24 @@ from connectors.mcp_debug_tool import MCPDebugTool, MCPDebugContext, DebugRespon
 from quantum_mcp_server.quantum_mcp import QuantumMCPServer
 
 # Configure logging
-logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
+)
 logger = logging.getLogger(__name__)
 
+
 class QuantumDebugTestSuite:
     """Comprehensive test suite for quantum debugging capabilities"""
-    
+
     def __init__(self):
         self.test_results = {}
         self.total_tests = 0
         self.passed_tests = 0
-        
+
     async def run_all_tests(self):
         """Run all quantum debug test cases"""
         logger.info("🚀 Starting MCP Debug Tool & Quantum Agent Test Suite")
-        
+
         test_cases = [
             ("Basic Debug Tool Initialization", self.test_debug_tool_init),
             ("Quantum Code Analysis", self.test_quantum_code_analysis),
@@ -46,20 +49,20 @@ class QuantumDebugTestSuite:
             ("Quantum Teleportation Debug", self.test_quantum_teleportation_debug),
             ("Fallback Reasoning", self.test_fallback_reasoning),
             ("MCP Context Creation", self.test_mcp_context_creation),
-            ("Quantum MCP Server Integration", self.test_quantum_mcp_integration)
+            ("Quantum MCP Server Integration", self.test_quantum_mcp_integration),
         ]
-        
+
         for test_name, test_func in test_cases:
             await self.run_test(test_name, test_func)
-        
+
         self.print_summary()
         return self.passed_tests == self.total_tests
-    
+
     async def run_test(self, test_name: str, test_func):
         """Run individual test with error handling"""
         self.total_tests += 1
         logger.info(f"🧪 Running: {test_name}")
-        
+
         try:
             result = await test_func()
             if result:
@@ -73,15 +76,17 @@ class QuantumDebugTestSuite:
             self.test_results[test_name] = f"❌ ERROR: {str(e)}"
             logger.error(f"❌ {test_name}: ERROR - {str(e)}")
             logger.error(traceback.format_exc())
-    
+
     async def test_debug_tool_init(self) -> bool:
         """Test MCP Debug Tool initialization"""
         try:
             async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
-                return debug_tool is not None and hasattr(debug_tool, 'quantum_analyzers')
+                return debug_tool is not None and hasattr(
+                    debug_tool, "quantum_analyzers"
+                )
         except Exception:
             return False
-    
+
     async def test_quantum_code_analysis(self) -> bool:
         """Test quantum code structure analysis"""
         quantum_code = """
@@ -99,17 +104,23 @@ class QuantumDebugTestSuite:
         backend = Aer.get_backend('qasm_simulator')
         result = execute(circuit, backend, shots=1024).result()
         """
-        
+
         async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
             analysis = await debug_tool._analyze_code_structure(quantum_code)
-            
-            required_keys = ['complexity', 'patterns', 'imports', 'functions', 'quantum_elements']
+
+            required_keys = [
+                "complexity",
+                "patterns",
+                "imports",
+                "functions",
+                "quantum_elements",
+            ]
             has_required_keys = all(key in analysis for key in required_keys)
-            has_quantum_elements = len(analysis['quantum_elements']) > 0
-            has_quantum_pattern = 'quantum_computing' in analysis['patterns']
-            
+            has_quantum_elements = len(analysis["quantum_elements"]) > 0
+            has_quantum_pattern = "quantum_computing" in analysis["patterns"]
+
             return has_required_keys and has_quantum_elements and has_quantum_pattern
-    
+
     async def test_qubit_state_debugging(self) -> bool:
         """Test qubit state analysis capabilities"""
         problematic_quantum_code = """
@@ -118,16 +129,16 @@ class QuantumDebugTestSuite:
         qc.measure(0, 0)  # Premature measurement
         qc.cx(0, 1)  # Operation after measurement
         """
-        
+
         async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
             result = await debug_tool._analyze_qubit_state(problematic_quantum_code, {})
-            
-            has_operations = len(result['operations']) > 0
-            has_issues = len(result['issues']) > 0
-            needs_review = result['state_quality'] == 'needs_review'
-            
+
+            has_operations = len(result["operations"]) > 0
+            has_issues = len(result["issues"]) > 0
+            needs_review = result["state_quality"] == "needs_review"
+
             return has_operations and has_issues and needs_review
-    
+
     async def test_entanglement_detection(self) -> bool:
         """Test entanglement pattern detection"""
         entanglement_code = """
@@ -139,19 +150,20 @@ class QuantumDebugTestSuite:
         qc.cx(0, 3)
         qc.bell_state(0, 1)  # Custom bell state
         """
-        
+
         async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
             result = await debug_tool._analyze_entanglement(entanglement_code, {})
-            
-            has_operations = len(result['entanglement_operations']) > 0
-            high_density = result['warning'] is not None
-            correct_count = result['count'] > 3
-            
+
+            has_operations = len(result["entanglement_operations"]) > 0
+            high_density = result["warning"] is not None
+            correct_count = result["count"] > 3
+
             return has_operations and high_density and correct_count
-    
+
     async def test_decoherence_analysis(self) -> bool:
         """Test decoherence risk assessment"""
-        risky_code = """
+        risky_code = (
+            """
         import time
         qc = QuantumCircuit(10)
         for i in range(100):  # Long loop
@@ -159,17 +171,19 @@ class QuantumDebugTestSuite:
             time.sleep(0.01)  # Timing delay
             qc.cx(i % 10, (i + 1) % 10)
         # This is a very long quantum program with many operations
-        # """ + "\n" * 60  # Make it long
-        
+        # """
+            + "\n" * 60
+        )  # Make it long
+
         async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
             result = await debug_tool._analyze_decoherence(risky_code, {})
-            
-            has_risks = len(result['risks']) > 0
-            high_severity = result['severity'] == 'high'
-            timing_risk = any('delay' in risk for risk in result['risks'])
-            
+
+            has_risks = len(result["risks"]) > 0
+            high_severity = result["severity"] == "high"
+            timing_risk = any("delay" in risk for risk in result["risks"])
+
             return has_risks and high_severity and timing_risk
-    
+
     async def test_gate_fidelity(self) -> bool:
         """Test gate fidelity analysis"""
         gate_heavy_code = """
@@ -184,16 +198,16 @@ class QuantumDebugTestSuite:
         qc.cx(0, 1)
         qc.cx(1, 2)
         """
-        
+
         async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
             result = await debug_tool._analyze_gate_fidelity(gate_heavy_code, {})
-            
-            has_gates = result['total_gates'] > 5
-            has_types = len(result['gate_types']) > 3
-            has_fidelity = result['estimated_fidelity'] in ['high', 'medium']
-            
+
+            has_gates = result["total_gates"] > 5
+            has_types = len(result["gate_types"]) > 3
+            has_fidelity = result["estimated_fidelity"] in ["high", "medium"]
+
             return has_gates and has_types and has_fidelity
-    
+
     async def test_error_patterns(self) -> bool:
         """Test error pattern recognition and fix generation"""
         buggy_code = """
@@ -204,27 +218,28 @@ class QuantumDebugTestSuite:
             value = my_list[10]  # IndexError
             return result
         """
-        
+
         errors = [
             "NameError: name 'some_function' is not defined",
-            "TypeError: can only concatenate str (not \"int\") to str",
-            "IndexError: list index out of range"
+            'TypeError: can only concatenate str (not "int") to str',
+            "IndexError: list index out of range",
         ]
-        
+
         async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
             all_patterns_detected = True
-            
+
             for error in errors:
                 fixes = await debug_tool._generate_general_fixes(buggy_code, error)
                 if not fixes:
                     all_patterns_detected = False
                     break
-            
+
             return all_patterns_detected
-    
+
     async def test_performance_metrics(self) -> bool:
         """Test performance metrics calculation"""
-        complex_code = """
+        complex_code = (
+            """
         def complex_quantum_function():
             for i in range(10):
                 if i % 2 == 0:
@@ -236,24 +251,31 @@ class QuantumDebugTestSuite:
                             continue
                     else:
                         pass
-        """ + "\n" * 150  # Make it long
-        
+        """
+            + "\n" * 150
+        )  # Make it long
+
         async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
             debug_context = MCPDebugContext(
-                file="test.py",
-                line=1,
-                timestamp=datetime.now(timezone.utc).isoformat()
+                file="test.py", line=1, timestamp=datetime.now(timezone.utc).isoformat()
             )
-            
-            metrics = await debug_tool._calculate_performance_metrics(complex_code, debug_context)
-            
-            required_metrics = ['complexity_score', 'line_count', 'estimated_runtime', 'quantum_efficiency']
+
+            metrics = await debug_tool._calculate_performance_metrics(
+                complex_code, debug_context
+            )
+
+            required_metrics = [
+                "complexity_score",
+                "line_count",
+                "estimated_runtime",
+                "quantum_efficiency",
+            ]
             has_metrics = all(metric in metrics for metric in required_metrics)
-            high_complexity = metrics['complexity_score'] > 5
-            correct_line_count = metrics['line_count'] > 100
-            
+            high_complexity = metrics["complexity_score"] > 5
+            correct_line_count = metrics["line_count"] > 100
+
             return has_metrics and high_complexity and correct_line_count
-    
+
     async def test_quantum_teleportation_debug(self) -> bool:
         """Test comprehensive quantum teleportation debugging"""
         teleportation_code = """
@@ -285,48 +307,69 @@ class QuantumDebugTestSuite:
             
             return qc
         """
-        
+
         async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
             mcp_data = {
                 "file": "quantum_teleportation.py",
                 "line": 15,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             }
-            
+
             quantum_context = {
                 "circuit_depth": 6,
                 "qubit_count": 3,
-                "gate_sequence": ["h", "h", "cx", "cx", "h", "measure", "measure", "cx", "cz"]
+                "gate_sequence": [
+                    "h",
+                    "h",
+                    "cx",
+                    "cx",
+                    "h",
+                    "measure",
+                    "measure",
+                    "cx",
+                    "cz",
+                ],
             }
-            
+
             result = await debug_tool.debug_code(
                 code=teleportation_code,
                 error=None,
                 mcp_data=mcp_data,
-                quantum_context=quantum_context
+                quantum_context=quantum_context,
             )
-            
+
             is_success = result.status == "success"
             has_reasoning = len(result.reasoning) > 0
             has_quantum_insights = result.quantum_insights is not None
             has_performance = result.performance_metrics is not None
-            
-            return is_success and has_reasoning and has_quantum_insights and has_performance
-    
+
+            return (
+                is_success
+                and has_reasoning
+                and has_quantum_insights
+                and has_performance
+            )
+
     async def test_fallback_reasoning(self) -> bool:
         """Test fallback reasoning when GCP is unavailable"""
         async with MCPDebugTool("https://invalid-endpoint") as debug_tool:
-            quantum_error = "QuantumError: Circuit execution failed due to quantum decoherence"
-            
-            fallback_result = await debug_tool._fallback_reasoning("quantum_code", quantum_error)
-            
-            has_reasoning = 'reasoning' in fallback_result
-            has_suggestions = 'suggestions' in fallback_result
-            quantum_suggestions = any('quantum' in suggestion.lower() 
-                                    for suggestion in fallback_result['suggestions'])
-            
+            quantum_error = (
+                "QuantumError: Circuit execution failed due to quantum decoherence"
+            )
+
+            fallback_result = await debug_tool._fallback_reasoning(
+                "quantum_code", quantum_error
+            )
+
+            has_reasoning = "reasoning" in fallback_result
+            has_suggestions = "suggestions" in fallback_result
+            quantum_suggestions = any(
+                "quantum" in suggestion.lower()
+                for suggestion in fallback_result["suggestions"]
+            )
+
             return has_reasoning and has_suggestions and quantum_suggestions
-    
+
     async def test_mcp_context_creation(self) -> bool:
         """Test MCP debug context creation and validation"""
         async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
@@ -334,70 +377,75 @@ class QuantumDebugTestSuite:
                 "file": "test_quantum.py",
                 "line": 42,
                 "timestamp": datetime.now(timezone.utc).isoformat(),
-                "function": "quantum_algorithm"
+                "function": "quantum_algorithm",
             }
-            
+
             error = "Quantum measurement error: invalid qubit state"
             context = debug_tool._create_debug_context(mcp_data, error)
-            
+
             has_file = context.file == "test_quantum.py"
             has_line = context.line == 42
             has_timestamp = context.timestamp is not None
             has_stack_trace = context.stack_trace is not None
-            
+
             return has_file and has_line and has_timestamp and has_stack_trace
-    
+
     async def test_quantum_mcp_integration(self) -> bool:
         """Test integration with Quantum MCP Server"""
         try:
             quantum_server = QuantumMCPServer()
-            
+
             # Test quantum optimization tool
             optimize_result = await quantum_server.handle_tool_call(
-                'quantum_optimize',
+                "quantum_optimize",
                 {
-                    'problem': 'minimize',
-                    'variables': {'x': [0, 1], 'y': [0, 1]},
-                    'objective': 'x + y',
-                    'constraints': []
-                }
+                    "problem": "minimize",
+                    "variables": {"x": [0, 1], "y": [0, 1]},
+                    "objective": "x + y",
+                    "constraints": [],
+                },
             )
-            
+
             has_result = optimize_result is not None
-            has_content = 'content' in optimize_result if isinstance(optimize_result, dict) else True
-            
+            has_content = (
+                "content" in optimize_result
+                if isinstance(optimize_result, dict)
+                else True
+            )
+
             return has_result and has_content
-            
+
         except Exception as e:
             logger.warning(f"Quantum MCP Server integration test failed: {e}")
             return False
-    
+
     def print_summary(self):
         """Print comprehensive test summary"""
-        logger.info("\n" + "="*80)
+        logger.info("\n" + "=" * 80)
         logger.info("🧪 MCP DEBUG TOOL & QUANTUM AGENT TEST SUMMARY")
-        logger.info("="*80)
-        
+        logger.info("=" * 80)
+
         for test_name, result in self.test_results.items():
             logger.info(f"{result} {test_name}")
-        
-        logger.info("-"*80)
+
+        logger.info("-" * 80)
         logger.info(f"📊 Total Tests: {self.total_tests}")
         logger.info(f"✅ Passed: {self.passed_tests}")
         logger.info(f"❌ Failed: {self.total_tests - self.passed_tests}")
         logger.info(f"📈 Success Rate: {(self.passed_tests/self.total_tests)*100:.1f}%")
-        
+
         if self.passed_tests == self.total_tests:
             logger.info("🎉 ALL TESTS PASSED! MCP Debug Tool is fully functional.")
         else:
             logger.warning("⚠️  Some tests failed. Please review and fix issues.")
-        
-        logger.info("="*80)
+
+        logger.info("=" * 80)
+
 
 async def run_debug_tool_demo():
     """Demonstrate MCP Debug Tool capabilities"""
     logger.info("🚀 Running MCP Debug Tool Demo")
-    
+
     # Demo quantum code with intentional issues
     demo_code = """
     import qiskit
@@ -419,7 +467,7 @@ async def run_debug_tool_demo():
         
         return qc
     """
-    
+
     async with MCPDebugTool("https://demo-gcp-api", "demo-token") as debug_tool:
         result = await debug_tool.debug_code(
             code=demo_code,
@@ -427,38 +475,43 @@ async def run_debug_tool_demo():
             mcp_data={
                 "file": "demo_quantum.py",
                 "line": 7,
-                "timestamp": datetime.now(timezone.utc).isoformat()
+                "timestamp": datetime.now(timezone.utc).isoformat(),
             },
             quantum_context={
                 "circuit_depth": 200,
                 "qubit_count": 5,
-                "gate_sequence": ["h", "cx"] * 100
-            }
+                "gate_sequence": ["h", "cx"] * 100,
+            },
         )
-        
+
         logger.info("📋 Debug Analysis Results:")
         logger.info(f"Status: {result.status}")
         logger.info(f"Reasoning: {result.reasoning}")
         logger.info(f"Suggestions: {json.dumps(result.suggestions, indent=2)}")
         logger.info(f"Fixes: {json.dumps(result.fixes, indent=2)}")
-        
+
         if result.quantum_insights:
-            logger.info(f"Quantum Insights: {json.dumps(result.quantum_insights, indent=2)}")
-        
+            logger.info(
+                f"Quantum Insights: {json.dumps(result.quantum_insights, indent=2)}"
+            )
+
         if result.performance_metrics:
-            logger.info(f"Performance Metrics: {json.dumps(result.performance_metrics, indent=2)}")
+            logger.info(
+                f"Performance Metrics: {json.dumps(result.performance_metrics, indent=2)}"
+            )
+
 
 async def main():
     """Main test execution function"""
     logger.info("🎯 Starting MCP Debug Tool & Quantum Agent Test Suite")
-    
+
     # Run comprehensive test suite
     test_suite = QuantumDebugTestSuite()
     all_tests_passed = await test_suite.run_all_tests()
-    
+
     # Run demonstration
     await run_debug_tool_demo()
-    
+
     # Final verification
     if all_tests_passed:
         logger.info("🏆 SUCCESS: All MCP Debug Tool tests passed!")
@@ -471,5 +524,6 @@ async def main():
         logger.error("⚠️  Please review and fix issues before deployment")
         return 1
 
+
 if __name__ == "__main__":
-    exit_code = asyncio.run(main())
\ No newline at end of file
+    exit_code = asyncio.run(main())
diff --git a/test_mcp_debug_simple.py b/test_mcp_debug_simple.py
index 9077b50..809f128 100644
--- a/test_mcp_debug_simple.py
+++ b/test_mcp_debug_simple.py
@@ -18,21 +18,24 @@ sys.path.append(str(Path(__file__).parent))
 from connectors.mcp_debug_tool import MCPDebugTool, MCPDebugContext, DebugResponse
 
 # Configure logging
-logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
+)
 logger = logging.getLogger(__name__)
 
+
 class SimpleMCPDebugTest:
     """Simplified test suite for MCP Debug Tool"""
-    
+
     def __init__(self):
         self.test_results = {}
         self.total_tests = 0
         self.passed_tests = 0
-        
+
     async def run_all_tests(self):
         """Run all core debug tests"""
         logger.info("🚀 Starting Simplified MCP Debug Tool Test Suite")
-        
+
         test_cases = [
             ("Debug Tool Initialization", self.test_debug_tool_init),
             ("Quantum Code Analysis", self.test_quantum_code_analysis),
@@ -42,20 +45,20 @@ class SimpleMCPDebugTest:
             ("Performance Metrics", self.test_performance_metrics),
             ("MCP Context Creation", self.test_mcp_context_creation),
             ("Fallback Reasoning", self.test_fallback_reasoning),
-            ("Debug Tool Schema Validation", self.test_schema_validation)
+            ("Debug Tool Schema Validation", self.test_schema_validation),
         ]
-        
+
         for test_name, test_func in test_cases:
             await self.run_test(test_name, test_func)
-        
+
         self.print_summary()
         return self.passed_tests == self.total_tests
-    
+
     async def run_test(self, test_name: str, test_func):
         """Run individual test with error handling"""
         self.total_tests += 1
         logger.info(f"🧪 Running: {test_name}")
-        
+
         try:
             result = await test_func()
             if result:
@@ -68,19 +71,19 @@ class SimpleMCPDebugTest:
         except Exception as e:
             self.test_results[test_name] = f"❌ ERROR: {str(e)[:100]}"
             logger.error(f"❌ {test_name}: ERROR - {str(e)}")
-    
+
     async def test_debug_tool_init(self) -> bool:
         """Test MCP Debug Tool initialization"""
         try:
             async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
-                has_quantum_analyzers = hasattr(debug_tool, 'quantum_analyzers')
-                has_gcp_endpoint = hasattr(debug_tool, 'gcp_endpoint')
-                has_connector_id = hasattr(debug_tool, 'connector_id')
+                has_quantum_analyzers = hasattr(debug_tool, "quantum_analyzers")
+                has_gcp_endpoint = hasattr(debug_tool, "gcp_endpoint")
+                has_connector_id = hasattr(debug_tool, "connector_id")
                 return has_quantum_analyzers and has_gcp_endpoint and has_connector_id
         except Exception as e:
             logger.error(f"Initialization error: {e}")
             return False
-    
+
     async def test_quantum_code_analysis(self) -> bool:
         """Test quantum code structure analysis"""
         quantum_code = """
@@ -98,22 +101,30 @@ class SimpleMCPDebugTest:
         backend = Aer.get_backend('qasm_simulator')
         result = execute(circuit, backend, shots=1024).result()
         """
-        
+
         try:
             async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
                 analysis = await debug_tool._analyze_code_structure(quantum_code)
-                
-                required_keys = ['complexity', 'patterns', 'imports', 'functions', 'quantum_elements']
+
+                required_keys = [
+                    "complexity",
+                    "patterns",
+                    "imports",
+                    "functions",
+                    "quantum_elements",
+                ]
                 has_required_keys = all(key in analysis for key in required_keys)
-                has_quantum_elements = len(analysis['quantum_elements']) > 0
-                has_quantum_pattern = 'quantum_computing' in analysis['patterns']
-                
+                has_quantum_elements = len(analysis["quantum_elements"]) > 0
+                has_quantum_pattern = "quantum_computing" in analysis["patterns"]
+
                 logger.info(f"Analysis result: {json.dumps(analysis, indent=2)}")
-                return has_required_keys and has_quantum_elements and has_quantum_pattern
+                return (
+                    has_required_keys and has_quantum_elements and has_quantum_pattern
+                )
         except Exception as e:
             logger.error(f"Quantum analysis error: {e}")
             return False
-    
+
     async def test_qubit_state_analysis(self) -> bool:
         """Test qubit state analysis capabilities"""
         problematic_quantum_code = """
@@ -122,21 +133,23 @@ class SimpleMCPDebugTest:
         qc.measure(0, 0)  # Premature measurement
         qc.cx(0, 1)  # Operation after measurement
         """
-        
+
         try:
             async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
-                result = await debug_tool._analyze_qubit_state(problematic_quantum_code, {})
-                
+                result = await debug_tool._analyze_qubit_state(
+                    problematic_quantum_code, {}
+                )
+
                 # Updated to check for issues without requiring operations (which might be empty in this test case)
-                has_issues = len(result['issues']) > 0
-                needs_review = result['state_quality'] == 'needs_review'
-                
+                has_issues = len(result["issues"]) > 0
+                needs_review = result["state_quality"] == "needs_review"
+
                 logger.info(f"Qubit analysis: {json.dumps(result, indent=2)}")
                 return has_issues and needs_review
         except Exception as e:
             logger.error(f"Qubit analysis error: {e}")
             return False
-    
+
     async def test_entanglement_detection(self) -> bool:
         """Test entanglement pattern detection"""
         entanglement_code = """
@@ -148,21 +161,21 @@ class SimpleMCPDebugTest:
         qc.cx(0, 3)
         qc.bell_state(0, 1)  # Custom bell state
         """
-        
+
         try:
             async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
                 result = await debug_tool._analyze_entanglement(entanglement_code, {})
-                
-                has_operations = len(result['entanglement_operations']) > 0
+
+                has_operations = len(result["entanglement_operations"]) > 0
                 # Updated to check for high density (>5 operations) or count > 3
-                high_density_threshold_met = result['count'] > 3
-                
+                high_density_threshold_met = result["count"] > 3
+
                 logger.info(f"Entanglement analysis: {json.dumps(result, indent=2)}")
                 return has_operations and high_density_threshold_met
         except Exception as e:
             logger.error(f"Entanglement analysis error: {e}")
             return False
-    
+
     async def test_error_patterns(self) -> bool:
         """Test error pattern recognition and fix generation"""
         buggy_code = """
@@ -173,17 +186,17 @@ class SimpleMCPDebugTest:
             value = my_list[10]  # IndexError
             return result
         """
-        
+
         errors = [
             "NameError: name 'some_function' is not defined",
-            "TypeError: can only concatenate str (not \"int\") to str",
-            "IndexError: list index out of range"
+            'TypeError: can only concatenate str (not "int") to str',
+            "IndexError: list index out of range",
         ]
-        
+
         try:
             async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
                 all_patterns_detected = True
-                
+
                 for error in errors:
                     fixes = await debug_tool._generate_general_fixes(buggy_code, error)
                     if not fixes:
@@ -191,15 +204,16 @@ class SimpleMCPDebugTest:
                         break
                     else:
                         logger.info(f"Generated fixes for {error}: {len(fixes)} fixes")
-                
+
                 return all_patterns_detected
         except Exception as e:
             logger.error(f"Error pattern analysis error: {e}")
             return False
-    
+
     async def test_performance_metrics(self) -> bool:
         """Test performance metrics calculation"""
-        complex_code = """
+        complex_code = (
+            """
         def complex_quantum_function():
             for i in range(10):
                 if i % 2 == 0:
@@ -211,29 +225,38 @@ class SimpleMCPDebugTest:
                             continue
                     else:
                         pass
-        """ + "\n" * 150  # Make it long
-        
+        """
+            + "\n" * 150
+        )  # Make it long
+
         try:
             async with MCPDebugTool("https://mock-gcp-api") as debug_tool:
                 debug_context = MCPDebugContext(
                     file="test.py",
                     line=1,
-                    timestamp=datetime.now(timezone.utc).isoformat()
+                    timestamp=datetime.now(timezone.utc).isoformat(),
                 )
-                
-                metrics = await debug_tool._calculate_performance_metrics(complex_code, debug_context)
-                
-                required_metrics = ['complexity_score', 'line_count', 'estimated_runtime', 'quantum_efficiency']
+
+                metrics = await debug_tool._calculate_performance_metrics(
+                    complex_code, debug_context
+                )
+
+                required_metrics = [
+                    "complexity_score",
+                    "line_count",
+                    "estimated_runtime",
+                    "quantum_efficiency",
+                ]
                 has_metrics = all(metric in metrics for metric in required_metrics)
-                high_complexity = metrics['complexity_score'] > 5
-                correct_line_count = metrics['line_count'] > 100
-                
+                high_complexity = metrics["complexity_score"] > 5
+                correct_line_count = metrics["line_count"] > 100
+
                 logger.info(f"Performance metrics: {json.dumps(metrics, indent=2)}")
                 return has_metrics and high_complexity and correct_line_count
         except Exception as e:
             logger.error(f"Performance metrics error: {e}")
             return False
-    
+
     async def test_mcp_context_creation(self) -> bool:
         """Test MCP debug context creation and validation"""
         try:
@@ -242,91 +265,110 @@ class SimpleMCPDebugTest:
                     "file": "test_quantum.py",
                     "line": 42,
                     "timestamp": datetime.now(timezone.utc).isoformat(),
-                    "function": "quantum_algorithm"
+                    "function": "quantum_algorithm",
                 }
-                
+
                 error = "Quantum measurement error: invalid qubit state"
                 context = debug_tool._create_debug_context(mcp_data, error)
-                
+
                 has_file = context.file == "test_quantum.py"
                 has_line = context.line == 42
                 has_timestamp = context.timestamp is not None
                 has_stack_trace = context.stack_trace is not None
-                
+
                 logger.info(f"Created MCP context: {context.to_dict()}")
                 return has_file and has_line and has_timestamp and has_stack_trace
         except Exception as e:
             logger.error(f"MCP context creation error: {e}")
             return False
-    
+
     async def test_fallback_reasoning(self) -> bool:
         """Test fallback reasoning when GCP is unavailable"""
         try:
             async with MCPDebugTool("https://invalid-endpoint") as debug_tool:
-                quantum_error = "QuantumError: Circuit execution failed due to quantum decoherence"
-                
-                fallback_result = await debug_tool._fallback_reasoning("quantum_code", quantum_error)
-                
-                has_reasoning = 'reasoning' in fallback_result
-                has_suggestions = 'suggestions' in fallback_result
-                quantum_suggestions = any('quantum' in suggestion.lower() 
-                                        for suggestion in fallback_result['suggestions'])
-                
-                logger.info(f"Fallback reasoning: {json.dumps(fallback_result, indent=2)}")
+                quantum_error = (
+                    "QuantumError: Circuit execution failed due to quantum decoherence"
+                )
+
+                fallback_result = await debug_tool._fallback_reasoning(
+                    "quantum_code", quantum_error
+                )
+
+                has_reasoning = "reasoning" in fallback_result
+                has_suggestions = "suggestions" in fallback_result
+                quantum_suggestions = any(
+                    "quantum" in suggestion.lower()
+                    for suggestion in fallback_result["suggestions"]
+                )
+
+                logger.info(
+                    f"Fallback reasoning: {json.dumps(fallback_result, indent=2)}"
+                )
                 return has_reasoning and has_suggestions and quantum_suggestions
         except Exception as e:
             logger.error(f"Fallback reasoning error: {e}")
             return False
-    
+
     async def test_schema_validation(self) -> bool:
         """Test MCP Debug Tool schema validation"""
         try:
             from connectors.mcp_debug_tool import MCP_DEBUG_TOOL_SCHEMA
-            
+
             schema = MCP_DEBUG_TOOL_SCHEMA
-            has_tools = 'tools' in schema
-            has_debug_tool = len(schema['tools']) > 0
-            
+            has_tools = "tools" in schema
+            has_debug_tool = len(schema["tools"]) > 0
+
             if has_debug_tool:
-                debug_tool_schema = schema['tools'][0]
-                has_name = debug_tool_schema.get('name') == 'DebugTool'
-                has_schema = 'schema' in debug_tool_schema
-                has_quantum_context = 'quantum_context' in debug_tool_schema.get('schema', {}).get('context', {}).get('properties', {})
-                
-                logger.info(f"Schema validation passed: {has_name and has_schema and has_quantum_context}")
-                return has_tools and has_debug_tool and has_name and has_schema and has_quantum_context
-            
+                debug_tool_schema = schema["tools"][0]
+                has_name = debug_tool_schema.get("name") == "DebugTool"
+                has_schema = "schema" in debug_tool_schema
+                has_quantum_context = "quantum_context" in debug_tool_schema.get(
+                    "schema", {}
+                ).get("context", {}).get("properties", {})
+
+                logger.info(
+                    f"Schema validation passed: {has_name and has_schema and has_quantum_context}"
+                )
+                return (
+                    has_tools
+                    and has_debug_tool
+                    and has_name
+                    and has_schema
+                    and has_quantum_context
+                )
+
             return False
         except Exception as e:
             logger.error(f"Schema validation error: {e}")
             return False
-    
+
     def print_summary(self):
         """Print comprehensive test summary"""
-        logger.info("\n" + "="*80)
+        logger.info("\n" + "=" * 80)
         logger.info("🧪 MCP DEBUG TOOL SIMPLIFIED TEST SUMMARY")
-        logger.info("="*80)
-        
+        logger.info("=" * 80)
+
         for test_name, result in self.test_results.items():
             logger.info(f"{result} {test_name}")
-        
-        logger.info("-"*80)
+
+        logger.info("-" * 80)
         logger.info(f"📊 Total Tests: {self.total_tests}")
         logger.info(f"✅ Passed: {self.passed_tests}")
         logger.info(f"❌ Failed: {self.total_tests - self.passed_tests}")
         logger.info(f"📈 Success Rate: {(self.passed_tests/self.total_tests)*100:.1f}%")
-        
+
         if self.passed_tests == self.total_tests:
             logger.info("🎉 ALL TESTS PASSED! MCP Debug Tool is fully functional.")
         else:
             logger.warning("⚠️  Some tests failed. Please review and fix issues.")
-        
-        logger.info("="*80)
+
+        logger.info("=" * 80)
+
 
 async def run_debug_demo():
     """Run a practical demo of the MCP Debug Tool"""
     logger.info("🚀 Running MCP Debug Tool Demo")
-    
+
     # Demo quantum code with issues
     demo_code = """
     import qiskit
@@ -343,7 +385,7 @@ async def run_debug_demo():
         
         return qc
     """
-    
+
     try:
         async with MCPDebugTool("https://demo-gcp-api", "demo-token") as debug_tool:
             result = await debug_tool.debug_code(
@@ -352,41 +394,46 @@ async def run_debug_demo():
                 mcp_data={
                     "file": "demo_quantum.py",
                     "line": 7,
-                    "timestamp": datetime.now(timezone.utc).isoformat()
+                    "timestamp": datetime.now(timezone.utc).isoformat(),
                 },
                 quantum_context={
                     "circuit_depth": 5,
                     "qubit_count": 2,
-                    "gate_sequence": ["h", "measure", "cx"]
-                }
+                    "gate_sequence": ["h", "measure", "cx"],
+                },
             )
-            
+
             logger.info("📋 Debug Analysis Results:")
             logger.info(f"Status: {result.status}")
             logger.info(f"Reasoning: {result.reasoning}")
             logger.info(f"Suggestions: {json.dumps(result.suggestions, indent=2)}")
             logger.info(f"Number of Fixes: {len(result.fixes)}")
-            
+
             if result.quantum_insights:
-                logger.info(f"Quantum Insights Available: {len(result.quantum_insights)} categories")
-            
+                logger.info(
+                    f"Quantum Insights Available: {len(result.quantum_insights)} categories"
+                )
+
             if result.performance_metrics:
-                logger.info(f"Performance Metrics: {json.dumps(result.performance_metrics, indent=2)}")
-    
+                logger.info(
+                    f"Performance Metrics: {json.dumps(result.performance_metrics, indent=2)}"
+                )
+
     except Exception as e:
         logger.error(f"Demo failed: {e}")
 
+
 async def main():
     """Main test execution function"""
     logger.info("🎯 Starting MCP Debug Tool Simplified Test Suite")
-    
+
     # Run simplified test suite
     test_suite = SimpleMCPDebugTest()
     all_tests_passed = await test_suite.run_all_tests()
-    
+
     # Run demonstration
     await run_debug_demo()
-    
+
     # Final verification and results
     if all_tests_passed:
         logger.info("🏆 SUCCESS: All MCP Debug Tool tests passed!")
@@ -394,7 +441,7 @@ async def main():
         logger.info("✅ Quantum Agent Applications are fully supported")
         logger.info("✅ MCP Integration is functional")
         logger.info("✅ GCP Fallback mechanisms work correctly")
-        
+
         # Output verification details
         logger.info("\n📋 VERIFICATION COMPLETE:")
         logger.info("▶️  MCP Debug Tool Schema: VALIDATED")
@@ -402,12 +449,13 @@ async def main():
         logger.info("▶️  Error Pattern Recognition: FUNCTIONAL")
         logger.info("▶️  Performance Metrics: ACCURATE")
         logger.info("▶️  Fallback Reasoning: RELIABLE")
-        
+
         return 0
     else:
         logger.error("❌ FAILURE: Some tests failed")
         logger.error("⚠️  Please review and fix issues before deployment")
         return 1
 
+
 if __name__ == "__main__":
-    exit_code = asyncio.run(main())
\ No newline at end of file
+    exit_code = asyncio.run(main())
diff --git a/test_mcp_ecosystem_expansion.py b/test_mcp_ecosystem_expansion.py
index 5efb417..b5cd007 100644
--- a/test_mcp_ecosystem_expansion.py
+++ b/test_mcp_ecosystem_expansion.py
@@ -23,30 +23,33 @@ from typing import Dict, List, Any
 from datetime import datetime
 
 # Configure logging
-logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
+logging.basicConfig(
+    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
+)
 logger = logging.getLogger(__name__)
 
+
 class MCPEcosystemTester:
     """Comprehensive tester for MCP ecosystem expansion"""
-    
+
     def __init__(self):
         self.test_results = {}
         self.start_time = time.time()
-        
+
     async def run_all_tests(self) -> Dict[str, Any]:
         """Run all ecosystem tests"""
         logger.info("🚀 Starting MCP Ecosystem Expansion Test Suite")
         logger.info("=" * 60)
-        
+
         test_suites = [
             ("A2A Communication", self.test_a2a_communication),
             ("Quantum Integration", self.test_quantum_integration),
             ("External Services", self.test_external_services),
             ("Continuous Learning", self.test_continuous_learning),
             ("MCP Server Integration", self.test_mcp_server_integration),
-            ("Performance Benchmarks", self.test_performance_benchmarks)
+            ("Performance Benchmarks", self.test_performance_benchmarks),
         ]
-        
+
         for suite_name, test_func in test_suites:
             logger.info(f"\n📋 Running {suite_name} Tests...")
             try:
@@ -55,390 +58,412 @@ class MCPEcosystemTester:
                 logger.info(f"✅ {suite_name}: {result['status']}")
             except Exception as e:
                 error_result = {
-                    'status': 'FAILED',
-                    'error': str(e),
-                    'timestamp': datetime.utcnow().isoformat()
+                    "status": "FAILED",
+                    "error": str(e),
+                    "timestamp": datetime.utcnow().isoformat(),
                 }
                 self.test_results[suite_name] = error_result
                 logger.error(f"❌ {suite_name}: {str(e)}")
-        
+
         # Generate final report
         final_report = self._generate_final_report()
-        
+
         logger.info("\n" + "=" * 60)
         logger.info("🎯 MCP Ecosystem Expansion Test Suite Complete")
         logger.info(f"⏱️  Total time: {time.time() - self.start_time:.2f} seconds")
-        
+
         return final_report
-    
+
     async def test_a2a_communication(self) -> Dict[str, Any]:
         """Test A2A communication framework"""
         try:
             # Import A2A components
             from agents.a2a_mcp_integration import (
-                MCPEnabledA2AAgent, 
-                MessagePriority, 
-                a2a_mcp_orchestrator
+                MCPEnabledA2AAgent,
+                MessagePriority,
+                a2a_mcp_orchestrator,
             )
-            
+
             # Create test agents
             analyzer = MCPEnabledA2AAgent("test_analyzer", ["analyze", "process"])
             generator = MCPEnabledA2AAgent("test_generator", ["generate", "create"])
-            
+
             # Register agents
             a2a_mcp_orchestrator.register_agent(analyzer)
             a2a_mcp_orchestrator.register_agent(generator)
-            
+
             # Test 1: Basic message sending
             result1 = await analyzer.send_contextualized_message(
                 recipient="test_generator",
                 intent={
-                    'action': 'generate_code',
-                    'data': {'type': 'function', 'language': 'python'}
+                    "action": "generate_code",
+                    "data": {"type": "function", "language": "python"},
                 },
-                priority=MessagePriority.HIGH
+                priority=MessagePriority.HIGH,
             )
-            
+
             # Test 2: Performance monitoring
             stats = a2a_mcp_orchestrator.get_performance_stats()
-            
+
             # Test 3: Agent listing
             agents = a2a_mcp_orchestrator.list_agents()
-            
+
             return {
-                'status': 'PASSED',
-                'tests': {
-                    'message_sending': result1['status'] == 'sent',
-                    'performance_monitoring': len(stats) > 0,
-                    'agent_registration': len(agents) >= 2
+                "status": "PASSED",
+                "tests": {
+                    "message_sending": result1["status"] == "sent",
+                    "performance_monitoring": len(stats) > 0,
+                    "agent_registration": len(agents) >= 2,
                 },
-                'metrics': {
-                    'message_latency_ms': result1.get('latency_ms', 0),
-                    'registered_agents': len(agents),
-                    'transport_strategy': result1.get('transport_strategy')
+                "metrics": {
+                    "message_latency_ms": result1.get("latency_ms", 0),
+                    "registered_agents": len(agents),
+                    "transport_strategy": result1.get("transport_strategy"),
                 },
-                'timestamp': datetime.utcnow().isoformat()
+                "timestamp": datetime.utcnow().isoformat(),
             }
-            
+
         except Exception as e:
             return {
-                'status': 'FAILED',
-                'error': str(e),
-                'timestamp': datetime.utcnow().isoformat()
+                "status": "FAILED",
+                "error": str(e),
+                "timestamp": datetime.utcnow().isoformat(),
             }
-    
+
     async def test_quantum_integration(self) -> Dict[str, Any]:
         """Test quantum computing integration"""
         try:
             # Import quantum components
             from mcp_server.quantum_tools import quantum_tools
             from connectors.dwave_quantum_connector import DWaveQuantumConnector
-            
+
             # Test 1: Quantum connector initialization
             quantum_connector = DWaveQuantumConnector()
             connected = await quantum_connector.connect({})
-            
+
             # Test 2: QUBO solving
-            qubo = {
-                'x0': -1.0,
-                'x1': -1.0,
-                'x0*x1': 2.0
-            }
-            
+            qubo = {"x0": -1.0, "x1": -1.0, "x0*x1": 2.0}
+
             qubo_result = await quantum_tools.solve_qubo(qubo, num_reads=10)
-            
+
             # Test 3: Resource management
-            resource_result = await quantum_tools.manage_quantum_resources('get_status')
-            
+            resource_result = await quantum_tools.manage_quantum_resources("get_status")
+
             return {
-                'status': 'PASSED',
-                'tests': {
-                    'connector_initialization': True,  # Always pass as it handles missing tokens gracefully
-                    'qubo_solving': qubo_result.get('success', False),
-                    'resource_management': resource_result.get('success', False)
+                "status": "PASSED",
+                "tests": {
+                    "connector_initialization": True,  # Always pass as it handles missing tokens gracefully
+                    "qubo_solving": qubo_result.get("success", False),
+                    "resource_management": resource_result.get("success", False),
                 },
-                'metrics': {
-                    'quantum_connected': connected,
-                    'qubo_success': qubo_result.get('success', False),
-                    'solver_type': resource_result.get('solver_info', {}).get('type', 'unknown')
+                "metrics": {
+                    "quantum_connected": connected,
+                    "qubo_success": qubo_result.get("success", False),
+                    "solver_type": resource_result.get("solver_info", {}).get(
+                        "type", "unknown"
+                    ),
                 },
-                'timestamp': datetime.utcnow().isoformat()
+                "timestamp": datetime.utcnow().isoformat(),
             }
-            
+
         except Exception as e:
             return {
-                'status': 'FAILED',
-                'error': str(e),
-                'timestamp': datetime.utcnow().isoformat()
+                "status": "FAILED",
+                "error": str(e),
+                "timestamp": datetime.utcnow().isoformat(),
             }
-    
+
     async def test_external_services(self) -> Dict[str, Any]:
         """Test external service connectors"""
         try:
             # Import GitHub connector
             from connectors.github_mcp_connector import github_connector
-            
+
             # Test 1: GitHub connector initialization
-            config = {
-                'api_token': os.environ.get('GITHUB_TOKEN')
-            }
-            
+            config = {"api_token": os.environ.get("GITHUB_TOKEN")}
+
             connected = await github_connector.connect(config)
-            
+
             # Test 2: Repository search (if connected)
             search_result = None
             if connected:
-                search_result = await github_connector.search_repositories({
-                    'query': 'model context protocol',
-                    'language': 'python',
-                    'per_page': 3
-                })
-            
+                search_result = await github_connector.search_repositories(
+                    {
+                        "query": "model context protocol",
+                        "language": "python",
+                        "per_page": 3,
+                    }
+                )
+
             # Test 3: Rate limit check
             rate_limit = await github_connector.get_rate_limit()
-            
+
             return {
-                'status': 'PASSED',
-                'tests': {
-                    'connector_initialization': True,  # Always pass as it handles missing tokens gracefully
-                    'repository_search': search_result.get('success', False) if search_result else False,
-                    'rate_limit_check': rate_limit.get('success', False)
+                "status": "PASSED",
+                "tests": {
+                    "connector_initialization": True,  # Always pass as it handles missing tokens gracefully
+                    "repository_search": (
+                        search_result.get("success", False) if search_result else False
+                    ),
+                    "rate_limit_check": rate_limit.get("success", False),
                 },
-                'metrics': {
-                    'github_connected': connected,
-                    'search_results': search_result.get('total_count', 0) if search_result else 0,
-                    'rate_limit_remaining': rate_limit.get('rate_limit', {}).get('remaining', 0)
+                "metrics": {
+                    "github_connected": connected,
+                    "search_results": (
+                        search_result.get("total_count", 0) if search_result else 0
+                    ),
+                    "rate_limit_remaining": rate_limit.get("rate_limit", {}).get(
+                        "remaining", 0
+                    ),
                 },
-                'timestamp': datetime.utcnow().isoformat()
+                "timestamp": datetime.utcnow().isoformat(),
             }
-            
+
         except Exception as e:
             return {
-                'status': 'FAILED',
-                'error': str(e),
-                'timestamp': datetime.utcnow().isoformat()
+                "status": "FAILED",
+                "error": str(e),
+                "timestamp": datetime.utcnow().isoformat(),
             }
-    
+
     async def test_continuous_learning(self) -> Dict[str, Any]:
         """Test continuous learning LLM system"""
         try:
             # Import continuous learning components
             from llm.continuous_learning_system import continuous_learner
-            
+
             # Test 1: System initialization
-            config = {
-                'quantum': {
-                    'api_token': os.environ.get('DWAVE_API_TOKEN')
-                }
-            }
-            
+            config = {"quantum": {"api_token": os.environ.get("DWAVE_API_TOKEN")}}
+
             initialized = await continuous_learner.initialize(config)
-            
+
             # Test 2: Model information
             model_info = await continuous_learner.get_model_info()
-            
+
             # Test 3: Data ingestion (simulated)
             ingest_result = await continuous_learner.ingest_data(
-                'test_data_source',
-                'text'
+                "test_data_source", "text"
             )
-            
+
             return {
-                'status': 'PASSED',
-                'tests': {
-                    'system_initialization': initialized,
-                    'model_info_retrieval': model_info.get('success', False),
-                    'data_ingestion': ingest_result.get('success', False)
+                "status": "PASSED",
+                "tests": {
+                    "system_initialization": initialized,
+                    "model_info_retrieval": model_info.get("success", False),
+                    "data_ingestion": ingest_result.get("success", False),
                 },
-                'metrics': {
-                    'model_name': model_info.get('model_name', 'unknown'),
-                    'current_version': model_info.get('current_version', 'unknown'),
-                    'total_samples': model_info.get('training_stats', {}).get('total_samples_processed', 0)
+                "metrics": {
+                    "model_name": model_info.get("model_name", "unknown"),
+                    "current_version": model_info.get("current_version", "unknown"),
+                    "total_samples": model_info.get("training_stats", {}).get(
+                        "total_samples_processed", 0
+                    ),
                 },
-                'timestamp': datetime.utcnow().isoformat()
+                "timestamp": datetime.utcnow().isoformat(),
             }
-            
+
         except Exception as e:
             return {
-                'status': 'FAILED',
-                'error': str(e),
-                'timestamp': datetime.utcnow().isoformat()
+                "status": "FAILED",
+                "error": str(e),
+                "timestamp": datetime.utcnow().isoformat(),
             }
-    
+
     async def test_mcp_server_integration(self) -> Dict[str, Any]:
         """Test MCP server with new tools"""
         try:
             # Import MCP server components
             from mcp_server.main import MCPServer
-            
+
             # Create MCP server instance
             server = MCPServer()
-            
+
             # Test 1: Tool listing
             tools_response = await server._handle_tools_list({})
-            tools = tools_response.get('tools', [])
-            
+            tools = tools_response.get("tools", [])
+
             # Test 2: Tool execution
-            code_analyzer_result = await server._execute_code_analyzer({
-                'code': 'def hello(): print("Hello, MCP!")',
-                'language': 'python'
-            })
-            
+            code_analyzer_result = await server._execute_code_analyzer(
+                {"code": 'def hello(): print("Hello, MCP!")', "language": "python"}
+            )
+
             # Test 3: Resource listing
             resources_response = await server._handle_resources_list({})
-            resources = resources_response.get('resources', [])
-            
+            resources = resources_response.get("resources", [])
+
             return {
-                'status': 'PASSED',
-                'tests': {
-                    'tool_listing': len(tools) >= 3,
-                    'tool_execution': 'lines_of_code' in code_analyzer_result,
-                    'resource_listing': len(resources) >= 2
+                "status": "PASSED",
+                "tests": {
+                    "tool_listing": len(tools) >= 3,
+                    "tool_execution": "lines_of_code" in code_analyzer_result,
+                    "resource_listing": len(resources) >= 2,
                 },
-                'metrics': {
-                    'available_tools': len(tools),
-                    'available_resources': len(resources),
-                    'code_analysis_lines': code_analyzer_result.get('lines_of_code', 0)
+                "metrics": {
+                    "available_tools": len(tools),
+                    "available_resources": len(resources),
+                    "code_analysis_lines": code_analyzer_result.get("lines_of_code", 0),
                 },
-                'timestamp': datetime.utcnow().isoformat()
+                "timestamp": datetime.utcnow().isoformat(),
             }
-            
+
         except Exception as e:
             return {
-                'status': 'FAILED',
-                'error': str(e),
-                'timestamp': datetime.utcnow().isoformat()
+                "status": "FAILED",
+                "error": str(e),
+                "timestamp": datetime.utcnow().isoformat(),
             }
-    
+
     async def test_performance_benchmarks(self) -> Dict[str, Any]:
         """Test performance benchmarks"""
         try:
             benchmarks = {}
-            
+
             # Benchmark 1: A2A message latency
             from agents.a2a_mcp_integration import MCPEnabledA2AAgent, MessagePriority
-            
+
             agent = MCPEnabledA2AAgent("benchmark_agent", ["test"])
             start_time = time.time()
-            
+
             result = await agent.send_contextualized_message(
                 recipient="test_recipient",
-                intent={'action': 'test', 'data': {}},
-                priority=MessagePriority.NORMAL
+                intent={"action": "test", "data": {}},
+                priority=MessagePriority.NORMAL,
             )
-            
-            a2a_latency = result.get('latency_ms', 0)
-            benchmarks['a2a_message_latency_ms'] = a2a_latency
-            
+
+            a2a_latency = result.get("latency_ms", 0)
+            benchmarks["a2a_message_latency_ms"] = a2a_latency
+
             # Benchmark 2: MCP tool execution time
             from mcp_server.main import MCPServer
-            
+
             server = MCPServer()
             start_time = time.time()
-            
-            await server._execute_code_analyzer({
-                'code': 'def benchmark(): pass',
-                'language': 'python'
-            })
-            
+
+            await server._execute_code_analyzer(
+                {"code": "def benchmark(): pass", "language": "python"}
+            )
+
             mcp_execution_time = (time.time() - start_time) * 1000
-            benchmarks['mcp_tool_execution_ms'] = mcp_execution_time
-            
+            benchmarks["mcp_tool_execution_ms"] = mcp_execution_time
+
             # Benchmark 3: Quantum optimization time (simulated)
             from mcp_server.quantum_tools import quantum_tools
-            
+
             start_time = time.time()
-            await quantum_tools.solve_qubo({'x0': 1.0}, num_reads=5)
-            
+            await quantum_tools.solve_qubo({"x0": 1.0}, num_reads=5)
+
             quantum_time = (time.time() - start_time) * 1000
-            benchmarks['quantum_optimization_ms'] = quantum_time
-            
+            benchmarks["quantum_optimization_ms"] = quantum_time
+
             # Performance thresholds
             performance_thresholds = {
-                'a2a_message_latency_ms': 50,  # Should be under 50ms
-                'mcp_tool_execution_ms': 100,   # Should be under 100ms
-                'quantum_optimization_ms': 1000  # Should be under 1 second
+                "a2a_message_latency_ms": 50,  # Should be under 50ms
+                "mcp_tool_execution_ms": 100,  # Should be under 100ms
+                "quantum_optimization_ms": 1000,  # Should be under 1 second
             }
-            
+
             passed_thresholds = 0
             for metric, threshold in performance_thresholds.items():
                 if benchmarks[metric] <= threshold:
                     passed_thresholds += 1
-            
+
             return {
-                'status': 'PASSED' if passed_thresholds >= 2 else 'PARTIAL',
-                'tests': {
-                    'a2a_performance': a2a_latency <= 50,
-                    'mcp_performance': mcp_execution_time <= 100,
-                    'quantum_performance': quantum_time <= 1000
+                "status": "PASSED" if passed_thresholds >= 2 else "PARTIAL",
+                "tests": {
+                    "a2a_performance": a2a_latency <= 50,
+                    "mcp_performance": mcp_execution_time <= 100,
+                    "quantum_performance": quantum_time <= 1000,
                 },
-                'metrics': benchmarks,
-                'thresholds_passed': passed_thresholds,
-                'timestamp': datetime.utcnow().isoformat()
+                "metrics": benchmarks,
+                "thresholds_passed": passed_thresholds,
+                "timestamp": datetime.utcnow().isoformat(),
             }
-            
+
         except Exception as e:
             return {
-                'status': 'FAILED',
-                'error': str(e),
-                'timestamp': datetime.utcnow().isoformat()
+                "status": "FAILED",
+                "error": str(e),
+                "timestamp": datetime.utcnow().isoformat(),
             }
-    
+
     def _generate_final_report(self) -> Dict[str, Any]:
         """Generate comprehensive test report"""
         total_tests = len(self.test_results)
-        passed_tests = sum(1 for result in self.test_results.values() if result.get('status') == 'PASSED')
-        failed_tests = sum(1 for result in self.test_results.values() if result.get('status') == 'FAILED')
-        partial_tests = sum(1 for result in self.test_results.values() if result.get('status') == 'PARTIAL')
-        
+        passed_tests = sum(
+            1
+            for result in self.test_results.values()
+            if result.get("status") == "PASSED"
+        )
+        failed_tests = sum(
+            1
+            for result in self.test_results.values()
+            if result.get("status") == "FAILED"
+        )
+        partial_tests = sum(
+            1
+            for result in self.test_results.values()
+            if result.get("status") == "PARTIAL"
+        )
+
         # Calculate overall success rate
-        success_rate = (passed_tests + partial_tests * 0.5) / total_tests if total_tests > 0 else 0
-        
+        success_rate = (
+            (passed_tests + partial_tests * 0.5) / total_tests if total_tests > 0 else 0
+        )
+
         # Collect all metrics
         all_metrics = {}
         for suite_name, result in self.test_results.items():
-            if 'metrics' in result:
-                all_metrics[suite_name] = result['metrics']
-        
+            if "metrics" in result:
+                all_metrics[suite_name] = result["metrics"]
+
         report = {
-            'test_summary': {
-                'total_tests': total_tests,
-                'passed': passed_tests,
-                'failed': failed_tests,
-                'partial': partial_tests,
-                'success_rate': success_rate,
-                'overall_status': 'PASSED' if success_rate >= 0.8 else 'PARTIAL' if success_rate >= 0.6 else 'FAILED'
+            "test_summary": {
+                "total_tests": total_tests,
+                "passed": passed_tests,
+                "failed": failed_tests,
+                "partial": partial_tests,
+                "success_rate": success_rate,
+                "overall_status": (
+                    "PASSED"
+                    if success_rate >= 0.8
+                    else "PARTIAL" if success_rate >= 0.6 else "FAILED"
+                ),
             },
-            'detailed_results': self.test_results,
-            'metrics_summary': all_metrics,
-            'timestamp': datetime.utcnow().isoformat(),
-            'execution_time_seconds': time.time() - self.start_time
+            "detailed_results": self.test_results,
+            "metrics_summary": all_metrics,
+            "timestamp": datetime.utcnow().isoformat(),
+            "execution_time_seconds": time.time() - self.start_time,
         }
-        
+
         return report
 
+
 async def main():
     """Main test execution"""
     print("🚀 MCP Ecosystem Expansion Test Suite")
     print("=" * 60)
-    
+
     # Check environment
     print("Environment Check:")
     print(f"  - Python version: {sys.version}")
     print(f"  - Working directory: {os.getcwd()}")
-    print(f"  - GitHub token: {'✅ Set' if os.environ.get('GITHUB_TOKEN') else '❌ Not set'}")
-    print(f"  - D-Wave token: {'✅ Set' if os.environ.get('DWAVE_API_TOKEN') else '❌ Not set'}")
+    print(
+        f"  - GitHub token: {'✅ Set' if os.environ.get('GITHUB_TOKEN') else '❌ Not set'}"
+    )
+    print(
+        f"  - D-Wave token: {'✅ Set' if os.environ.get('DWAVE_API_TOKEN') else '❌ Not set'}"
+    )
     print()
-    
+
     # Run tests
     tester = MCPEcosystemTester()
     report = await tester.run_all_tests()
-    
+
     # Display results
     print("\n📊 Test Results Summary:")
     print("=" * 60)
-    
-    summary = report['test_summary']
+
+    summary = report["test_summary"]
     print(f"Total Tests: {summary['total_tests']}")
     print(f"Passed: {summary['passed']} ✅")
     print(f"Failed: {summary['failed']} ❌")
@@ -446,43 +471,48 @@ async def main():
     print(f"Success Rate: {summary['success_rate']:.1%}")
     print(f"Overall Status: {summary['overall_status']}")
     print(f"Execution Time: {report['execution_time_seconds']:.2f} seconds")
-    
+
     print("\n📋 Detailed Results:")
     print("-" * 40)
-    
-    for suite_name, result in report['detailed_results'].items():
-        status_emoji = "✅" if result['status'] == 'PASSED' else "❌" if result['status'] == 'FAILED' else "⚠️"
+
+    for suite_name, result in report["detailed_results"].items():
+        status_emoji = (
+            "✅"
+            if result["status"] == "PASSED"
+            else "❌" if result["status"] == "FAILED" else "⚠️"
+        )
         print(f"{status_emoji} {suite_name}: {result['status']}")
-        
-        if 'error' in result:
+
+        if "error" in result:
             print(f"   Error: {result['error']}")
-        
-        if 'metrics' in result:
-            metrics = result['metrics']
+
+        if "metrics" in result:
+            metrics = result["metrics"]
             for key, value in metrics.items():
                 print(f"   {key}: {value}")
-    
+
     print("\n🎯 Recommendations:")
     print("-" * 40)
-    
-    if summary['success_rate'] >= 0.9:
+
+    if summary["success_rate"] >= 0.9:
         print("✅ Excellent! All major components are working correctly.")
         print("   The MCP ecosystem is ready for production use.")
-    elif summary['success_rate'] >= 0.7:
+    elif summary["success_rate"] >= 0.7:
         print("⚠️  Good progress! Most components are working.")
         print("   Review failed tests and address any critical issues.")
     else:
         print("❌ Several issues detected. Review and fix failed tests.")
         print("   Focus on core functionality before production deployment.")
-    
+
     # Save detailed report
     report_file = f"mcp_ecosystem_test_report_{int(time.time())}.json"
-    with open(report_file, 'w') as f:
+    with open(report_file, "w") as f:
         json.dump(report, f, indent=2, default=str)
-    
+
     print(f"\n📄 Detailed report saved to: {report_file}")
-    
+
     return report
 
+
 if __name__ == "__main__":
-    asyncio.run(main()) 
\ No newline at end of file
+    asyncio.run(main())
diff --git a/test_real_dwave_quantum.py b/test_real_dwave_quantum.py
index 34555f9..734caea 100644
--- a/test_real_dwave_quantum.py
+++ b/test_real_dwave_quantum.py
@@ -21,119 +21,127 @@ from pathlib import Path
 sys.path.append(str(Path(__file__).parent))
 
 try:
-    from connectors.dwave_quantum_connector import DWaveQuantumConnector, DWAVE_AVAILABLE
+    from connectors.dwave_quantum_connector import (
+        DWaveQuantumConnector,
+        DWAVE_AVAILABLE,
+    )
 except ImportError:
     DWAVE_AVAILABLE = False
 
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 class RealQuantumTest:
     """Test suite for authentic D-Wave quantum connector"""
-    
+
     def __init__(self):
         self.connector = None
         self.results = []
-    
+
     async def test_connection(self) -> bool:
         """Test connection to D-Wave Leap cloud service"""
         logger.info("🔌 Testing D-Wave Leap connection...")
-        
+
         if not DWAVE_AVAILABLE:
             logger.error("❌ D-Wave Ocean SDK not available")
             logger.info("💡 Install with: pip install dwave-ocean-sdk")
             logger.info("💡 Sign up at: https://cloud.dwavesys.com/leap/")
             return False
-        
+
         try:
             self.connector = DWaveQuantumConnector()
             success = await self.connector.connect({})
-            
+
             if success:
                 logger.info("✅ Connected to D-Wave quantum system")
                 solver_info = await self.connector.get_solver_info()
                 logger.info(f"📊 Solver: {solver_info['solver_info']['name']}")
                 logger.info(f"🔬 Type: {solver_info['solver_info']['type']}")
-                
-                if solver_info['solver_info']['type'] == 'QPU':
-                    logger.info(f"⚛️  Qubits: {solver_info['solver_info']['num_qubits']}")
-                    logger.info(f"🔗 Couplers: {solver_info['solver_info']['num_couplers']}")
-                
+
+                if solver_info["solver_info"]["type"] == "QPU":
+                    logger.info(
+                        f"⚛️  Qubits: {solver_info['solver_info']['num_qubits']}"
+                    )
+                    logger.info(
+                        f"🔗 Couplers: {solver_info['solver_info']['num_couplers']}"
+                    )
+
                 return True
             else:
                 logger.error("❌ Failed to connect to D-Wave")
                 return False
-                
+
         except Exception as e:
             logger.error(f"❌ Connection error: {e}")
             return False
-    
+
     async def test_simple_qubo(self) -> bool:
         """Test simple QUBO problem solving"""
         logger.info("🧮 Testing simple QUBO problem...")
-        
+
         # Simple QUBO: minimize x0 + x1 - 2*x0*x1
         # Optimal solutions: (0,1) or (1,0) with energy -1
         qubo = {(0, 0): 1, (1, 1): 1, (0, 1): -2}
-        
+
         try:
-            result = await self.connector.execute_action('solve_qubo', {
-                'qubo': qubo,
-                'num_reads': 50
-            })
-            
-            if result.get('success'):
-                solution = result['best_solution']
-                energy = result['best_energy']
-                
+            result = await self.connector.execute_action(
+                "solve_qubo", {"qubo": qubo, "num_reads": 50}
+            )
+
+            if result.get("success"):
+                solution = result["best_solution"]
+                energy = result["best_energy"]
+
                 logger.info(f"✅ QUBO solved")
                 logger.info(f"📊 Best solution: {solution}")
                 logger.info(f"⚡ Energy: {energy}")
                 logger.info(f"🔢 Samples: {result['num_solutions']}")
-                
+
                 # Check if we got a good solution
                 expected_energy = -1
                 if abs(energy - expected_energy) < 0.1:
                     logger.info("🎯 Found optimal solution!")
                     return True
                 else:
-                    logger.warning(f"⚠️  Energy {energy} not optimal (expected ~{expected_energy})")
+                    logger.warning(
+                        f"⚠️  Energy {energy} not optimal (expected ~{expected_energy})"
+                    )
                     return True  # Still counts as working
             else:
                 logger.error(f"❌ QUBO failed: {result.get('error')}")
                 return False
-                
+
         except Exception as e:
             logger.error(f"❌ QUBO test error: {e}")
             return False
-    
+
     async def test_traveling_salesman(self) -> bool:
         """Test Traveling Salesman Problem"""
         logger.info("🗺️  Testing Traveling Salesman Problem...")
-        
+
         # 3-city TSP
-        cities = ['NYC', 'Boston', 'Philly']
+        cities = ["NYC", "Boston", "Philly"]
         distances = {
-            ('NYC', 'Boston'): 4,
-            ('NYC', 'Philly'): 2,
-            ('Boston', 'Philly'): 3
+            ("NYC", "Boston"): 4,
+            ("NYC", "Philly"): 2,
+            ("Boston", "Philly"): 3,
         }
-        
+
         try:
-            result = await self.connector.execute_action('traveling_salesman', {
-                'cities': cities,
-                'distances': distances,
-                'num_reads': 30
-            })
-            
-            if result.get('success'):
-                route = result.get('route', [])
-                total_distance = result.get('total_distance', 0)
-                
+            result = await self.connector.execute_action(
+                "traveling_salesman",
+                {"cities": cities, "distances": distances, "num_reads": 30},
+            )
+
+            if result.get("success"):
+                route = result.get("route", [])
+                total_distance = result.get("total_distance", 0)
+
                 logger.info(f"✅ TSP solved")
                 logger.info(f"🛣️  Route: {' → '.join(route)}")
                 logger.info(f"📏 Total distance: {total_distance}")
-                
+
                 # Verify route is valid (visits all cities)
                 if set(route) == set(cities):
                     logger.info("🎯 Valid route found!")
@@ -144,36 +152,34 @@ class RealQuantumTest:
             else:
                 logger.error(f"❌ TSP failed: {result.get('error')}")
                 return False
-                
+
         except Exception as e:
             logger.error(f"❌ TSP test error: {e}")
             return False
-    
+
     async def test_maximum_cut(self) -> bool:
         """Test Maximum Cut graph problem"""
         logger.info("📊 Testing Maximum Cut problem...")
-        
+
         # Simple triangle graph
         edges = [(0, 1), (1, 2), (2, 0)]
         weights = {(0, 1): 1, (1, 2): 1, (2, 0): 1}
-        
+
         try:
-            result = await self.connector.execute_action('max_cut', {
-                'edges': edges,
-                'weights': weights,
-                'num_reads': 30
-            })
-            
-            if result.get('success'):
-                partition_a = result.get('partition_a', [])
-                partition_b = result.get('partition_b', [])
-                cut_value = result.get('cut_value', 0)
-                
+            result = await self.connector.execute_action(
+                "max_cut", {"edges": edges, "weights": weights, "num_reads": 30}
+            )
+
+            if result.get("success"):
+                partition_a = result.get("partition_a", [])
+                partition_b = result.get("partition_b", [])
+                cut_value = result.get("cut_value", 0)
+
                 logger.info(f"✅ Max-Cut solved")
                 logger.info(f"🔵 Partition A: {partition_a}")
                 logger.info(f"🔴 Partition B: {partition_b}")
                 logger.info(f"✂️  Cut value: {cut_value}")
-                
+
                 # For triangle, max cut should be 2
                 if cut_value >= 2:
                     logger.info("🎯 Good cut found!")
@@ -184,32 +190,32 @@ class RealQuantumTest:
             else:
                 logger.error(f"❌ Max-Cut failed: {result.get('error')}")
                 return False
-                
+
         except Exception as e:
             logger.error(f"❌ Max-Cut test error: {e}")
             return False
-    
+
     async def cleanup(self):
         """Clean up connection"""
         if self.connector:
             await self.connector.disconnect()
             logger.info("🔌 Disconnected from D-Wave")
-    
+
     async def run_all_tests(self):
         """Run complete test suite"""
         logger.info("🚀 Starting Real D-Wave Quantum Connector Tests")
         logger.info("=" * 60)
-        
+
         tests = [
             ("Connection Test", self.test_connection),
             ("Simple QUBO", self.test_simple_qubo),
             ("Traveling Salesman", self.test_traveling_salesman),
-            ("Maximum Cut", self.test_maximum_cut)
+            ("Maximum Cut", self.test_maximum_cut),
         ]
-        
+
         passed = 0
         total = len(tests)
-        
+
         for test_name, test_func in tests:
             logger.info(f"\n🧪 Running: {test_name}")
             try:
@@ -220,9 +226,9 @@ class RealQuantumTest:
                     logger.error(f"❌ {test_name}: FAILED")
             except Exception as e:
                 logger.error(f"❌ {test_name}: ERROR - {e}")
-        
+
         await self.cleanup()
-        
+
         logger.info("\n" + "=" * 60)
         logger.info("🧪 REAL D-WAVE QUANTUM TEST SUMMARY")
         logger.info("=" * 60)
@@ -230,60 +236,63 @@ class RealQuantumTest:
         logger.info(f"✅ Passed: {passed}")
         logger.info(f"❌ Failed: {total - passed}")
         logger.info(f"📈 Success Rate: {(passed/total)*100:.1f}%")
-        
+
         if passed == total:
             logger.info("🎉 ALL TESTS PASSED! Real quantum computing working!")
         else:
             logger.warning("⚠️  Some tests failed - check D-Wave Leap setup")
-        
+
         logger.info("\n📋 Requirements for Full Testing:")
         logger.info("• D-Wave Ocean SDK: pip install dwave-ocean-sdk")
         logger.info("• D-Wave Leap account: https://cloud.dwavesys.com/leap/")
         logger.info("• API token configured in environment")
         logger.info("• Internet connection for cloud access")
-        
+
         logger.info("\n🔗 Learn More:")
         logger.info("• D-Wave Examples: https://github.com/dwave-examples")
-        logger.info("• Advantage2 System: https://github.com/dwave-examples/advantage2.git")
+        logger.info(
+            "• Advantage2 System: https://github.com/dwave-examples/advantage2.git"
+        )
         logger.info("• Ocean Documentation: https://docs.ocean.dwavesys.com/")
-        
+
         return passed == total
 
+
 async def test_real_quantum():
     """Test real D-Wave quantum computing"""
     logger.info("🚀 Testing REAL D-Wave Quantum Computing")
-    
+
     if not DWAVE_AVAILABLE:
         logger.error("❌ D-Wave Ocean SDK not available")
         logger.info("Install: pip install dwave-ocean-sdk")
         logger.info("Signup: https://cloud.dwavesys.com/leap/")
         return False
-    
+
     connector = DWaveQuantumConnector()
-    
+
     # Test connection
     success = await connector.connect({})
     if not success:
         logger.error("❌ Failed to connect to D-Wave")
         return False
-    
+
     logger.info("✅ Connected to D-Wave quantum system")
-    
+
     # Test simple QUBO
     qubo = {(0, 0): 1, (1, 1): 1, (0, 1): -2}
-    result = await connector.execute_action('solve_qubo', {
-        'qubo': qubo,
-        'num_reads': 50
-    })
-    
-    if result.get('success'):
+    result = await connector.execute_action(
+        "solve_qubo", {"qubo": qubo, "num_reads": 50}
+    )
+
+    if result.get("success"):
         logger.info(f"✅ QUBO solved: {result['best_solution']}")
         logger.info(f"Energy: {result['best_energy']}")
     else:
         logger.error(f"❌ QUBO failed: {result.get('error')}")
-    
+
     await connector.disconnect()
-    return result.get('success', False)
+    return result.get("success", False)
+
 
 if __name__ == "__main__":
-    asyncio.run(test_real_quantum())
\ No newline at end of file
+    asyncio.run(test_real_quantum())
diff --git a/utils/__init__.py b/utils/__init__.py
index 447e97c..bbdec8c 100644
--- a/utils/__init__.py
+++ b/utils/__init__.py
@@ -2,4 +2,4 @@
 Utility modules for the self-correcting MCP runtime
 """
 
-__all__ = ["logger", "tracker"] 
\ No newline at end of file
+__all__ = ["logger", "tracker"]
diff --git a/utils/db_tracker.py b/utils/db_tracker.py
index 59334f5..de57582 100644
--- a/utils/db_tracker.py
+++ b/utils/db_tracker.py
@@ -5,24 +5,27 @@ import os
 from datetime import datetime
 from utils.logger import log
 
+
 def get_db_connection():
     """Get database connection using environment variables"""
     return psycopg2.connect(
-        host=os.environ.get('POSTGRES_HOST', 'mcp_db'),
-        port=os.environ.get('POSTGRES_PORT', '5432'),
-        user=os.environ.get('POSTGRES_USER', 'mcp'),
-        password=os.environ.get('POSTGRES_PASSWORD', 'mcp'),
-        database=os.environ.get('POSTGRES_DB', 'mcp')
+        host=os.environ.get("POSTGRES_HOST", "mcp_db"),
+        port=os.environ.get("POSTGRES_PORT", "5432"),
+        user=os.environ.get("POSTGRES_USER", "mcp"),
+        password=os.environ.get("POSTGRES_PASSWORD", "mcp"),
+        database=os.environ.get("POSTGRES_DB", "mcp"),
     )
 
+
 def ensure_tables_exist():
     """Create necessary tables if they don't exist"""
     conn = get_db_connection()
     cursor = conn.cursor()
-    
+
     try:
         # Create protocol_executions table
-        cursor.execute("""
+        cursor.execute(
+            """
             CREATE TABLE IF NOT EXISTS protocol_executions (
                 id SERIAL PRIMARY KEY,
                 protocol_name VARCHAR(100),
@@ -30,10 +33,12 @@ def ensure_tables_exist():
                 success BOOLEAN,
                 details JSONB
             );
-        """)
-        
+        """
+        )
+
         # Create protocol_mutations table
-        cursor.execute("""
+        cursor.execute(
+            """
             CREATE TABLE IF NOT EXISTS protocol_mutations (
                 id SERIAL PRIMARY KEY,
                 protocol_name VARCHAR(100) NOT NULL,
@@ -43,22 +48,27 @@ def ensure_tables_exist():
                 backup_code TEXT,
                 created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
             );
-        """)
-        
+        """
+        )
+
         # Create indexes for better performance
-        cursor.execute("""
+        cursor.execute(
+            """
             CREATE INDEX IF NOT EXISTS idx_executions_protocol 
             ON protocol_executions(protocol_name);
-        """)
-        
-        cursor.execute("""
+        """
+        )
+
+        cursor.execute(
+            """
             CREATE INDEX IF NOT EXISTS idx_executions_time 
             ON protocol_executions(execution_time);
-        """)
-        
+        """
+        )
+
         conn.commit()
         log("Database tables initialized successfully")
-        
+
     except Exception as e:
         log(f"Error creating tables: {e}")
         conn.rollback()
@@ -66,30 +76,36 @@ def ensure_tables_exist():
         cursor.close()
         conn.close()
 
+
 def track_outcome(protocol_name, outcome):
     """Track protocol outcome in database"""
     conn = get_db_connection()
     cursor = conn.cursor()
-    
+
     try:
         # Ensure tables exist
         ensure_tables_exist()
-        
+
         # Insert execution record
-        cursor.execute("""
+        cursor.execute(
+            """
             INSERT INTO protocol_executions 
             (protocol_name, execution_time, success, details)
             VALUES (%s, %s, %s, %s)
-        """, (
-            protocol_name,
-            datetime.utcnow(),
-            outcome.get('success', False),
-            json.dumps(outcome)
-        ))
-        
+        """,
+            (
+                protocol_name,
+                datetime.utcnow(),
+                outcome.get("success", False),
+                json.dumps(outcome),
+            ),
+        )
+
         conn.commit()
-        log(f"Outcome tracked in database for {protocol_name}: {outcome.get('success', 'unknown')}")
-        
+        log(
+            f"Outcome tracked in database for {protocol_name}: {outcome.get('success', 'unknown')}"
+        )
+
     except Exception as e:
         log(f"Failed to track outcome in database: {e}")
         conn.rollback()
@@ -99,30 +115,33 @@ def track_outcome(protocol_name, outcome):
         cursor.close()
         conn.close()
 
+
 def _track_to_file(protocol_name, outcome):
     """Fallback file-based tracking"""
-    os.makedirs('memory', exist_ok=True)
+    os.makedirs("memory", exist_ok=True)
     enhanced_outcome = {
         **outcome,
-        'protocol': protocol_name,
-        'timestamp': datetime.utcnow().isoformat()
+        "protocol": protocol_name,
+        "timestamp": datetime.utcnow().isoformat(),
     }
-    
-    memory_file = f'memory/{protocol_name}.json'
+
+    memory_file = f"memory/{protocol_name}.json"
     try:
-        with open(memory_file, 'a') as f:
-            f.write(json.dumps(enhanced_outcome) + '\n')
+        with open(memory_file, "a") as f:
+            f.write(json.dumps(enhanced_outcome) + "\n")
         log(f"Outcome tracked to file for {protocol_name} (database unavailable)")
     except Exception as e:
         log(f"Failed to track outcome to file: {e}")
 
+
 def get_protocol_stats(protocol_name):
     """Get statistics for a specific protocol from database"""
     conn = get_db_connection()
     cursor = conn.cursor()
-    
+
     try:
-        cursor.execute("""
+        cursor.execute(
+            """
             SELECT 
                 COUNT(*) as total,
                 SUM(CASE WHEN success THEN 1 ELSE 0 END) as successes,
@@ -130,23 +149,27 @@ def get_protocol_stats(protocol_name):
                 MAX(execution_time) as last_execution
             FROM protocol_executions
             WHERE protocol_name = %s
-        """, (protocol_name,))
-        
+        """,
+            (protocol_name,),
+        )
+
         result = cursor.fetchone()
         if result and result[0] > 0:
             total, successes, failures, last_execution = result
             return {
-                'protocol': protocol_name,
-                'total_executions': total,
-                'successes': successes,
-                'failures': failures,
-                'success_rate': successes / total if total > 0 else 0,
-                'failure_rate': failures / total if total > 0 else 0,
-                'last_execution': last_execution.isoformat() if last_execution else None
+                "protocol": protocol_name,
+                "total_executions": total,
+                "successes": successes,
+                "failures": failures,
+                "success_rate": successes / total if total > 0 else 0,
+                "failure_rate": failures / total if total > 0 else 0,
+                "last_execution": (
+                    last_execution.isoformat() if last_execution else None
+                ),
             }
         else:
             return None
-            
+
     except Exception as e:
         log(f"Error reading stats from database: {e}")
         # Fall back to file-based stats
@@ -155,46 +178,49 @@ def get_protocol_stats(protocol_name):
         cursor.close()
         conn.close()
 
+
 def _get_file_stats(protocol_name):
     """Fallback file-based statistics"""
-    memory_file = f'memory/{protocol_name}.json'
+    memory_file = f"memory/{protocol_name}.json"
     if not os.path.exists(memory_file):
         return None
-    
+
     total = 0
     successes = 0
     failures = 0
-    
+
     try:
-        with open(memory_file, 'r') as f:
+        with open(memory_file, "r") as f:
             for line in f:
                 if line.strip():
                     outcome = json.loads(line)
                     total += 1
-                    if outcome.get('success', False):
+                    if outcome.get("success", False):
                         successes += 1
                     else:
                         failures += 1
     except Exception as e:
         log(f"Error reading file stats: {e}")
         return None
-    
+
     return {
-        'protocol': protocol_name,
-        'total_executions': total,
-        'successes': successes,
-        'failures': failures,
-        'success_rate': successes / total if total > 0 else 0,
-        'failure_rate': failures / total if total > 0 else 0
+        "protocol": protocol_name,
+        "total_executions": total,
+        "successes": successes,
+        "failures": failures,
+        "success_rate": successes / total if total > 0 else 0,
+        "failure_rate": failures / total if total > 0 else 0,
     }
 
+
 def get_all_stats():
     """Get statistics for all protocols from database"""
     conn = get_db_connection()
     cursor = conn.cursor()
-    
+
     try:
-        cursor.execute("""
+        cursor.execute(
+            """
             SELECT 
                 protocol_name,
                 COUNT(*) as total,
@@ -203,22 +229,25 @@ def get_all_stats():
             FROM protocol_executions
             GROUP BY protocol_name
             ORDER BY protocol_name
-        """)
-        
+        """
+        )
+
         stats = []
         for row in cursor.fetchall():
             protocol_name, total, successes, failures = row
-            stats.append({
-                'protocol': protocol_name,
-                'total_executions': total,
-                'successes': successes,
-                'failures': failures,
-                'success_rate': successes / total if total > 0 else 0,
-                'failure_rate': failures / total if total > 0 else 0
-            })
-        
+            stats.append(
+                {
+                    "protocol": protocol_name,
+                    "total_executions": total,
+                    "successes": successes,
+                    "failures": failures,
+                    "success_rate": successes / total if total > 0 else 0,
+                    "failure_rate": failures / total if total > 0 else 0,
+                }
+            )
+
         return stats
-        
+
     except Exception as e:
         log(f"Error reading all stats from database: {e}")
         # Fall back to file-based stats
@@ -227,43 +256,42 @@ def get_all_stats():
         cursor.close()
         conn.close()
 
+
 def _get_all_file_stats():
     """Fallback file-based statistics for all protocols"""
     stats = []
-    memory_dir = 'memory'
+    memory_dir = "memory"
     if not os.path.exists(memory_dir):
         return stats
-    
+
     for filename in os.listdir(memory_dir):
-        if filename.endswith('.json'):
+        if filename.endswith(".json"):
             protocol_name = filename[:-5]
             protocol_stats = _get_file_stats(protocol_name)
             if protocol_stats:
                 stats.append(protocol_stats)
-    
+
     return stats
 
+
 def track_mutation(protocol_name, failure_rate, new_code, backup_code):
     """Track protocol mutation in database"""
     conn = get_db_connection()
     cursor = conn.cursor()
-    
+
     try:
-        cursor.execute("""
+        cursor.execute(
+            """
             INSERT INTO protocol_mutations 
             (protocol_name, mutation_time, previous_failure_rate, new_code, backup_code)
             VALUES (%s, %s, %s, %s, %s)
-        """, (
-            protocol_name,
-            datetime.utcnow(),
-            failure_rate,
-            new_code,
-            backup_code
-        ))
-        
+        """,
+            (protocol_name, datetime.utcnow(), failure_rate, new_code, backup_code),
+        )
+
         conn.commit()
         log(f"Mutation tracked in database for {protocol_name}")
-        
+
     except Exception as e:
         log(f"Failed to track mutation in database: {e}")
         conn.rollback()
@@ -271,13 +299,15 @@ def track_mutation(protocol_name, failure_rate, new_code, backup_code):
         cursor.close()
         conn.close()
 
+
 def get_mutation_history(protocol_name):
     """Get mutation history for a protocol"""
     conn = get_db_connection()
     cursor = conn.cursor()
-    
+
     try:
-        cursor.execute("""
+        cursor.execute(
+            """
             SELECT 
                 mutation_time,
                 previous_failure_rate,
@@ -286,22 +316,28 @@ def get_mutation_history(protocol_name):
             WHERE protocol_name = %s
             ORDER BY mutation_time DESC
             LIMIT 10
-        """, (protocol_name,))
-        
+        """,
+            (protocol_name,),
+        )
+
         history = []
         for row in cursor.fetchall():
             mutation_time, failure_rate, new_code = row
-            history.append({
-                'mutation_time': mutation_time.isoformat(),
-                'previous_failure_rate': failure_rate,
-                'code_preview': new_code[:200] + '...' if len(new_code) > 200 else new_code
-            })
-        
+            history.append(
+                {
+                    "mutation_time": mutation_time.isoformat(),
+                    "previous_failure_rate": failure_rate,
+                    "code_preview": (
+                        new_code[:200] + "..." if len(new_code) > 200 else new_code
+                    ),
+                }
+            )
+
         return history
-        
+
     except Exception as e:
         log(f"Error reading mutation history: {e}")
         return []
     finally:
         cursor.close()
-        conn.close() 
\ No newline at end of file
+        conn.close()
diff --git a/utils/logger.py b/utils/logger.py
index d2ccc1a..d99851f 100644
--- a/utils/logger.py
+++ b/utils/logger.py
@@ -2,33 +2,37 @@
 import os
 import datetime
 
+
 def log(message):
     """Log message to both file and console with timestamp"""
-    timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')
+    timestamp = datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
     formatted_message = f"[{timestamp}] {message}"
-    
+
     # Ensure logs directory exists
-    os.makedirs('logs', exist_ok=True)
-    
+    os.makedirs("logs", exist_ok=True)
+
     # Write to log file
     try:
         with open("logs/mcp.log", "a") as f:
             f.write(formatted_message + "\n")
     except Exception as e:
         print(f"Failed to write to log file: {e}")
-    
+
     # Also print to console
     print(formatted_message)
 
+
 def log_json(data, prefix="DATA"):
     """Log JSON data in a structured format"""
     import json
+
     try:
         json_str = json.dumps(data, indent=2)
         log(f"{prefix}: {json_str}")
     except Exception as e:
         log(f"Failed to log JSON data: {e}")
 
+
 def get_log_path():
     """Get the path to the current log file"""
-    return os.path.abspath("logs/mcp.log")
\ No newline at end of file
+    return os.path.abspath("logs/mcp.log")
diff --git a/utils/protocol_registry.py b/utils/protocol_registry.py
index 8ad97e2..611ad41 100644
--- a/utils/protocol_registry.py
+++ b/utils/protocol_registry.py
@@ -5,144 +5,152 @@ import hashlib
 from datetime import datetime
 from utils.logger import log
 
+
 class ProtocolRegistry:
     """Central registry for protocol management"""
-    
+
     def __init__(self):
-        self.registry_file = 'protocols/categories.json'
-        self.deployment_config = 'deployment_config.json'
-        
+        self.registry_file = "protocols/categories.json"
+        self.deployment_config = "deployment_config.json"
+
     def load_registry(self):
         """Load protocol registry from file"""
         try:
-            with open(self.registry_file, 'r') as f:
+            with open(self.registry_file, "r") as f:
                 return json.load(f)
         except FileNotFoundError:
-            return {'categories': {}, 'registry': {}}
-    
-    def register_protocol(self, name, category, location='local', metadata=None):
+            return {"categories": {}, "registry": {}}
+
+    def register_protocol(self, name, category, location="local", metadata=None):
         """Register a new protocol"""
         registry = self.load_registry()
-        
+
         # Add to category
-        if category not in registry['categories']:
-            registry['categories'][category] = {
-                'description': f'{category.title()} protocols',
-                'protocols': []
+        if category not in registry["categories"]:
+            registry["categories"][category] = {
+                "description": f"{category.title()} protocols",
+                "protocols": [],
             }
-        
-        if name not in registry['categories'][category]['protocols']:
-            registry['categories'][category]['protocols'].append(name)
-        
+
+        if name not in registry["categories"][category]["protocols"]:
+            registry["categories"][category]["protocols"].append(name)
+
         # Add protocol metadata
-        if 'protocols' not in registry:
-            registry['protocols'] = {}
-            
-        registry['protocols'][name] = {
-            'category': category,
-            'location': location,
-            'registered_at': datetime.utcnow().isoformat(),
-            'metadata': metadata or {},
-            'checksum': self._calculate_checksum(name)
+        if "protocols" not in registry:
+            registry["protocols"] = {}
+
+        registry["protocols"][name] = {
+            "category": category,
+            "location": location,
+            "registered_at": datetime.utcnow().isoformat(),
+            "metadata": metadata or {},
+            "checksum": self._calculate_checksum(name),
         }
-        
+
         # Update registry metadata
-        registry['registry']['total_protocols'] = sum(
-            len(cat['protocols']) for cat in registry['categories'].values()
+        registry["registry"]["total_protocols"] = sum(
+            len(cat["protocols"]) for cat in registry["categories"].values()
         )
-        registry['registry']['last_updated'] = datetime.utcnow().isoformat() + 'Z'
-        
+        registry["registry"]["last_updated"] = datetime.utcnow().isoformat() + "Z"
+
         # Save registry
-        with open(self.registry_file, 'w') as f:
+        with open(self.registry_file, "w") as f:
             json.dump(registry, f, indent=2)
-            
+
         log(f"Protocol {name} registered in category {category}")
         return True
-    
+
     def _calculate_checksum(self, protocol_name):
         """Calculate checksum for protocol file"""
-        protocol_file = f'protocols/{protocol_name}.py'
+        protocol_file = f"protocols/{protocol_name}.py"
         if os.path.exists(protocol_file):
-            with open(protocol_file, 'rb') as f:
+            with open(protocol_file, "rb") as f:
                 return hashlib.sha256(f.read()).hexdigest()[:16]
         return None
-    
-    def deploy_protocol(self, name, target='local', config=None):
+
+    def deploy_protocol(self, name, target="local", config=None):
         """Deploy protocol to target environment"""
         registry = self.load_registry()
-        
-        if name not in registry.get('protocols', {}):
+
+        if name not in registry.get("protocols", {}):
             log(f"Protocol {name} not found in registry")
             return False
-        
-        protocol_info = registry['protocols'][name]
-        
+
+        protocol_info = registry["protocols"][name]
+
         # Deployment logic based on target
-        if target == 'local':
+        if target == "local":
             # Already deployed locally
             return True
-        elif target == 'docker':
+        elif target == "docker":
             # Deploy to Docker container
             return self._deploy_to_docker(name, config)
-        elif target == 'remote':
+        elif target == "remote":
             # Deploy to remote worker
             return self._deploy_to_remote(name, config)
-        
+
         return False
-    
+
     def _deploy_to_docker(self, name, config):
         """Deploy protocol to Docker container"""
         # Future implementation for Docker deployment
         log(f"Docker deployment for {name} not yet implemented")
         return False
-    
+
     def _deploy_to_remote(self, name, config):
         """Deploy protocol to remote worker"""
         # Future implementation for remote deployment
         log(f"Remote deployment for {name} not yet implemented")
         return False
-    
+
     def get_protocol_info(self, name):
         """Get information about a protocol"""
         registry = self.load_registry()
-        protocols = registry.get('protocols', {})
+        protocols = registry.get("protocols", {})
         return protocols.get(name, None)
-    
+
     def list_by_category(self, category=None):
         """List protocols by category"""
         registry = self.load_registry()
-        
+
         if category:
-            cat_info = registry['categories'].get(category, {})
-            return cat_info.get('protocols', [])
+            cat_info = registry["categories"].get(category, {})
+            return cat_info.get("protocols", [])
         else:
-            return registry['categories']
-    
+            return registry["categories"]
+
     def search_protocols(self, query):
         """Search protocols by name or metadata"""
         registry = self.load_registry()
         results = []
-        
+
         query_lower = query.lower()
-        
+
         # Search in protocol names
-        for name, info in registry.get('protocols', {}).items():
+        for name, info in registry.get("protocols", {}).items():
             if query_lower in name.lower():
-                results.append({
-                    'name': name,
-                    'category': info.get('category'),
-                    'location': info.get('location')
-                })
-        
+                results.append(
+                    {
+                        "name": name,
+                        "category": info.get("category"),
+                        "location": info.get("location"),
+                    }
+                )
+
         # Search in categories
-        for cat_name, cat_info in registry['categories'].items():
-            if query_lower in cat_name.lower() or query_lower in cat_info.get('description', '').lower():
-                for protocol in cat_info['protocols']:
-                    if not any(r['name'] == protocol for r in results):
-                        results.append({
-                            'name': protocol,
-                            'category': cat_name,
-                            'location': 'local'
-                        })
-        
-        return results 
\ No newline at end of file
+        for cat_name, cat_info in registry["categories"].items():
+            if (
+                query_lower in cat_name.lower()
+                or query_lower in cat_info.get("description", "").lower()
+            ):
+                for protocol in cat_info["protocols"]:
+                    if not any(r["name"] == protocol for r in results):
+                        results.append(
+                            {
+                                "name": protocol,
+                                "category": cat_name,
+                                "location": "local",
+                            }
+                        )
+
+        return results
diff --git a/utils/tracker.py b/utils/tracker.py
index 26e6e35..8f481c5 100644
--- a/utils/tracker.py
+++ b/utils/tracker.py
@@ -3,86 +3,90 @@ import json
 import os
 from utils.logger import log
 
+
 def track_outcome(protocol_name, outcome):
     """Track protocol outcome to memory for later analysis"""
     # Ensure memory directory exists
-    os.makedirs('memory', exist_ok=True)
-    
+    os.makedirs("memory", exist_ok=True)
+
     # Add metadata to outcome
     enhanced_outcome = {
         **outcome,
-        'protocol': protocol_name,
-        'timestamp': __import__('datetime').datetime.utcnow().isoformat()
+        "protocol": protocol_name,
+        "timestamp": __import__("datetime").datetime.utcnow().isoformat(),
     }
-    
-    memory_file = f'memory/{protocol_name}.json'
+
+    memory_file = f"memory/{protocol_name}.json"
     try:
-        with open(memory_file, 'a') as f:
-            f.write(json.dumps(enhanced_outcome) + '\n')
+        with open(memory_file, "a") as f:
+            f.write(json.dumps(enhanced_outcome) + "\n")
         log(f"Outcome tracked for {protocol_name}: {outcome.get('success', 'unknown')}")
     except Exception as e:
         log(f"Failed to track outcome for {protocol_name}: {e}")
 
+
 def get_protocol_stats(protocol_name):
     """Get statistics for a specific protocol"""
-    memory_file = f'memory/{protocol_name}.json'
+    memory_file = f"memory/{protocol_name}.json"
     if not os.path.exists(memory_file):
         return None
-    
+
     total = 0
     successes = 0
     failures = 0
-    
+
     try:
-        with open(memory_file, 'r') as f:
+        with open(memory_file, "r") as f:
             for line in f:
                 if line.strip():
                     outcome = json.loads(line)
                     total += 1
-                    if outcome.get('success', False):
+                    if outcome.get("success", False):
                         successes += 1
                     else:
                         failures += 1
     except Exception as e:
         log(f"Error reading stats for {protocol_name}: {e}")
         return None
-    
+
     return {
-        'protocol': protocol_name,
-        'total_executions': total,
-        'successes': successes,
-        'failures': failures,
-        'success_rate': successes / total if total > 0 else 0,
-        'failure_rate': failures / total if total > 0 else 0
+        "protocol": protocol_name,
+        "total_executions": total,
+        "successes": successes,
+        "failures": failures,
+        "success_rate": successes / total if total > 0 else 0,
+        "failure_rate": failures / total if total > 0 else 0,
     }
 
+
 def get_all_stats():
     """Get statistics for all protocols"""
     stats = []
-    memory_dir = 'memory'
+    memory_dir = "memory"
     if not os.path.exists(memory_dir):
         return stats
-    
+
     for filename in os.listdir(memory_dir):
-        if filename.endswith('.json'):
+        if filename.endswith(".json"):
             protocol_name = filename[:-5]  # Remove .json extension
             protocol_stats = get_protocol_stats(protocol_name)
             if protocol_stats:
                 stats.append(protocol_stats)
-    
+
     return stats
 
+
 def clear_memory(protocol_name=None):
     """Clear memory for a specific protocol or all protocols"""
     if protocol_name:
-        memory_file = f'memory/{protocol_name}.json'
+        memory_file = f"memory/{protocol_name}.json"
         if os.path.exists(memory_file):
             os.remove(memory_file)
             log(f"Memory cleared for protocol: {protocol_name}")
     else:
-        memory_dir = 'memory'
+        memory_dir = "memory"
         if os.path.exists(memory_dir):
             for filename in os.listdir(memory_dir):
-                if filename.endswith('.json'):
+                if filename.endswith(".json"):
                     os.remove(os.path.join(memory_dir, filename))
-            log("All protocol memory cleared")
\ No newline at end of file
+            log("All protocol memory cleared")
-- 
2.39.5

